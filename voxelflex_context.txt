==========================================================
        VoxelFlex (Temperature-Aware) Project Context     
     (Workflow: Metadata Preprocessing / On-Demand HDF5)    
==========================================================

Project Goal & Workflow Overview:
---------------------------------

Dataset Scale (Approximate):
----------------------------
*   **Domains:** ~5,400 unique domain IDs in the source HDF5 file. (~5,378 found relevant across provided splits).
*   **RMSF Data:** > 3.3 million rows in the aggregated CSV file (representing multiple temperatures per residue).
*   **Samples Generated:** The preprocessing step generates > 3.3 million individual samples (residue@temperature points) stored in the `master_samples.parquet` file.
*   **Voxel Data Size:** Individual processed voxel arrays (float32, 5x21x21x21) are ~180KB each. The raw HDF5 file is likely hundreds of GBs.
*   **Intermediate Data:** The `master_samples.parquet` file (metadata only) is relatively small (MBs to low GBs). **This workflow avoids large intermediate voxel storage.**

Compute Environment Specifications:
---------------------------------
*   **CPU:** 36 Cores (Based on user info and `htop` output)
*   **RAM:** ~62.6 GB Total System Memory
*   **GPU:** 1x NVIDIA Quadro RTX 8000 (49152 MiB / ~47.5 GB VRAM)
*   **CUDA Version:** 12.2 (from `nvidia-smi`)
*   **NVIDIA Driver Version:** 535.183.01 (from `nvidia-smi`)
*   **Operating System:** Linux (Implied)
*   **Filesystem Mount:** `/home/s_felix` located on `/dev/sdc2` (Previously experienced read-only remount issues under heavy I/O load).

Input Data Formats (Expected):
------------------------------
1.  **Voxel Data (HDF5):**
    *   Path: `input.voxel_file` in config.
    *   Format: HDF5 (`.hdf5`).
    *   Structure: `DomainID` -> `ChainID` -> `ResidueID` (string digit) -> HDF5 Dataset.
    *   Voxel Dataset: Expected `bool`, shape `(21, 21, 21, 5)`. Processed on-demand to `float32`, shape `(5, 21, 21, 21)`.

2.  **Aggregated RMSF Data (CSV):**
    *   Path: `input.aggregated_rmsf_file` in config.
    *   Format: CSV (`.csv`).
    *   Required Columns: `domain_id`, `resid`, `resname`, `temperature_feature`, `target_rmsf`.
    *   Optional Columns: `relative_accessibility`, `dssp`, `secondary_structure_encoded`.

3.  **Domain Split Files (.txt):**
    *   Paths: `input.train_split_file`, etc. in config.
    *   Format: Plain text (`.txt`), one HDF5 `DomainID` per line.

Output Data Formats (Expected):
-------------------------------
Outputs saved within `outputs/<run_name>/`.

1.  **From `preprocess`:**
    *   `input_data/processed/master_samples.parquet` (or `.csv`): Single file with sample metadata. **NO VOXELS.**
    *   `outputs/<run_name>/models/temp_scaling_params.json`: Scaler min/max.
    *   `outputs/<run_name>/failed_preprocess_domains.txt`: Domains failing initial checks.

2.  **From `train`:**
    *   `outputs/<run_name>/models/*.pt`: Model checkpoints.
    *   `outputs/<run_name>/training_history.json`: Epoch metrics.
    *   `outputs/<run_name>/logs/voxelflex.log`: Detailed log.

3.  **From `predict`:**
    *   `outputs/<run_name>/metrics/predictions_*.csv`: Predictions CSV.

4.  **From `evaluate`:**
    *   `outputs/<run_name>/metrics/evaluation_metrics_*.json`: Metrics results JSON.

5.  **From `visualize`:**
    *   `outputs/<run_name>/visualizations/*.png`: Plot images.
    *   `outputs/<run_name>/visualizations/*_data.csv` (Optional): Plot data CSVs.

Project Folder Structure:
-------------------------
(Showing relative paths from project root)

.
├── create_voxelflex_context.sh
├── create_VoxelFlex.sh
├── input_data
│   ├── processed
│   │   ├── master_samples.parquet
│   │   ├── test
│   │   ├── train
│   │   └── val
│   ├── rmsf
│   │   ├── aggregated_rmsf_all_temps.csv
│   │   └── replica_average
│   ├── test_domains.txt
│   ├── train_domains.txt
│   ├── val_domains.txt
│   └── voxel
│       └── mdcath_voxelized.hdf5
├── LICENSE
├── pyproject.toml
├── README.md
├── requirements.txt
├── src
│   └── voxelflex
│       ├── cli
│       ├── config
│       ├── data
│       ├── __init__.py
│       ├── models
│       └── utils
├── tests
│   ├── __init__.py
│   └── test_placeholder.py
└── voxelflex_context.txt

16 directories, 16 files

File Contents:
----------------
(Snippets shown for input data files like *.txt)

==========================================================
===== FILE: pyproject.toml =====
==========================================================

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "voxelflex"
# Increment version for refactoring, consider using dynamic versioning later
version = "0.3.0"
authors = [
  # Replace with your actual name and email
  { name="Your Name", email="your.email@example.com" },
]
description = "Temperature-aware protein flexibility (RMSF) prediction from 3D voxel data using CNNs and a preprocessing workflow."
readme = "README.md"
requires-python = ">=3.9" # Based on type hints and f-strings
license = { file = "LICENSE" } # Add a LICENSE file (e.g., MIT)
classifiers = [
    "Development Status :: 3 - Alpha", # Update as project matures
    "Intended Audience :: Science/Research",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "License :: OSI Approved :: MIT License", # Choose your license
    "Operating System :: OS Independent",
    "Topic :: Scientific/Engineering :: Bio-Informatics",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Typing :: Typed",
]
dependencies = [
    "numpy>=1.21", # Check specific version needs
    "pandas>=1.3",
    "torch>=1.10.0", # Consider torch version carefully based on hardware/CUDA
    "scipy",
    "scikit-learn",
    "h5py",
    "pyyaml",
    "matplotlib", # Ensure compatible version if issues arise
    "seaborn",
    "psutil",
    "tqdm",
    # Add specific versions if needed for reproducibility
    # e.g., "torch==2.0.1", "pandas==2.0.3"
]

[project.urls]
"Homepage" = "https://github.com/yourusername/voxelflex" # Replace with your repo URL
"Bug Tracker" = "https://github.com/yourusername/voxelflex/issues" # Replace

# Define the command-line script entry point
[project.scripts]
voxelflex = "voxelflex.cli.cli:main"

[tool.setuptools.packages.find]
where = ["src"]

# Optional: Tool configurations (can be added later)
# [tool.black]
# line-length = 88
# target-version = ['py310']

# [tool.isort]
# profile = "black"

# [tool.mypy]
# python_version = "3.10"
# warn_return_any = true
# warn_unused_configs = true
# ignore_missing_imports = true # Initially, relax this if needed

==========================================================
===== FILE: README.md =====
==========================================================

# VoxelFlex (Temperature-Aware - Preprocessing Workflow)

Predicting temperature-dependent protein flexibility (RMSF) from 3D voxel data using deep learning, optimized with a robust preprocessing pipeline.

## Overview

This package provides tools to train and use 3D Convolutional Neural Networks (CNNs) for predicting per-residue Root Mean Square Fluctuation (RMSF) values. This version is **temperature-aware**, meaning it takes the simulation temperature as an input feature, allowing a single model to predict flexibility across different temperatures.

It utilizes voxelized representations of protein structures (e.g., from Aposteriori) and RMSF data derived from Molecular Dynamics (MD) simulations (e.g., from the mdCATH dataset).

**Key Feature:** This version implements an optimized **preprocessing workflow**. Raw voxel and RMSF data are converted into batched PyTorch tensor files (`.pt`) before training. This significantly simplifies the training loop, improves performance, enhances robustness, and allows for efficient handling of very large datasets within defined memory limits.

## Features

*   Temperature-aware 3D CNN models (MultipathRMSFNet, DenseNet3D, DilatedResNet3D).
*   **Robust preprocessing pipeline:** Converts raw HDF5/CSV to optimized `.pt` batch files.
    *   Handles HDF5 boolean dtype casting and shape transposition.
    *   Manages memory via batch-wise HDF5 loading and optional caching.
    *   Scales temperature feature based on training set statistics.
    *   Generates metadata files (`.meta`) for easy loading.
*   **Simplified and efficient training:** Uses preprocessed batches directly in the DataLoader.
*   Support for mixed-precision training and standard optimizers/schedulers.
*   Rigorous evaluation including stratified metrics and permutation feature importance for temperature.
*   Visualization tools for analyzing training progress and model performance.
*   Command-line interface (`preprocess`, `train`, `predict`, `evaluate`, `visualize`).
*   Designed for use with large-scale MD datasets like mdCATH.

## Installation

```bash
# Recommended: Create and activate a virtual environment
python -m venv venv
source venv/bin/activate # or venv\Scripts\activate on Windows

# Install the package from the project root directory
pip install .

# Or for development (changes in src/ reflect immediately):
pip install -e .
```

Install required dependencies:
```bash
pip install -r requirements.txt
```

## Usage

The package is primarily used via the command line interface.

**1. Preprocess Data:** (Run this first!)
```bash
voxelflex preprocess --config path/to/your/config.yaml [-v|-vv]
```
This reads raw data specified in the config, performs processing and scaling, and saves batches to `input_data/processed/` (or as configured in `data.processed_dir`).

**2. Train Model:**
```bash
voxelflex train --config path/to/your/config.yaml [-v|-vv] [--force_preprocess]
```
This loads the preprocessed data and trains the model, saving outputs (checkpoints, logs) to `outputs/<run_name>/`. Use `--force_preprocess` to re-run preprocessing before training.

**3. Predict RMSF:**
```bash
voxelflex predict --config path/to/config.yaml --model path/to/model.pt --temperature 320 [-v|-vv] [--domains ID1 ID2 ...] [--output_csv filename.csv]
```
Loads the trained model and predicts RMSF for specific domains (or test split domains) at the given temperature. Saves results to `outputs/<run_name>/metrics/`.

**4. Evaluate Model:**
```bash
voxelflex evaluate --config path/to/config.yaml --model path/to/model.pt --predictions path/to/preds.csv [-v|-vv]
```
Compares predictions against ground truth (from the aggregated RMSF file) and calculates performance metrics, saving results to `outputs/<run_name>/metrics/`.

**5. Visualize Results:**
```bash
voxelflex visualize --config path/to/config.yaml --predictions path/to/preds.csv [-v|-vv] [--history path/to/history.json]
```
Generates performance plots based on prediction/evaluation data and optional training history, saving them to `outputs/<run_name>/visualizations/`.

Use `-v` for INFO level logging and `-vv` for DEBUG level logging for any command.

## Configuration

Modify the `src/voxelflex/config/default_config.yaml` file or create a copy and adjust parameters. Key sections:

*   `input`: Paths to raw voxel HDF5, aggregated RMSF CSV, and domain split files (`.txt`).
*   `data`: Paths for processed data output, preprocessing batch size, cache limit.
*   `output`: Base directory for run outputs (logs, models, metrics, visualizations).
*   `model`: CNN architecture choice and hyperparameters.
*   `training`: Epochs, batch size (for loading `.pt` files), learning rate, optimizer, scheduler, etc.
*   `prediction`: Batch size for inference.
*   `evaluation`: Settings for stratified metrics and permutation importance.
*   `logging`: Logging levels and progress bar visibility.
*   `visualization`: Toggles for different plots and output settings.
*   `system_utilization`: GPU preference.

## Data Preparation

1.  **Voxel Data (HDF5):** Place your HDF5 file (e.g., `mdcath_voxelized.hdf5`) in `input_data/voxel/`. Ensure it follows the expected structure (`DomainID -> ChainID -> ResidueID -> Dataset`) and that datasets are predominantly `bool` type with shape `(X, Y, Z, Channels)` where `Channels=5`.
2.  **RMSF Data (CSV):** Create an aggregated CSV file (e.g., `aggregated_rmsf_all_temps.csv`) containing RMSF data from all relevant temperatures. Required columns: `domain_id` (must be mappable to HDF5 keys), `resid` (integer), `resname` (string), `temperature_feature` (float, Kelvin), `target_rmsf` (float). Optional columns for evaluation: `relative_accessibility`, `dssp` (or `secondary_structure_encoded`). Place in `input_data/rmsf/`.
3.  **Splits (.txt):** Generate plain text files listing HDF5 domain keys for training, validation, and testing (one ID per line). Place these in `input_data/`. Using splits generated by tools like `mdcath-sampling` is highly recommended to mitigate homology bias.

## Contributing

[Optional: Add guidelines for contributions if applicable]

## License

[Specify License, e.g., MIT License] - Remember to add a LICENSE file.
# VoxelFlex_T
# VoxelFlex_T

==========================================================
===== FILE: requirements.txt =====
==========================================================

# VoxelFlex Core Dependencies
# Pin versions for better reproducibility, update as needed.
numpy>=1.21
pandas>=1.3
# PyTorch: Choose version compatible with your CUDA toolkit if using GPU
# Example for CUDA 11.8: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
# Example for CUDA 12.1: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
# Example for CPU only: pip install torch torchvision torchaudio
torch>=1.10.0
scipy
scikit-learn
h5py
pyyaml
matplotlib
seaborn
psutil
tqdm
pyarrow

# Optional but recommended for development/testing:
# pytest
# flake8
# black
# isort
# mypy

==========================================================
===== FILE: src/voxelflex/config/default_config.yaml =====
==========================================================

# Default configuration for VoxelFlex (Temperature-Aware)

input:
  voxel_file: input_data/voxel/mdcath_voxelized.hdf5
  aggregated_rmsf_file: input_data/rmsf/aggregated_rmsf_all_temps.csv
  train_split_file: input_data/train_domains.txt
  val_split_file: input_data/val_domains.txt
  test_split_file: input_data/test_domains.txt
  max_domains: null  # Set to null or remove if no limit desired

data:
  processed_dir: input_data/processed/
  master_samples_file: "master_samples.parquet"
  # Path for temp scaling params generated by preprocess
  temp_scaling_params_file: "temp_scaling_params.json"

output:
  base_dir: outputs/
  run_name: "voxelflex_run_{timestamp}"
  log_file: voxelflex.log
  # run_dir, log_dir etc. are generated dynamically by config.py

model:
  architecture: multipath_rmsf_net  # Or your preferred model
  input_channels: 5
  voxel_depth: 21
  voxel_height: 21
  voxel_width: 21
  # --- Architecture Specific Params ---
  densenet:
    growth_rate: 16
    block_config: [4, 4, 4]
    num_init_features: 32
    bn_size: 4
  channel_growth_rate: 1.5
  num_residual_blocks: 3
  base_filters: 32
  dropout_rate: 0.3

training:
  # --- Chunk size for IterableDataset ---
  chunk_size: 100  # Number of domains per chunk
  # -------------------------------------------
  batch_size: 128  # Keep relatively high if RAM allows chunk loading
  num_epochs: 5
  learning_rate: 0.0005
  weight_decay: 1e-4
  seed: 42
  num_workers: 4  # Adjust based on CPU cores and memory for chunk processing
  pin_memory: true  # Usually beneficial with GPU
  prefetch_factor: 2  # Default for DataLoader
  resume_checkpoint: null
  save_best_metric: "val_pearson"  # Or val_loss
  save_best_mode: "max"  # Or min
  checkpoint_interval: 5
  gradient_clipping:
    enabled: true
    max_norm: 1.0
  mixed_precision:
    enabled: true  # Highly recommended
  gradient_accumulation_steps: 1  # Adjust if batch_size needs reduction
  scheduler:
    type: reduce_on_plateau
    monitor_metric: "val_pearson"
    mode: "max"
    patience: 5
    factor: 0.5
    min_lr: 1e-07
    threshold: 0.001
  early_stopping:
    enabled: true
    patience: 10
    monitor_metric: "val_pearson"
    mode: "max"
    min_delta: 0.001

prediction:
  batch_size: 256  # Batch size for predict/evaluate

evaluation:
  calculate_stratified_metrics: true
  calculate_permutation_importance: true
  sasa_bins: [0.0, 0.1, 0.4, 1.01]
  permutation_n_repeats: 5

logging:
  level: INFO  # Overall level for file/console if others not set
  console_level: INFO
  file_level: DEBUG
  show_progress_bars: true
  log_timing: false  # Enable if you want detailed timing info
  log_memory_usage: true  # Keep this enabled

visualization:
  plot_loss: true
  plot_correlation: true
  plot_predictions: true
  plot_density_scatter: true
  plot_error_distribution: true
  plot_residue_type_analysis: true
  plot_sasa_error_analysis: true
  plot_ss_error_analysis: true
  plot_amino_acid_performance: false
  save_format: png
  dpi: 150
  max_scatter_points: 1000
  save_plot_data: true

system_utilization:
  detect_cores: true
  adjust_for_gpu: true
==========================================================
===== FILE: src/voxelflex/config/config.py =====
==========================================================

"""
Configuration module for VoxelFlex (Temperature-Aware).

Handles loading, validation, merging with defaults, and path expansion.
"""
import os
import logging
import time
import socket
import sys
from pathlib import Path
from typing import Dict, Any, Optional, List
import yaml

logger = logging.getLogger("voxelflex.config")
from voxelflex.utils.file_utils import resolve_path, ensure_dir

def load_config(config_path: str) -> Dict[str, Any]:
    """
    Load configuration from YAML file and merge with defaults.
    
    Args:
        config_path: Path to configuration YAML file
        
    Returns:
        Complete configuration dictionary
        
    Raises:
        FileNotFoundError: If config file not found
        ValueError: If config YAML is invalid
        RuntimeError: If default config could not be loaded
    """
    # Resolve config path
    config_path_resolved = resolve_path(config_path)
    logger.info(f"Loading user config: {config_path_resolved}")
    
    if not os.path.exists(config_path_resolved):
        raise FileNotFoundError(f"Config file not found: {config_path_resolved}")
        
    # Load user config
    try:
        with open(config_path_resolved, 'r') as f:
            user_config = yaml.safe_load(f)
    except yaml.YAMLError as e:
        raise ValueError(f"Invalid YAML in {config_path_resolved}") from e
        
    # Handle empty config
    if user_config is None:
        logger.warning(f"User config {config_path_resolved} empty.")
        user_config = {}
        
    # Load default config
    default_config = get_default_config()
    if not default_config:
        raise RuntimeError("Failed to load default config.")
        
    # Merge configs
    config = merge_configs(default_config, user_config)
    
    # Validate merged config
    validate_config(config)
    
    # Expand paths
    config = expand_paths(config)
    
    # Add timestamp to run name if needed
    if "run_name" not in config["output"] or "{timestamp}" in config["output"].get("run_name", ""):
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        run_name_template = config["output"].get("run_name", "voxelflex_run_{timestamp}")
        config["output"]["run_name"] = run_name_template.format(timestamp=timestamp)
        logger.info(f"Run name: {config['output']['run_name']}")
    
    # Create output directory structure
    base_output_dir = config["output"]["base_dir"]
    run_output_dir = os.path.join(base_output_dir, config["output"]["run_name"])
    
    # Add output directories to config
    config["output"]["run_dir"] = run_output_dir
    config["output"]["log_dir"] = os.path.join(run_output_dir, "logs")
    config["output"]["models_dir"] = os.path.join(run_output_dir, "models")
    config["output"]["metrics_dir"] = os.path.join(run_output_dir, "metrics")
    config["output"]["visualizations_dir"] = os.path.join(run_output_dir, "visualizations")
    
    # Create directories
    ensure_dir(config["output"]["log_dir"])
    ensure_dir(config["output"]["models_dir"])
    ensure_dir(config["output"]["metrics_dir"])
    ensure_dir(config["output"]["visualizations_dir"])
    
    # Setup temperature scaling file path
    scaling_file_name = os.path.basename(config["data"]["temp_scaling_params_file"])
    if not scaling_file_name:
        scaling_file_name = "temp_scaling_params.json"
    config["data"]["temp_scaling_params_file"] = os.path.join(config["output"]["models_dir"], scaling_file_name)
    logger.debug(f"Temp scaling file path: {config['data']['temp_scaling_params_file']}")
    
    # Set master samples path
    processed_base = config["data"]["processed_dir"]
    config["data"]["master_samples_path"] = os.path.join(processed_base, config["data"]["master_samples_file"])
    logger.debug(f"Master samples file path: {config['data']['master_samples_path']}")
    
    # Remove old processed paths if they exist from previous versions
    config["data"].pop("processed_train_dir", None)
    config["data"].pop("processed_val_dir", None)
    config["data"].pop("processed_test_dir", None)
    config["data"].pop("processed_train_meta", None)
    config["data"].pop("processed_val_meta", None)
    config["data"].pop("processed_test_meta", None)
    
    # Add runtime information
    config["runtime"] = {
        "timestamp": timestamp,
        "hostname": socket.gethostname(),
        "python_version": sys.version
    }
    
    logger.debug("Configuration loaded successfully.")
    return config

def merge_configs(default: Dict[str, Any], user: Dict[str, Any]) -> Dict[str, Any]:
    """
    Recursively merge user config with default config.
    User values override defaults.
    
    Args:
        default: Default configuration
        user: User configuration
        
    Returns:
        Merged configuration
    """
    merged = default.copy()
    
    for key, value in user.items():
        if isinstance(value, dict) and isinstance(merged.get(key), dict):
            # Recursively merge nested dictionaries
            merged[key] = merge_configs(merged[key], value)
        else:
            # Override default with user value
            merged[key] = value
            
    return merged

def validate_config(config: Dict[str, Any]) -> None:
    """
    Validate configuration structure and essential parameters.
    
    Args:
        config: Configuration to validate
        
    Raises:
        ValueError: For invalid configurations
    """
    logger.debug("Validating configuration structure...")
    
    # Check required sections
    required_sections = [
        'input', 'output', 'model', 'training', 'data', 
        'logging', 'evaluation', 'visualization', 'system_utilization'
    ]
    
    for section in required_sections:
        if section not in config:
            raise ValueError(f"Missing config section: '{section}'")
        if not isinstance(config[section], dict):
            raise ValueError(f"Section '{section}' must be a dictionary.")
    
    # Check input configuration
    input_cfg = config['input']
    req_input = ['voxel_file', 'aggregated_rmsf_file', 'train_split_file', 'val_split_file']
    
    for key in req_input:
        if key not in input_cfg or not input_cfg[key]:
            raise ValueError(f"Missing input param: 'input.{key}'")
            
    if 'test_split_file' not in input_cfg or not input_cfg['test_split_file']:
        logger.warning(f"Optional input 'input.test_split_file' missing.")
    
    # Check data configuration
    data_cfg = config['data']
    req_data = ['processed_dir', 'master_samples_file', 'temp_scaling_params_file']
    
    for key in req_data:
        if key not in data_cfg or not data_cfg[key]:
            raise ValueError(f"Missing data param: 'data.{key}'")
    
    # Check output configuration
    if 'base_dir' not in config['output'] or not config['output']['base_dir']:
        raise ValueError("Missing output param: 'output.base_dir'")
    
    # Check model configuration
    model_cfg = config.get('model', {})
    if 'architecture' not in model_cfg:
        raise ValueError("Missing 'model.architecture'")
        
    valid_arch = ['densenet3d_regression', 'dilated_resnet3d', 'multipath_rmsf_net']
    if model_cfg['architecture'] not in valid_arch:
        raise ValueError(f"Invalid 'model.architecture'. Use: {valid_arch}")
    
    # Check model dimensions
    req_model_dims = ['input_channels', 'voxel_depth', 'voxel_height', 'voxel_width']
    for key in req_model_dims:
        if key not in model_cfg or not isinstance(model_cfg[key], int) or model_cfg[key] <= 0:
            raise ValueError(f"Missing/invalid positive integer for 'model.{key}' (needed by VoxelDataset)")
    
    # Check specific model architecture parameters
    if model_cfg['architecture'] == 'densenet3d_regression':
        if 'densenet' not in model_cfg or not isinstance(model_cfg['densenet'], dict):
            raise ValueError("Missing 'model.densenet' section.")
            
        req_densenet = ['growth_rate', 'block_config', 'num_init_features', 'bn_size']
        for key in req_densenet:
            if key not in model_cfg['densenet']:
                raise ValueError(f"Missing 'model.densenet.{key}'")
                
        if not isinstance(model_cfg['densenet']['block_config'], list):
            raise ValueError("'model.densenet.block_config' must be a list.")
    
    # Check training configuration
    train_cfg = config.get('training', {})
    req_train = ['batch_size', 'num_epochs', 'learning_rate', 'weight_decay', 'seed']
    
    for key in req_train:
        if key not in train_cfg:
            raise ValueError(f"Missing 'training.{key}'")
    
    # Check training parameters
    if not isinstance(train_cfg.get('batch_size'), int) or train_cfg.get('batch_size', 0) <= 0:
        raise ValueError("'training.batch_size' must be positive.")
        
    if not isinstance(train_cfg.get('num_epochs'), int) or train_cfg.get('num_epochs', 0) <= 0:
        raise ValueError("'training.num_epochs' must be positive.")
        
    if not isinstance(train_cfg.get('num_workers', 0), int) or train_cfg.get('num_workers', 0) < 0:
        raise ValueError("'training.num_workers' must be non-negative.")
    
    # Check metrics
    valid_metrics = ['val_loss', 'val_pearson']
    
    def validate_monitor_metric(cfg_section: dict, section_key: str, section_name: str):
        metric = cfg_section.get(section_key, {}).get('monitor_metric')
        if metric and metric not in valid_metrics:
            raise ValueError(f"'training.{section_name}.monitor_metric' ('{metric}') must be one of {valid_metrics}")
    
    if 'save_best_metric' in train_cfg and train_cfg['save_best_metric'] not in valid_metrics:
        raise ValueError(f"'training.save_best_metric' must be one of {valid_metrics}")
        
    validate_monitor_metric(train_cfg, 'scheduler', 'scheduler')
    validate_monitor_metric(train_cfg, 'early_stopping', 'early_stopping')
    
    # Check scheduler
    sched_cfg = train_cfg.get('scheduler', {})
    if 'type' in sched_cfg and sched_cfg['type'] not in ['reduce_on_plateau', 'cosine_annealing', 'step']:
        raise ValueError(f"Invalid scheduler type: {sched_cfg['type']}")
    
    # Check system configuration
    sys_cfg = config.get('system_utilization', {})
    if not isinstance(sys_cfg.get('detect_cores'), bool):
        raise ValueError("'system_utilization.detect_cores' must be boolean.")
        
    if not isinstance(sys_cfg.get('adjust_for_gpu'), bool):
        raise ValueError("'system_utilization.adjust_for_gpu' must be boolean.")
    
    logger.debug("Configuration validation passed.")

def expand_paths(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Expand relative paths in configuration to absolute paths.
    
    Args:
        config: Configuration with paths to expand
        
    Returns:
        Configuration with expanded paths
    """
    logger.debug("Expanding paths in configuration...")
    
    # List of (section, key) tuples for paths to expand
    paths_to_expand = {
        ('input', 'voxel_file'), 
        ('input', 'aggregated_rmsf_file'), 
        ('input', 'train_split_file'), 
        ('input', 'val_split_file'), 
        ('input', 'test_split_file'), 
        ('output', 'base_dir'), 
        ('data', 'processed_dir'), 
        ('training', 'resume_checkpoint'),
    }
    
    for section, key in paths_to_expand:
        if config.get(section) is not None and isinstance(config[section], dict) and config[section].get(key):
            original_path = config[section][key]
            if isinstance(original_path, str) and original_path:
                config[section][key] = resolve_path(original_path)
            elif not original_path:
                config[section][key] = None
                
    return config

def get_default_config() -> Dict[str, Any]:
    """
    Load default configuration from default_config.yaml.
    
    Returns:
        Default configuration or empty dict if file not found/valid
    """
    default_config_path = os.path.join(os.path.dirname(__file__), 'default_config.yaml')
    local_logger = logging.getLogger("voxelflex.config.default")
    local_logger.debug(f"Loading default config: {default_config_path}")
    
    if not os.path.exists(default_config_path):
        local_logger.error(f"Default config NOT FOUND: {default_config_path}")
        return {}
        
    try:
        with open(default_config_path, 'r') as f:
            default_config = yaml.safe_load(f)
            
        if default_config is None:
            local_logger.error("Default config empty!")
            return {}
            
        local_logger.debug("Default config loaded.")
        return default_config
    except Exception as e:
        local_logger.error(f"Failed load default config: {e}")
        return {}
==========================================================
===== FILE: src/voxelflex/data/validators.py =====
==========================================================

"""
Data validation module for VoxelFlex (Temperature-Aware).

Provides validation functions for input data formats, focusing on RMSF data.
Voxel validation happens during loading rather than separately.
"""

import logging
from typing import Dict, List, Set, Any, Optional, Tuple

import numpy as np
import pandas as pd

# Use the centralized logger
logger = logging.getLogger("voxelflex.data")

def validate_aggregated_rmsf_data(rmsf_df: pd.DataFrame) -> pd.DataFrame:
    """
    Validate aggregated RMSF DataFrame for required columns, types, and potential issues.
    
    Args:
        rmsf_df: DataFrame loaded from the aggregated RMSF CSV.
        
    Returns:
        Validated and potentially filtered DataFrame.
        
    Raises:
        ValueError: If input is empty or essential columns are missing/invalid.
    """
    logger.info("Validating aggregated RMSF data...")
    
    if not isinstance(rmsf_df, pd.DataFrame) or rmsf_df.empty:
        raise ValueError("Input RMSF data is not a non-empty DataFrame.")

    df_validated = rmsf_df.copy()

    # Define required columns and their expected types
    required_cols = {
        'domain_id': str,
        'resid': int,
        'resname': str,
        'temperature_feature': float,
        'target_rmsf': float
    }
    
    # Check for required columns
    missing_req = [col for col in required_cols if col not in df_validated.columns]
    if missing_req:
        raise ValueError(f"Aggregated RMSF data missing required columns: {missing_req}. Found: {list(df_validated.columns)}")

    # Optional columns for stratification and analysis
    optional_cols = ['relative_accessibility', 'dssp', 'secondary_structure_encoded']
    ss_col_found = 'dssp' in df_validated.columns or 'secondary_structure_encoded' in df_validated.columns
    available_optional = [col for col in optional_cols if col in df_validated.columns]
    
    # Log initial state
    initial_rows = len(df_validated)
    logger.info(f"Initial RMSF rows: {initial_rows}")

    # --- Optimize Type Conversion ---
    # Use predefined types for numeric columns - more efficient than multiple conversions
    type_mapping = {
        'resid': 'Int64',  # Use nullable Int type to handle potential NaNs
        'temperature_feature': float,
        'target_rmsf': float
    }
    
    # Convert only the columns needed (dtype dictionary for efficiency)
    for col, dtype in type_mapping.items():
        if col in df_validated.columns:
            try:
                df_validated[col] = pd.to_numeric(df_validated[col], errors='coerce')
                if dtype == 'Int64':
                    df_validated[col] = df_validated[col].astype('Int64')
            except Exception as e:
                logger.warning(f"Error converting column '{col}': {e}")

    # Drop rows with NaNs in required columns
    orig_len = len(df_validated)
    df_validated.dropna(subset=list(required_cols.keys()), inplace=True)
    rows_dropped_nan = orig_len - len(df_validated)
    
    if rows_dropped_nan > 0:
        logger.info(f"Dropped {rows_dropped_nan} rows due to NaN/invalid values in required columns.")

    # Check and fix negative RMSF values
    if 'target_rmsf' in df_validated.columns:
        neg_rmsf_mask = df_validated['target_rmsf'] < 0
        neg_count = neg_rmsf_mask.sum()
        
        if neg_count > 0:
            logger.warning(f"Found {neg_count} negative 'target_rmsf' values. Setting them to 0.")
            df_validated.loc[neg_rmsf_mask, 'target_rmsf'] = 0.0

    # Check for duplicate entries (domain_id, resid, temperature_feature)
    key_cols = ['domain_id', 'resid', 'temperature_feature']
    if all(c in df_validated.columns for c in key_cols):
        # Handle the resid/Int64 conversion properly
        try:
            duplicates_mask = df_validated.duplicated(subset=key_cols, keep='first')
            dup_count = duplicates_mask.sum()
            
            if dup_count > 0:
                logger.warning(f"Found {dup_count} duplicate entries based on {key_cols}. Keeping first occurrence.")
                df_validated = df_validated[~duplicates_mask]
        except Exception as e:
            logger.warning(f"Could not check for duplicates: {e}")

    # --- Summarize Results ---
    final_rows = len(df_validated)
    
    if initial_rows > 0:
        percentage_str = f"({final_rows / initial_rows:.1%})"
    else:
        percentage_str = "(0.0%)"
        
    logger.info(f"RMSF validation finished. Valid rows: {final_rows} / {initial_rows} {percentage_str}.")

    if final_rows == 0:
        raise ValueError("No valid RMSF data remaining after validation.")

    # --- Log Summary Statistics ---
    logger.info("Summary statistics of validated RMSF data:")
    try:
        logger.info(f"  Unique Domains: {df_validated['domain_id'].nunique()}")
        logger.info(f"  Unique Temperatures: {sorted(df_validated['temperature_feature'].unique())}")
        logger.info(f"  Target RMSF Range: [{df_validated['target_rmsf'].min():.4f}, {df_validated['target_rmsf'].max():.4f}]")
        logger.info(f"  Target RMSF Mean: {df_validated['target_rmsf'].mean():.4f}")
    except Exception as e:
        logger.warning(f"Could not generate full summary statistics: {e}")

    return df_validated
==========================================================
===== FILE: src/voxelflex/data/data_loader.py =====
==========================================================

# src/voxelflex/data/data_loader.py (Chunked IterableDataset Implementation for All Splits)
import os
from pathlib import Path
import logging
import time
import gc
import h5py
import json
import psutil
import math
from typing import Dict, List, Tuple, Optional, Union, Any, Callable, Set, Iterator
from collections import defaultdict, OrderedDict, namedtuple

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader, IterableDataset, get_worker_info

try:
    import pyarrow.parquet as pq
    PYARROW_AVAILABLE = True
except ImportError:
    PYARROW_AVAILABLE = False

logger = logging.getLogger("voxelflex.data")

# Utils and Validators (ensure they are imported)
from voxelflex.data.validators import validate_aggregated_rmsf_data
from voxelflex.utils.file_utils import resolve_path, ensure_dir, load_json, save_json
from voxelflex.utils.logging_utils import EnhancedProgressBar

# --- Constants and Helpers ---
MASTER_SAMPLES_FILENAME = "master_samples.parquet"
TARGET_SHAPE: Tuple[int, int, int, int] = (5, 21, 21, 21)
TARGET_DTYPE: np.dtype = np.dtype(np.float32)
INPUT_CHANNELS: int = TARGET_SHAPE[0]

def process_voxel(voxel_raw: np.ndarray) -> Optional[np.ndarray]:
    """Processes a raw voxel array from HDF5 to the target format."""
    try:
        if not isinstance(voxel_raw, np.ndarray): return None
        if voxel_raw.dtype == bool: processed_array = voxel_raw.astype(TARGET_DTYPE)
        elif np.issubdtype(voxel_raw.dtype, np.floating): processed_array = voxel_raw.astype(TARGET_DTYPE, copy=False)
        elif np.issubdtype(voxel_raw.dtype, np.integer): processed_array = voxel_raw.astype(TARGET_DTYPE)
        else: return None
        if processed_array.ndim == 4 and processed_array.shape[-1] == INPUT_CHANNELS: processed_array = np.transpose(processed_array, (3, 0, 1, 2))
        elif processed_array.ndim != len(TARGET_SHAPE): return None
        if processed_array.shape != TARGET_SHAPE: return None
        if not np.isfinite(processed_array).all(): return None
        return processed_array
    except Exception: return None

def load_process_domain_from_handle(
    h5_handle,
    domain_id: str,
    expected_channels: int = INPUT_CHANNELS,
    target_shape_chw: Optional[Tuple[int, ...]] = TARGET_SHAPE,
    log_per_residue: bool = False
) -> Dict[str, np.ndarray]:
    """Loads and processes all valid residues for a given domain_id from an open HDF5 handle."""
    domain_data_dict = {}
    processed_count = 0; failed_count = 0; t_start = time.perf_counter()
    try:
        if domain_id not in h5_handle: logger.warning(f"Domain '{domain_id}' not found..."); return {}
        domain_group = h5_handle[domain_id]; residue_group = None
        potential_chain_keys = [k for k in domain_group.keys() if isinstance(domain_group[k], h5py.Group)]
        for chain_key in potential_chain_keys:
             try:
                  potential_res_group = domain_group[chain_key]
                  if any(key.isdigit() for key in potential_res_group.keys()): residue_group = potential_res_group; break
             except Exception: continue
        if residue_group is None: logger.warning(f"No valid residue group for {domain_id}."); return {}
        residue_keys = sorted([k for k in residue_group.keys() if k.isdigit()], key=int) # Process residues in order
        for resid_str in residue_keys:
            voxel_raw = None
            try:
                voxel_dataset = residue_group[resid_str]
                if not isinstance(voxel_dataset, h5py.Dataset): continue
                voxel_raw = voxel_dataset[:]
                processed_array = process_voxel(voxel_raw)
                if processed_array is not None: domain_data_dict[resid_str] = processed_array; processed_count += 1
                else: failed_count += 1
            except Exception as e: failed_count += 1; logger.warning(f"Error reading/processing {domain_id}:{resid_str}: {e}", exc_info=False)
            finally: del voxel_raw
        t_end = time.perf_counter()
        if processed_count > 0 or failed_count > 0: logger.debug(f"Domain {domain_id}: Processed={processed_count}, Failed={failed_count}. Time: {(t_end-t_start)*1000:.1f}ms")
        return domain_data_dict
    except Exception as e: logger.error(f"Critical error loading domain {domain_id}: {e}", exc_info=True); return {}


try:
    import pyarrow.parquet as pq
    PYARROW_AVAILABLE = True
except ImportError:
    PYARROW_AVAILABLE = False

# ... [Keep other imports and helper functions as they are] ...

class ChunkedVoxelDataset(IterableDataset):
    """
    IterableDataset that loads and processes protein voxel data in chunks of domains.
    
    This dataset is designed for memory-efficient loading of large HDF5 files,
    processing domains in manageable chunks rather than all at once.
    """
    def __init__(self,
                 master_samples_path: str,
                 split: str,
                 domain_list: List[str],
                 voxel_hdf5_path: str,
                 temp_scaling_params: Dict[str, float],
                 chunk_size: int = 100,
                 shuffle_domain_list: bool = False):
        """
        Initialize the chunked dataset for a specific split.
        
        Args:
            master_samples_path: Path to the master_samples.parquet file
            split: Dataset split ('train', 'val', 'test')
            domain_list: Complete list of domain IDs for this split
            voxel_hdf5_path: Path to the HDF5 voxel file
            temp_scaling_params: Dict with 'temp_min' and 'temp_max'
            chunk_size: Number of domains to load per chunk
            shuffle_domain_list: Whether to shuffle domains before chunking
        """
        super().__init__()
        self.split = split
        self.domain_list = domain_list  # Keep original order initially
        self.voxel_hdf5_path = Path(voxel_hdf5_path).resolve()
        self.temp_min = temp_scaling_params.get('temp_min', 280.0)
        self.temp_max = temp_scaling_params.get('temp_max', 360.0)
        
        if not isinstance(chunk_size, int) or chunk_size <= 0:
            logger.warning(f"Invalid chunk_size ({chunk_size}), defaulting to 100.")
            self.chunk_size = 100
        else:
            self.chunk_size = chunk_size
            
        self.shuffle_domain_list = shuffle_domain_list

        # Load metadata from master samples file
        self.metadata_lookup = {}
        try:
            logger.info(f"IterableDataset [{split}]: Loading metadata from {master_samples_path}")
            t0 = time.time()
            
            # Select only necessary columns
            cols = ['hdf5_domain_id', 'resid_str', 'raw_temp', 'target_rmsf', 'split']
            
            # Efficient reading with filtering if pyarrow is available
            if PYARROW_AVAILABLE:
                filters = [('split', '==', split)]
                df = pd.read_parquet(master_samples_path, columns=cols, filters=filters)
            else:
                df_full = pd.read_csv(master_samples_path, usecols=cols)
                df = df_full[df_full['split'] == split].copy()
                del df_full  # Free memory

            # Create lookup with string keys for consistency - this is the critical fix
            for _, row in df.iterrows():
                key = (str(row['hdf5_domain_id']), str(row['resid_str']))
                if key not in self.metadata_lookup:
                    self.metadata_lookup[key] = []
                self.metadata_lookup[key].append((float(row['raw_temp']), float(row['target_rmsf'])))

            logger.info(f"IterableDataset [{split}]: Metadata lookup created ({len(self.metadata_lookup)} entries) in {time.time()-t0:.2f}s")
            del df
            gc.collect()
        except Exception as e:
            logger.exception(f"IterableDataset [{split}]: Failed to load metadata lookup: {e}")
            self.metadata_lookup = {}
            self.domain_list = []

    def _load_process_chunk_voxels(self, domain_chunk: List[str], worker_id: int) -> Dict[str, Dict[str, np.ndarray]]:
        """
        Loads and processes voxels for a list of domains.
        
        Args:
            domain_chunk: List of domain IDs to process
            worker_id: ID of the worker processing this chunk
            
        Returns:
            Dictionary mapping domain IDs to dictionaries of residue voxels
        """
        t_start = time.perf_counter()
        logger.debug(f"Worker {worker_id}: Loading chunk ({len(domain_chunk)} domains): {domain_chunk[:3]}...")
        chunk_voxel_data: Dict[str, Dict[str, np.ndarray]] = {}
        
        try:
            # Open HDF5 file locally within this method for the chunk
            with h5py.File(self.voxel_hdf5_path, 'r') as h5_file:
                for domain_id in domain_chunk:
                    domain_data = load_process_domain_from_handle(
                        h5_file, domain_id,
                        expected_channels=INPUT_CHANNELS,
                        target_shape_chw=TARGET_SHAPE,
                        log_per_residue=False
                    )
                    if domain_data:
                        chunk_voxel_data[domain_id] = domain_data
                        
            t_end = time.perf_counter()
            logger.info(f"Worker {worker_id}: Loaded/processed chunk ({len(chunk_voxel_data)}/{len(domain_chunk)} domains ok) in {t_end - t_start:.2f}s.")
            return chunk_voxel_data
        except Exception as e:
            logger.error(f"Worker {worker_id}: Failed to open/read HDF5 for chunk: {e}", exc_info=True)
            return {}

    def _prepare_samples_for_chunk(self, chunk_voxel_data: Dict[str, Dict[str, np.ndarray]], worker_id: int) -> List[Dict[str, Any]]:
        """
        Combines loaded voxels with metadata for the chunk.
        
        Args:
            chunk_voxel_data: Dictionary of domain voxel data
            worker_id: ID of the worker processing this chunk
            
        Returns:
            List of sample dictionaries with voxels, temperatures, and targets
        """
        t_start = time.perf_counter()
        chunk_samples = []
        skipped_meta = 0
        temp_range = self.temp_max - self.temp_min
        use_midpoint = abs(temp_range) < 1e-6

        # Process domains/residues in a deterministic order for consistency
        for domain_id in sorted(chunk_voxel_data.keys()):
            residues = chunk_voxel_data[domain_id]
            for resid_str in sorted(residues.keys(), key=lambda x: int(x) if x.isdigit() else 0):
                voxel_np = residues[resid_str]
                try:
                    # Use string keys for lookup - matching the format used during initialization
                    key = (str(domain_id), str(resid_str))
                    metadata_entries = self.metadata_lookup.get(key, [])
                    
                    if not metadata_entries:
                        # Debug logging to help diagnose lookup issues
                        logger.debug(f"Worker {worker_id}: No metadata for {domain_id}:{resid_str}")
                        skipped_meta += 1
                        continue
                        
                    for raw_temp, target_rmsf in metadata_entries:
                        # Scale temperature between 0-1
                        if use_midpoint:
                            scaled_temp = 0.5
                        else:
                            scaled_temp = (raw_temp - self.temp_min) / temp_range
                        scaled_temp = min(max(scaled_temp, 0.0), 1.0)

                        # Ensure voxel array is C-contiguous for efficient tensor conversion
                        if not voxel_np.flags['C_CONTIGUOUS']:
                            voxel_np = np.ascontiguousarray(voxel_np)
                            
                        # Convert to PyTorch tensors
                        voxel_tensor = torch.from_numpy(voxel_np)
                        scaled_temp_tensor = torch.tensor([scaled_temp], dtype=torch.float32)
                        target_tensor = torch.tensor(target_rmsf, dtype=torch.float32)

                        # Add sample to batch
                        chunk_samples.append({
                            'voxels': voxel_tensor, 
                            'scaled_temps': scaled_temp_tensor, 
                            'targets': target_tensor
                        })
                except Exception as e:
                    logger.warning(f"Worker {worker_id}: Error preparing sample {domain_id}:{resid_str}: {e}")
                    skipped_meta += 1

        t_end = time.perf_counter()
        if chunk_samples:
            logger.info(f"Worker {worker_id} [{self.split}]: Yielding {len(chunk_samples)} samples for chunk.")
        else:
            logger.warning(f"Worker {worker_id} [{self.split}]: No samples prepared for chunk. Skipped {skipped_meta} residues.")
        logger.debug(f"Worker {worker_id}: Prepared {len(chunk_samples)} samples, skipped {skipped_meta} (meta). Time: {t_end - t_start:.2f}s.")
        return chunk_samples

    def __iter__(self) -> Iterator[Dict[str, torch.Tensor]]:
        """
        Iterator logic loading, processing, and yielding samples chunk by chunk.
        
        Yields:
            Dictionary with 'voxels', 'scaled_temps', and 'targets' tensors
        """
        # Get worker info for parallel processing
        worker_info = get_worker_info()
        if worker_info is None:
            # Single-worker case (e.g., debugging)
            worker_id = 0
            num_workers = 1
            domains_for_worker = self.domain_list
        else:
            # Multi-worker case
            worker_id = worker_info.id
            num_workers = worker_info.num_workers
            # Partition domains equally among workers
            per_worker = int(math.ceil(len(self.domain_list) / float(num_workers)))
            start_idx = worker_id * per_worker
            end_idx = min(start_idx + per_worker, len(self.domain_list))
            domains_for_worker = self.domain_list[start_idx:end_idx]

        if not domains_for_worker:
            logger.warning(f"Worker {worker_id}: No domains assigned for split '{self.split}'.")
            return

        # Make a copy to avoid modifying original list
        effective_domain_list = list(domains_for_worker)
        
        # Shuffle if requested (typically for training)
        if self.shuffle_domain_list:
            np.random.shuffle(effective_domain_list)
            logger.info(f"Worker {worker_id}: Shuffled domain list for split '{self.split}'.")
        else:
            logger.info(f"Worker {worker_id}: Processing domains in original order for split '{self.split}'.")

        # Process domains in chunks
        num_chunks = math.ceil(len(effective_domain_list) / self.chunk_size)
        logger.info(f"Worker {worker_id}: Starting iteration for '{self.split}' split over {num_chunks} chunks (size ~{self.chunk_size}).")

        for i in range(num_chunks):
            chunk_start_idx = i * self.chunk_size
            chunk_end_idx = min(chunk_start_idx + self.chunk_size, len(effective_domain_list))
            domain_chunk_ids = effective_domain_list[chunk_start_idx:chunk_end_idx]
            if not domain_chunk_ids:
                continue

            logger.info(f"Worker {worker_id} [{self.split}]: Processing chunk {i+1}/{num_chunks} ({len(domain_chunk_ids)} domains)")
            
            # Load and process voxels for this chunk
            chunk_voxels = self._load_process_chunk_voxels(domain_chunk_ids, worker_id)
            if not chunk_voxels:
                logger.warning(f"Worker {worker_id} [{self.split}]: No voxels loaded for chunk {i+1}.")
                continue

            # Prepare samples from the loaded chunk
            chunk_samples = self._prepare_samples_for_chunk(chunk_voxels, worker_id)
            
            # Free voxel memory after preparing samples
            del chunk_voxels
            gc.collect()

            if not chunk_samples:
                logger.warning(f"Worker {worker_id} [{self.split}]: No samples prepared for chunk {i+1}.")
                continue

            # Yield samples one by one
            for sample in chunk_samples:
                yield sample

            # Clean up after yielding all samples from this chunk
            logger.debug(f"Worker {worker_id}: Finished yielding chunk {i+1}. Clearing samples.")
            del chunk_samples
            gc.collect()

        logger.info(f"Worker {worker_id}: Finished iteration for split '{self.split}'.")


# No longer needs to manage persistent HDF5 handles for ChunkedVoxelDataset
def worker_init_fn(worker_id):
    """Worker initialization function (minimal version)."""
    worker_info = get_worker_info()
    if worker_info:
        seed = worker_info.seed % 2**32 # Ensure seed is in valid range
        np.random.seed(seed)
        # torch.manual_seed(seed) # Can cause issues if main process also seeds
        logger.debug(f"Worker {worker_id}: Initialized with NumPy seed {seed}")
    else:
         logger.warning(f"Worker {worker_id}: Could not get worker_info for seeding.")

# --- simple_collate_fn (Keep as is) ---
def simple_collate_fn(batch):
    # ... (implementation as before) ...
    batch = [item for item in batch if item is not None]
    if not batch: logger.warning("Collate fn received empty batch."); return None
    try:
        return {
            'voxels': torch.stack([item['voxels'] for item in batch]),
            'scaled_temps': torch.stack([item['scaled_temps'] for item in batch]),
            'targets': torch.stack([item['targets'] for item in batch])
        }
    except RuntimeError as e: logger.error(f"Collation error (shape mismatch?): {e}"); return None
    except Exception as e: logger.error(f"Unexpected collation error: {e}", exc_info=True); return None



# --- Preprocessing Helpers (Keep as before) ---
def load_aggregated_rmsf_data(aggregated_rmsf_file: str) -> pd.DataFrame:
    rmsf_file = resolve_path(aggregated_rmsf_file)
    logger.info(f"Loading RMSF data from: {rmsf_file}")
    if not os.path.exists(rmsf_file): raise FileNotFoundError(f"RMSF file not found: {rmsf_file}")
    try:
        dtype_spec = {'domain_id': str, 'resid': 'Int64', 'temperature_feature': float, 'target_rmsf': float}
        rmsf_df = pd.read_csv(rmsf_file, dtype=dtype_spec, low_memory=False)
        logger.info(f"Loaded {len(rmsf_df)} rows with optimized types")
        return validate_aggregated_rmsf_data(rmsf_df)
    except Exception as e: logger.exception(f"Failed to read RMSF CSV: {e}"); raise

def create_master_rmsf_lookup(rmsf_df: pd.DataFrame) -> Dict[Tuple[str, int], List[Tuple[float, float]]]:
    logger.info("Creating RMSF lookup..."); t0 = time.time(); required = ['domain_id', 'resid', 'temperature_feature', 'target_rmsf']
    if not all(c in rmsf_df.columns for c in required): raise ValueError(f"Missing: {required}")
    lookup = defaultdict(list); grouped = rmsf_df.groupby(['domain_id', 'resid'])
    for name, group in grouped:
        try: lookup[(str(name[0]), int(name[1]))] = list(zip(group['temperature_feature'].astype(float), group['target_rmsf'].astype(float)))
        except (ValueError, TypeError): logger.warning(f"Skipping RMSF lookup entry due to conversion error for {name}")
    base_lookup={}; added_count=0
    for k, v in lookup.items():
        base = k[0].split('_')[0]
        if base != k[0]: base_key=(base, k[1]);
        if base_key not in lookup and base_key not in base_lookup: base_lookup[base_key]=v; added_count+=1
    lookup.update(base_lookup)
    logger.info(f"RMSF lookup created: {len(lookup)} keys ({added_count} base names) in {time.time()-t0:.2f}s")
    return dict(lookup)

def create_domain_mapping(voxel_domain_keys: List[str], rmsf_domain_ids: List[str]) -> Dict[str, str]:
    logger.info("Creating domain mapping...");
    if not voxel_domain_keys: logger.warning("Voxel keys empty."); return {}
    if not rmsf_domain_ids: logger.warning("RMSF IDs empty."); return {}
    v_set = set(voxel_domain_keys); r_set = set(rmsf_domain_ids); mapping={}; matches={'exact':0, 'base':0}
    r_base_map={r.split('_')[0]: r for r in rmsf_domain_ids};
    for h_key in v_set:
        if h_key in r_set: mapping[h_key]=h_key; matches['exact']+=1; continue
        base_h = h_key.split('_')[0]
        if base_h in r_set: mapping[h_key]=base_h; matches['base']+=1;
        elif base_h in r_base_map: mapping[h_key]=r_base_map[base_h]; matches['base']+=1
    total = len(mapping); cov = (total/len(voxel_domain_keys))*100 if voxel_domain_keys else 0
    logger.info(f"Domain mapping: {total}/{len(voxel_domain_keys)} keys ({cov:.1f}%)"); logger.info(f"  Matches: Exact={matches['exact']}, Base={matches['base']}")
    return mapping


# --- PredictionDataset for evaluation and inference ---
class PredictionDataset(Dataset):
    """
    Map-style dataset for prediction/evaluation that loads voxels
    for specific domain-residue pairs.
    """
    def __init__(self, 
                 samples_to_load: List[Tuple[str, str]],
                 voxel_hdf5_path: str,
                 expected_channels: int = INPUT_CHANNELS,
                 target_shape_chw: Tuple[int, ...] = TARGET_SHAPE):
        """
        Initialize prediction dataset.
        
        Args:
            samples_to_load: List of (domain_id, resid_str) tuples to load
            voxel_hdf5_path: Path to HDF5 file with voxel data
            expected_channels: Number of channels in voxel data
            target_shape_chw: Expected shape of processed voxels
        """
        super().__init__()
        self.voxel_hdf5_path = Path(voxel_hdf5_path).resolve()
        self.expected_channels = expected_channels
        self.target_shape = target_shape_chw
        
        # Load and filter samples
        self.samples = []
        self.voxel_data = {}
        
        logger.info(f"PredictionDataset: Loading {len(samples_to_load)} samples from HDF5...")
        
        # Pre-load and validate all samples
        self._preload_samples(samples_to_load)
        
        logger.info(f"PredictionDataset: Successfully loaded {len(self.samples)} valid samples.")

    def _preload_samples(self, samples_to_load: List[Tuple[str, str]]):
        """
        Pre-load and validate voxel data for specified samples.
        
        Args:
            samples_to_load: List of (domain_id, resid_str) tuples to load
        """
        # Group by domain for efficient loading
        domain_to_residues = defaultdict(list)
        for domain_id, resid_str in samples_to_load:
            domain_to_residues[domain_id].append(resid_str)
            
        # Load domains
        valid_count = 0
        with h5py.File(self.voxel_hdf5_path, 'r') as h5_file:
            for domain_id, residues in domain_to_residues.items():
                domain_data = load_process_domain_from_handle(
                    h5_file, domain_id,
                    expected_channels=self.expected_channels,
                    target_shape_chw=self.target_shape
                )
                
                # Add valid samples to the dataset
                for resid_str in residues:
                    if resid_str in domain_data:
                        self.samples.append((domain_id, resid_str))
                        self.voxel_data[(domain_id, resid_str)] = domain_data[resid_str]
                        valid_count += 1
                        
        logger.info(f"PredictionDataset: Loaded {valid_count}/{len(samples_to_load)} valid voxel arrays.")

    def __len__(self):
        """Return number of samples in the dataset."""
        return len(self.samples)

    def __getitem__(self, idx):
        """
        Get a sample by index.
        
        Args:
            idx: Sample index
            
        Returns:
            Tuple of (domain_id, resid_str, voxel_tensor)
        """
        domain_id, resid_str = self.samples[idx]
        voxel_array = self.voxel_data[(domain_id, resid_str)]
        voxel_tensor = torch.from_numpy(voxel_array)
        return domain_id, resid_str, voxel_tensor

# --- DataLoader helper functions ---
def worker_init_fn(worker_id):
    """
    Worker initialization function for DataLoader.
    
    Args:
        worker_id: ID of the worker being initialized
    """
    worker_info = get_worker_info()
    if worker_info:
        # Set worker-specific random seed for reproducibility
        seed = worker_info.seed % 2**32  # Ensure seed is in valid range
        np.random.seed(seed)
        logger.debug(f"Worker {worker_id}: Initialized with NumPy seed {seed}")
    else:
        logger.warning(f"Worker {worker_id}: Could not get worker_info for seeding.")

def simple_collate_fn(batch):
    """
    Collate function for DataLoader that handles None values and shape mismatches.
    
    Args:
        batch: Batch of samples to collate
        
    Returns:
        Dictionary of batched tensors or None if batch is empty
    """
    # Filter out None values
    batch = [item for item in batch if item is not None]
    if not batch:
        logger.warning("Collate fn received empty batch.")
        return None
        
    try:
        return {
            'voxels': torch.stack([item['voxels'] for item in batch]),
            'scaled_temps': torch.stack([item['scaled_temps'] for item in batch]),
            'targets': torch.stack([item['targets'] for item in batch])
        }
    except RuntimeError as e:
        logger.error(f"Collation error (shape mismatch?): {e}")
        return None
    except Exception as e:
        logger.error(f"Unexpected collation error: {e}", exc_info=True)
        return None

# --- Preprocessing Helpers ---
def load_aggregated_rmsf_data(aggregated_rmsf_file: str) -> pd.DataFrame:
    """
    Load and validate aggregated RMSF data from CSV.
    
    Args:
        aggregated_rmsf_file: Path to CSV file with RMSF data
        
    Returns:
        DataFrame with validated RMSF data
        
    Raises:
        FileNotFoundError: If RMSF file not found
        Exception: For other loading errors
    """
    rmsf_file = Path(aggregated_rmsf_file).resolve()
    logger.info(f"Loading RMSF data from: {rmsf_file}")
    
    if not rmsf_file.exists():
        raise FileNotFoundError(f"RMSF file not found: {rmsf_file}")
        
    try:
        # Define expected dtypes for optimized loading
        dtype_spec = {
            'domain_id': str, 
            'resid': 'Int64', 
            'temperature_feature': float, 
            'target_rmsf': float
        }
        
        # Load CSV with optimized settings
        rmsf_df = pd.read_csv(rmsf_file, dtype=dtype_spec, low_memory=False)
        logger.info(f"Loaded {len(rmsf_df)} rows with optimized types")
        
        # Validate and clean data
        from voxelflex.data.validators import validate_aggregated_rmsf_data
        return validate_aggregated_rmsf_data(rmsf_df)
    except Exception as e:
        logger.exception(f"Failed to read RMSF CSV: {e}")
        raise

def create_master_rmsf_lookup(rmsf_df: pd.DataFrame) -> Dict[Tuple[str, int], List[Tuple[float, float]]]:
    """
    Create lookup mapping (domain_id, resid) to list of (temperature, rmsf) pairs.
    
    Args:
        rmsf_df: DataFrame with RMSF data
        
    Returns:
        Dictionary mapping (domain_id, resid) to [(temp1, rmsf1), (temp2, rmsf2), ...]
        
    Raises:
        ValueError: If required columns are missing
    """
    logger.info("Creating RMSF lookup...")
    t0 = time.time()
    
    # Check required columns
    required = ['domain_id', 'resid', 'temperature_feature', 'target_rmsf']
    if not all(c in rmsf_df.columns for c in required):
        raise ValueError(f"Missing required columns: {required}")
    
    # Group by domain and residue for efficient lookup
    lookup = defaultdict(list)
    grouped = rmsf_df.groupby(['domain_id', 'resid'])
    
    for name, group in grouped:
        try:
            # Create list of (temp, rmsf) tuples
            lookup[(str(name[0]), int(name[1]))] = list(zip(
                group['temperature_feature'].astype(float),
                group['target_rmsf'].astype(float)
            ))
        except (ValueError, TypeError):
            logger.warning(f"Skipping RMSF lookup entry due to conversion error for {name}")
    
    # Handle base domain IDs (without chain/model suffix)
    base_lookup = {}
    added_count = 0
    
    for k, v in lookup.items():
        base = k[0].split('_')[0]
        if base != k[0]:
            base_key = (base, k[1])
            if base_key not in lookup and base_key not in base_lookup:
                base_lookup[base_key] = v
                added_count += 1
    
    # Merge base lookups into main lookup
    lookup.update(base_lookup)
    
    logger.info(f"RMSF lookup created: {len(lookup)} keys ({added_count} base names) in {time.time()-t0:.2f}s")
    return dict(lookup)

def create_domain_mapping(voxel_domain_keys: List[str], rmsf_domain_ids: List[str]) -> Dict[str, str]:
    """
    Create mapping from HDF5 domain keys to RMSF domain IDs.
    
    Args:
        voxel_domain_keys: List of domain keys from HDF5 file
        rmsf_domain_ids: List of domain IDs from RMSF data
        
    Returns:
        Dictionary mapping HDF5 keys to RMSF IDs
    """
    logger.info("Creating domain mapping...")
    
    if not voxel_domain_keys:
        logger.warning("Voxel keys empty.")
        return {}
        
    if not rmsf_domain_ids:
        logger.warning("RMSF IDs empty.")
        return {}
    
    # Create sets for efficient lookups
    v_set = set(voxel_domain_keys)
    r_set = set(rmsf_domain_ids)
    
    mapping = {}
    matches = {'exact': 0, 'base': 0}
    
    # Create mapping from base domain IDs to full IDs
    r_base_map = {r.split('_')[0]: r for r in rmsf_domain_ids}
    
    # Match each HDF5 key to RMSF ID
    for h_key in v_set:
        # Try exact match first
        if h_key in r_set:
            mapping[h_key] = h_key
            matches['exact'] += 1
            continue
            
        # Try base domain match
        base_h = h_key.split('_')[0]
        if base_h in r_set:
            mapping[h_key] = base_h
            matches['base'] += 1
        elif base_h in r_base_map:
            mapping[h_key] = r_base_map[base_h]
            matches['base'] += 1
    
    # Log mapping statistics
    total = len(mapping)
    cov = (total/len(voxel_domain_keys))*100 if voxel_domain_keys else 0
    logger.info(f"Domain mapping: {total}/{len(voxel_domain_keys)} keys ({cov:.1f}%)")
    logger.info(f"  Matches: Exact={matches['exact']}, Base={matches['base']}")
    
    return mapping

def load_list_from_file(file_path: str) -> List[str]:
    """
    Load a list of strings from a text file, one item per line.
    
    Args:
        file_path: Path to text file
        
    Returns:
        List of strings from file
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        logger.warning(f"File not found: {file_path}. Returning empty list.")
        return []
        
    try:
        with open(file_path, 'r') as f:
            # Read lines, strip whitespace, and filter out empty lines
            items = [line.strip() for line in f if line.strip()]
        
        logger.debug(f"Loaded {len(items)} items from: {file_path}")
        return items
    except Exception as e:
        logger.error(f"Failed to load list from {file_path}: {e}")
        return []  # Return empty list on error
==========================================================
===== FILE: src/voxelflex/models/cnn_models.py =====
==========================================================

"""
CNN models for VoxelFlex (Temperature-Aware).

Contains 3D CNN architectures adapted for temperature-aware RMSF prediction,
including DenseNet3D, DilatedResNet3D, and MultipathRMSFNet.
"""

import logging
from typing import List, Tuple, Dict, Any, Optional, Union, Sequence

import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict

# Use centralized logger
logger = logging.getLogger("voxelflex.models")

# --- DenseNet Building Blocks ---

class _DenseLayer(nn.Module):
    """
    Single layer within a DenseBlock that implements the dense connectivity pattern.
    
    Each layer consists of:
    1. Batch Normalization -> ReLU -> 1x1 Conv (bottleneck)
    2. Batch Normalization -> ReLU -> 3x3 Conv
    
    The layer connects to all preceding layers via concatenation.
    """
    def __init__(
        self, 
        num_input_features: int, 
        growth_rate: int, 
        bn_size: int, 
        drop_rate: float, 
        memory_efficient: bool = False
    ):
        """
        Initialize DenseLayer.
        
        Args:
            num_input_features: Number of input channels
            growth_rate: Growth rate (k) - how many features each layer adds
            bn_size: Bottleneck size multiplier for 1x1 conv
            drop_rate: Dropout rate after each dense layer
            memory_efficient: Whether to use checkpointing to save memory
        """
        super().__init__()
        self.norm1: nn.BatchNorm3d
        self.add_module('norm1', nn.BatchNorm3d(num_input_features))
        self.relu1: nn.ReLU
        self.add_module('relu1', nn.ReLU(inplace=True))
        self.conv1: nn.Conv3d
        self.add_module('conv1', nn.Conv3d(num_input_features, bn_size * growth_rate, 
                                          kernel_size=1, stride=1, bias=False))
        self.norm2: nn.BatchNorm3d
        self.add_module('norm2', nn.BatchNorm3d(bn_size * growth_rate))
        self.relu2: nn.ReLU
        self.add_module('relu2', nn.ReLU(inplace=True))
        self.conv2: nn.Conv3d
        self.add_module('conv2', nn.Conv3d(bn_size * growth_rate, growth_rate,
                                          kernel_size=3, stride=1, padding=1, bias=False))
        self.drop_rate = float(drop_rate)
        self.memory_efficient = memory_efficient

    def bn_function(self, inputs: List[torch.Tensor]) -> torch.Tensor:
        """
        Bottleneck function: BN->ReLU->Conv(1x1)
        
        Args:
            inputs: List of tensors to concatenate
            
        Returns:
            Output of bottleneck convolution
        """
        concated_features = torch.cat(inputs, 1)
        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))
        return bottleneck_output

    def forward(self, input_features: Union[torch.Tensor, List[torch.Tensor]]) -> torch.Tensor:
        """
        Forward pass through dense layer.
        
        Args:
            input_features: Input tensor or list of tensors
            
        Returns:
            New features tensor
        """
        # Ensure input is treated as a list for concatenation
        if isinstance(input_features, torch.Tensor):
            prev_features = [input_features]
        else:
            prev_features = input_features

        bottleneck_output = self.bn_function(prev_features)
        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))
        
        if self.drop_rate > 0:
            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)
            
        return new_features


class _DenseBlock(nn.ModuleDict):
    """
    Dense Block containing multiple densely connected layers.
    
    Each layer receives feature maps from all preceding layers,
    creating a dense connectivity pattern.
    """
    def __init__(
        self,
        num_layers: int,
        num_input_features: int,
        bn_size: int,
        growth_rate: int,
        drop_rate: float,
        memory_efficient: bool = False,
    ):
        """
        Initialize DenseBlock.
        
        Args:
            num_layers: Number of dense layers in this block
            num_input_features: Number of input channels to the block
            bn_size: Bottleneck size multiplier
            growth_rate: Growth rate (k) - new features per layer
            drop_rate: Dropout rate
            memory_efficient: Whether to use checkpointing
        """
        super().__init__()
        for i in range(num_layers):
            # Each layer takes all previous feature maps as input
            layer = _DenseLayer(
                num_input_features + i * growth_rate,
                growth_rate=growth_rate,
                bn_size=bn_size,
                drop_rate=drop_rate,
                memory_efficient=memory_efficient,
            )
            self.add_module('denselayer%d' % (i + 1), layer)

    def forward(self, init_features: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through dense block.
        
        Args:
            init_features: Initial input tensor
            
        Returns:
            Concatenated output of all layers
        """
        features = [init_features]
        
        # Pass through each layer, collecting outputs
        for layer in self.values():
            new_features = layer(features)
            features.append(new_features)
            
        # Concatenate all features
        return torch.cat(features, 1)


class _Transition(nn.Sequential):
    """
    Transition layer between dense blocks.
    
    Reduces feature map size and number of channels using:
    1. 1x1 convolution to reduce channels
    2. 2x2 average pooling to reduce spatial dimensions
    """
    def __init__(self, num_input_features: int, num_output_features: int) -> None:
        """
        Initialize transition layer.
        
        Args:
            num_input_features: Number of input channels
            num_output_features: Number of output channels (typically num_input_features // 2)
        """
        super().__init__()
        self.add_module('norm', nn.BatchNorm3d(num_input_features))
        self.add_module('relu', nn.ReLU(inplace=True))
        self.add_module('conv', nn.Conv3d(num_input_features, num_output_features, 
                                         kernel_size=1, stride=1, bias=False))
        self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2))


# --- Temperature-Aware Models ---

class DenseNet3DRegression(nn.Module):
    """
    3D DenseNet architecture adapted for temperature-aware RMSF regression.
    
    Features:
    - 3D convolutional DenseNet backbone for voxel processing
    - Temperature feature integration at the final regression layer
    - Configurable depth and width parameters
    """
    def __init__(
        self,
        input_channels: int = 5,
        growth_rate: int = 16,
        block_config: Tuple[int, ...] = (4, 4, 4),
        num_init_features: int = 32,
        bn_size: int = 4,
        dropout_rate: float = 0.3,
        memory_efficient: bool = False
    ):
        """
        Initialize DenseNet3DRegression.
        
        Args:
            input_channels: Number of input voxel channels
            growth_rate: Growth rate (k) for dense layers
            block_config: Tuple containing number of layers in each dense block
            num_init_features: Number of filters in initial convolution
            bn_size: Bottleneck size multiplier for dense layers
            dropout_rate: Dropout rate for dense layers and regression head
            memory_efficient: Whether to use checkpointing to save memory
        """
        super().__init__()
        logger.info("Initializing DenseNet3DRegression model...")

        # --- Initial Convolution ---
        self.features = nn.Sequential(OrderedDict([
            ('conv0', nn.Conv3d(input_channels, num_init_features, kernel_size=7, 
                                stride=2, padding=3, bias=False)),
            ('norm0', nn.BatchNorm3d(num_init_features)),
            ('relu0', nn.ReLU(inplace=True)),
            ('pool0', nn.MaxPool3d(kernel_size=3, stride=2, padding=1)),
        ]))
        logger.info(f"  Initial Conv: {input_channels} -> {num_init_features} features")

        # --- Dense Blocks ---
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            # Add dense block
            block = _DenseBlock(
                num_layers=num_layers,
                num_input_features=num_features,
                bn_size=bn_size,
                growth_rate=growth_rate,
                drop_rate=dropout_rate,
                memory_efficient=memory_efficient,
            )
            self.features.add_module('denseblock%d' % (i + 1), block)
            num_features = num_features + num_layers * growth_rate
            logger.info(f"  Dense Block {i+1}: {num_layers} layers, Output features: {num_features}")

            # Add transition layer if not the last block
            if i != len(block_config) - 1:
                num_output_features = num_features // 2
                trans = _Transition(num_input_features=num_features, 
                                   num_output_features=num_output_features)
                self.features.add_module('transition%d' % (i + 1), trans)
                num_features = num_output_features
                logger.info(f"  Transition {i+1}: Output features: {num_features}")

        # --- Final Batch Norm ---
        self.features.add_module('norm_final', nn.BatchNorm3d(num_features))

        # --- Global Pooling ---
        self.global_avg_pool = nn.AdaptiveAvgPool3d(1)

        # --- Temperature-Aware Regression Head ---
        # Input size = features from DenseNet + 1 (scaled temperature)
        self.classifier_input_features = num_features + 1
        
        # Define regression head layers
        self.classifier = nn.Sequential(
            nn.Linear(self.classifier_input_features, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, 1)  # Final RMSF prediction
        )

        logger.info(f"  Regression Head Input Features: {self.classifier_input_features}")

        # --- Weight Initialization ---
        self._initialize_weights()
        logger.info("DenseNet3DRegression initialized.")

    def _initialize_weights(self):
        """Initialize model weights with appropriate distributions."""
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, voxel_input: torch.Tensor, scaled_temp: torch.Tensor) -> torch.Tensor:
        """
        Forward pass incorporating voxel data and scaled temperature.
        
        Args:
            voxel_input: Tensor of shape (Batch, Channels, D, H, W)
            scaled_temp: Tensor of shape (Batch, 1) with scaled temperature values [0, 1]
            
        Returns:
            Tensor of shape (Batch,) with predicted RMSF values
        """
        # Process voxel data through DenseNet features
        features = self.features(voxel_input)
        out = F.relu(features, inplace=True)  # Final ReLU after last BatchNorm
        out = self.global_avg_pool(out)
        voxel_features = torch.flatten(out, 1)  # Shape: (Batch, num_features)

        # Ensure scaled_temp has shape (Batch, 1)
        if scaled_temp.ndim == 1:
            scaled_temp = scaled_temp.unsqueeze(1)
        elif scaled_temp.shape[1] != 1:
            raise ValueError(f"scaled_temp input must have shape (Batch, 1), but got {scaled_temp.shape}")

        # Concatenate voxel features and scaled temperature
        combined_features = torch.cat((voxel_features, scaled_temp), dim=1)

        # Pass through regression head
        predictions = self.classifier(combined_features)

        return predictions.squeeze(1)  # Return shape (Batch,)


# --- Residual Block (Used by DilatedResNet3D) ---
class ResidualBlock3D(nn.Module):
    """
    3D Residual block with optional dilation and dropout.
    
    Features:
    - 3D convolutions with dilation for increased receptive field
    - Skip connection for residual learning
    - Optional dropout for regularization
    """
    def __init__(self, in_channels: int, out_channels: int, dilation: int = 1, dropout_rate: float = 0.0):
        """
        Initialize ResidualBlock3D.
        
        Args:
            in_channels: Number of input channels
            out_channels: Number of output channels
            dilation: Dilation factor for convolutions
            dropout_rate: Dropout rate after first activation
        """
        super().__init__()
        padding = dilation  # Standard padding for dilated convolution
        
        # Main path
        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, 
                              padding=padding, dilation=dilation, bias=False)
        self.bn1 = nn.BatchNorm3d(out_channels)
        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, 
                              padding=padding, dilation=dilation, bias=False)
        self.bn2 = nn.BatchNorm3d(out_channels)
        
        # Dropout layer (or Identity if no dropout)
        self.dropout = nn.Dropout3d(dropout_rate) if dropout_rate > 0 else nn.Identity()

        # Skip connection: Adjust channels if necessary
        self.skip = nn.Sequential()
        if in_channels != out_channels:
            self.skip = nn.Sequential(
                nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),
                nn.BatchNorm3d(out_channels)
            )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through residual block.
        
        Args:
            x: Input tensor
            
        Returns:
            Output tensor after residual connection
        """
        residual = self.skip(x)
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out, inplace=True)
        out = self.dropout(out)  # Apply dropout after first activation
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        out = out + residual  # Add skip connection BEFORE final activation
        out = F.relu(out, inplace=True)
        
        return out


class DilatedResNet3D(nn.Module):
    """
    Dilated ResNet 3D architecture adapted for temperature-aware RMSF prediction.
    
    Features:
    - 3D convolutional backbone with dilated convolutions
    - Growing dilation pattern to increase receptive field efficiently
    - Temperature feature integration at the final regression layer
    """
    def __init__(
        self,
        input_channels: int = 5,
        base_filters: int = 32,
        channel_growth_rate: float = 1.5,
        num_residual_blocks: int = 4,
        dropout_rate: float = 0.3
    ):
        """
        Initialize DilatedResNet3D.
        
        Args:
            input_channels: Number of input voxel channels
            base_filters: Initial number of filters
            channel_growth_rate: Factor by which to increase channels between blocks
            num_residual_blocks: Number of residual blocks
            dropout_rate: Dropout rate for residual blocks and regression head
        """
        super().__init__()
        logger.info("Initializing DilatedResNet3D model...")

        # Initial convolution
        self.conv1 = nn.Conv3d(input_channels, base_filters, kernel_size=7, 
                              stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm3d(base_filters)
        self.pool1 = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)

        # Calculate channel sizes for each block
        channels = [base_filters]
        for i in range(num_residual_blocks):
            # Ensure channel count increases, minimum of +1 channel
            next_channels = max(channels[-1] + 1, int(channels[-1] * channel_growth_rate))
            channels.append(next_channels)

        # Create residual blocks
        self.res_blocks = nn.ModuleList()
        for i in range(num_residual_blocks):
            # Use a cyclic dilation pattern: 1, 2, 4, 1...
            dilation = 2**(i % 3)  
            block = ResidualBlock3D(channels[i], channels[i+1], 
                                   dilation=dilation, 
                                   dropout_rate=dropout_rate)
            self.res_blocks.append(block)
            logger.info(f"  Res Block {i+1}: {channels[i]}->{channels[i+1]} filters, Dilation={dilation}")

        # Global pooling
        self.global_avg_pool = nn.AdaptiveAvgPool3d(1)

        # Temperature-Aware Regression Head
        self.cnn_output_features = channels[-1]
        self.classifier_input_features = self.cnn_output_features + 1  # +1 for temp
        self.classifier = nn.Sequential(
            nn.Linear(self.classifier_input_features, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, 1)  # Final RMSF prediction
        )

        logger.info(f"  Regression Head Input Features: {self.classifier_input_features}")
        self._initialize_weights()
        logger.info("DilatedResNet3D initialized.")

    def _initialize_weights(self):
        """Initialize model weights with appropriate distributions."""
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, voxel_input: torch.Tensor, scaled_temp: torch.Tensor) -> torch.Tensor:
        """
        Forward pass incorporating voxel data and scaled temperature.
        
        Args:
            voxel_input: Tensor of shape (Batch, Channels, D, H, W)
            scaled_temp: Tensor of shape (Batch, 1) with scaled temperature values [0, 1]
            
        Returns:
            Tensor of shape (Batch,) with predicted RMSF values
        """
        # Initial convolution
        x = self.conv1(voxel_input)
        x = self.bn1(x)
        x = F.relu(x, inplace=True)
        x = self.pool1(x)

        # Residual blocks
        for block in self.res_blocks:
            x = block(x)

        # Global average pooling
        x = self.global_avg_pool(x)
        voxel_features = torch.flatten(x, 1)  # Shape: (Batch, cnn_output_features)

        # Prepare and concatenate temperature
        if scaled_temp.ndim == 1:
            scaled_temp = scaled_temp.unsqueeze(1)  # Ensure (Batch, 1)
        elif scaled_temp.shape[1] != 1:
            raise ValueError(f"scaled_temp input must have shape (Batch, 1), but got {scaled_temp.shape}")
            
        combined_features = torch.cat((voxel_features, scaled_temp), dim=1)

        # Pass through regression head
        predictions = self.classifier(combined_features)

        return predictions.squeeze(1)


class MultipathRMSFNet(nn.Module):
    """
    Multi-path 3D CNN architecture adapted for temperature-aware RMSF prediction.
    
    Features:
    - Multiple parallel paths with different kernel sizes to capture features at various scales
    - Feature fusion through concatenation and 1x1 convolution
    - Temperature feature integration at the final regression layer
    """
    def __init__(
        self,
        input_channels: int = 5,
        base_filters: int = 32,
        channel_growth_rate: float = 1.5,
        num_residual_blocks: int = 3,  # Number of blocks *per path* after initial pooling
        dropout_rate: float = 0.3
    ):
        """
        Initialize MultipathRMSFNet.
        
        Args:
            input_channels: Number of input voxel channels
            base_filters: Initial number of filters
            channel_growth_rate: Factor by which to increase channels after fusion
            num_residual_blocks: Number of convolutional blocks in each path
            dropout_rate: Dropout rate for regularization
        """
        super().__init__()
        logger.info("Initializing MultipathRMSFNet model...")

        c1 = base_filters
        # Channel size within paths (after first conv in path)
        c2 = max(c1 + 1, int(c1 * channel_growth_rate))
        # Channel size after fusion
        c3 = max(c2*3 + 1, int(c2 * 3 * channel_growth_rate / 2))  # Grow channels after fusion

        # Initial shared convolution
        self.conv1 = nn.Conv3d(input_channels, c1, kernel_size=7, 
                              stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm3d(c1)
        self.pool1 = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)

        # Multi-path branches with different kernel sizes
        self.path1 = self._create_path(c1, c2, kernel_size=3, 
                                      blocks=num_residual_blocks, 
                                      dropout_rate=dropout_rate)
        self.path2 = self._create_path(c1, c2, kernel_size=5, 
                                      blocks=num_residual_blocks, 
                                      dropout_rate=dropout_rate)
        self.path3 = self._create_path(c1, c2, kernel_size=7, 
                                      blocks=num_residual_blocks, 
                                      dropout_rate=dropout_rate)
        logger.info(f"  Paths created: {num_residual_blocks} blocks each, output channels={c2}")

        # Fusion layer (adjusts channels after concatenation)
        self.fusion_conv = nn.Conv3d(c2 * 3, c3, kernel_size=1, bias=False)  # 1x1 convolution for fusion
        self.fusion_bn = nn.BatchNorm3d(c3)
        logger.info(f"  Fusion layer: {c2*3} -> {c3} channels")

        self.global_avg_pool = nn.AdaptiveAvgPool3d(1)

        # Temperature-Aware Regression Head
        self.cnn_output_features = c3
        self.classifier_input_features = self.cnn_output_features + 1  # +1 for temp
        self.classifier = nn.Sequential(
            nn.Linear(self.classifier_input_features, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, 1)  # Final RMSF prediction
        )

        logger.info(f"  Regression Head Input Features: {self.classifier_input_features}")
        self._initialize_weights()
        logger.info("MultipathRMSFNet initialized.")

    def _create_path(
        self, in_channels: int, path_channels: int, kernel_size: int, blocks: int, dropout_rate: float
    ) -> nn.Sequential:
        """
        Create a single path for the Multipath network.
        
        Args:
            in_channels: Number of input channels
            path_channels: Number of output channels for the path
            kernel_size: Size of convolutional kernels in this path
            blocks: Number of convolutional blocks in this path
            dropout_rate: Dropout rate for regularization
            
        Returns:
            Sequential module containing the path layers
        """
        layers = []
        padding = kernel_size // 2

        # First convolution in the path
        layers.append(nn.Conv3d(in_channels, path_channels, kernel_size=kernel_size, 
                               padding=padding, bias=False))
        layers.append(nn.BatchNorm3d(path_channels))
        layers.append(nn.ReLU(inplace=True))

        # Additional blocks within the path
        current_channels = path_channels
        for _ in range(blocks - 1):  # If blocks=3, adds 2 more conv layers
            layers.append(nn.Conv3d(current_channels, current_channels, 
                                   kernel_size=kernel_size, padding=padding, bias=False))
            layers.append(nn.BatchNorm3d(current_channels))
            layers.append(nn.ReLU(inplace=True))
            if dropout_rate > 0:
                layers.append(nn.Dropout3d(dropout_rate))

        return nn.Sequential(*layers)

    def _initialize_weights(self):
        """Initialize model weights with appropriate distributions."""
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, voxel_input: torch.Tensor, scaled_temp: torch.Tensor) -> torch.Tensor:
        """
        Forward pass incorporating voxel data and scaled temperature.
        
        Args:
            voxel_input: Tensor of shape (Batch, Channels, D, H, W)
            scaled_temp: Tensor of shape (Batch, 1) with scaled temperature values [0, 1]
            
        Returns:
            Tensor of shape (Batch,) with predicted RMSF values
        """
        # Initial shared convolution & pooling
        x = self.conv1(voxel_input)
        x = self.bn1(x)
        x = F.relu(x, inplace=True)
        x_pooled = self.pool1(x)  # Pool once after initial conv

        # Multi-path processing
        out1 = self.path1(x_pooled)
        out2 = self.path2(x_pooled)
        out3 = self.path3(x_pooled)

        # Concatenate path outputs along the channel dimension
        out_cat = torch.cat([out1, out2, out3], dim=1)

        # Fusion layer
        fused = self.fusion_conv(out_cat)
        fused = self.fusion_bn(fused)
        fused = F.relu(fused, inplace=True)

        # Global average pooling
        pooled = self.global_avg_pool(fused)
        voxel_features = torch.flatten(pooled, 1)  # Shape: (Batch, cnn_output_features)

        # Prepare and concatenate temperature
        if scaled_temp.ndim == 1:
            scaled_temp = scaled_temp.unsqueeze(1)  # Ensure (Batch, 1)
        elif scaled_temp.shape[1] != 1:
            raise ValueError(f"scaled_temp input must have shape (Batch, 1), but got {scaled_temp.shape}")
            
        combined_features = torch.cat((voxel_features, scaled_temp), dim=1)

        # Pass through regression head
        predictions = self.classifier(combined_features)

        return predictions.squeeze(1)


# --- Helper function to get model instance ---

def get_model(config: Dict[str, Any], input_shape: Tuple[int, ...]) -> nn.Module:
    """
    Get a model instance based on the configuration.
    
    Args:
        config: Model configuration dictionary section (config['model'])
        input_shape: Shape of a single voxel input (Channels, D, H, W). Used for validation.
        
    Returns:
        Initialized PyTorch model
        
    Raises:
        ValueError: If architecture is invalid or required parameters are missing
    """
    architecture = config.get('architecture')
    input_channels = config.get('input_channels', 5)  # Get from config or default
    dropout_rate = config.get('dropout_rate', 0.3)

    # Verify input channels match data shape (first dimension)
    if input_shape[0] != input_channels:
        logger.warning(f"Model 'input_channels' in config ({input_channels}) does not match "
                      f"detected data channels ({input_shape[0]}). Using detected data channels value.")
        input_channels = input_shape[0]  # Override config value

    logger.info(f"Creating model architecture: {architecture} with {input_channels} input channels.")

    if architecture == "densenet3d_regression":
        densenet_cfg = config.get('densenet', {})
        if not densenet_cfg:
            logger.warning("DenseNet config section ('model.densenet') not found or empty in config. Using defaults.")
            
        return DenseNet3DRegression(
            input_channels=input_channels,
            growth_rate=densenet_cfg.get('growth_rate', 16),
            block_config=tuple(densenet_cfg.get('block_config', [4, 4, 4])),
            num_init_features=densenet_cfg.get('num_init_features', 32),
            bn_size=densenet_cfg.get('bn_size', 4),
            dropout_rate=dropout_rate
        )
        
    elif architecture == "dilated_resnet3d":
        return DilatedResNet3D(
            input_channels=input_channels,
            base_filters=config.get('base_filters', 32),
            channel_growth_rate=config.get('channel_growth_rate', 1.5),
            num_residual_blocks=config.get('num_residual_blocks', 4),
            dropout_rate=dropout_rate
        )
        
    elif architecture == "multipath_rmsf_net":
        return MultipathRMSFNet(
            input_channels=input_channels,
            base_filters=config.get('base_filters', 32),
            channel_growth_rate=config.get('channel_growth_rate', 1.5),
            num_residual_blocks=config.get('num_residual_blocks', 3),
            dropout_rate=dropout_rate
        )
        
    else:
        raise ValueError(f"Unknown architecture specified in config: {architecture}. "
                        f"Valid options: 'densenet3d_regression', 'dilated_resnet3d', 'multipath_rmsf_net'")
==========================================================
===== FILE: src/voxelflex/utils/file_utils.py =====
==========================================================

"""
File system utilities for VoxelFlex.

Provides functions for file path resolution, directory creation, and JSON handling.
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Union, Optional
import os

logger = logging.getLogger("voxelflex.utils.file")

def resolve_path(path: Union[str, Path]) -> str:
    """
    Resolves a path relative to CWD or user home.
    
    Args:
        path: Input path string or Path object
        
    Returns:
        Resolved absolute path as string
    """
    if path is None:
        return None
        
    p = Path(str(path)).expanduser()
    
    # Resolve relative paths based on the current working directory
    if not p.is_absolute():
        p = Path.cwd() / p
        
    # Return absolute path
    try:
        # Use the absolute path to avoid issues with non-existent path components
        return str(p.absolute())
    except Exception as e:
        logger.warning(f"Could not fully resolve path {p}: {e}. Returning absolute path.")
        return str(p.absolute())


def ensure_dir(dir_path: Union[str, Path]) -> None:
    """
    Ensure a directory exists, creating it if necessary.
    
    Args:
        dir_path: Directory path to ensure exists
        
    Raises:
        NotADirectoryError: If path exists but is not a directory
    """
    if dir_path:
        path = Path(dir_path)
        if not path.exists():
            logger.debug(f"Creating directory: {path}")
            path.mkdir(parents=True, exist_ok=True)
        elif not path.is_dir():
            raise NotADirectoryError(f"Path exists but is not a directory: {path}")


def save_json(data: Union[Dict, List], file_path: Union[str, Path], indent: int = 4) -> None:
    """
    Save dictionary or list to JSON file.
    
    Args:
        data: Data to save (must be JSON serializable)
        file_path: Path where JSON will be saved
        indent: Indentation level for pretty printing
        
    Raises:
        Exception: If saving fails
    """
    file_path = Path(file_path)
    ensure_dir(file_path.parent)
    try:
        with open(file_path, 'w') as f:
            json.dump(data, f, indent=indent)
        logger.debug(f"Saved JSON data to: {file_path}")
    except Exception as e:
        logger.error(f"Failed to save JSON to {file_path}: {e}")
        raise


def load_json(file_path: Union[str, Path]) -> Union[Dict, List]:
    """
    Load dictionary or list from JSON file.
    
    Args:
        file_path: Path to JSON file
        
    Returns:
        Loaded data structure from JSON
        
    Raises:
        FileNotFoundError: If file doesn't exist
        ValueError: If file contains invalid JSON
        Exception: For other loading errors
    """
    file_path = Path(file_path)
    if not file_path.exists():
        raise FileNotFoundError(f"JSON file not found: {file_path}")
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        logger.debug(f"Loaded JSON data from: {file_path}")
        return data
    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON format in {file_path}: {e}")
        raise ValueError(f"Invalid JSON format in {file_path}") from e
    except Exception as e:
        logger.error(f"Failed to load JSON from {file_path}: {e}")
        raise


def load_list_from_file(file_path: Union[str, Path]) -> List[str]:
    """
    Load a list of strings from a file, one item per line.
    
    Args:
        file_path: Path to text file
        
    Returns:
        List of strings from file (empty list if file not found or error)
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        logger.warning(f"File not found for loading list: {file_path}. Returning empty list.")
        return []
        
    try:
        with open(file_path, 'r') as f:
            # Read lines, strip whitespace, and filter out empty lines
            items = [line.strip() for line in f if line.strip()]
        logger.debug(f"Loaded {len(items)} items from list file: {file_path}")
        return items
    except Exception as e:
        logger.error(f"Failed to load list from {file_path}: {e}")
        return []  # Return empty list on error


def save_list_to_file(data: List[str], file_path: Union[str, Path]) -> None:
    """
    Save a list of strings to a file, one item per line.
    
    Args:
        data: List of strings to save
        file_path: Path where file will be saved
        
    Raises:
        Exception: If saving fails
    """
    file_path = Path(file_path)
    ensure_dir(file_path.parent)
    try:
        with open(file_path, 'w') as f:
            for item in data:
                f.write(f"{item}\n")
        logger.debug(f"Saved {len(data)} items to list file: {file_path}")
    except Exception as e:
        logger.error(f"Failed to save list to {file_path}: {e}")
        raise


def remove_file(file_path: Union[str, Path], ignore_errors: bool = False) -> bool:
    """
    Safely remove a file if it exists.
    
    Args:
        file_path: Path to file to remove
        ignore_errors: Whether to ignore errors during removal
        
    Returns:
        True if file was removed or didn't exist, False on error when ignore_errors=False
    """
    file_path = Path(file_path)
    if not file_path.exists():
        return True
        
    try:
        file_path.unlink()
        logger.debug(f"Removed file: {file_path}")
        return True
    except Exception as e:
        if ignore_errors:
            logger.debug(f"Ignored error removing file {file_path}: {e}")
            return True
        else:
            logger.error(f"Failed to remove file {file_path}: {e}")
            return False


def is_file_empty(file_path: Union[str, Path]) -> bool:
    """
    Check if a file exists and is empty.
    
    Args:
        file_path: Path to file to check
        
    Returns:
        True if file doesn't exist or is empty, False otherwise
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        return True
        
    return file_path.stat().st_size == 0
==========================================================
===== FILE: src/voxelflex/utils/logging_utils.py =====
==========================================================

"""
Logging utilities for VoxelFlex.

Provides enhanced logging functionality, progress bars, and performance monitoring.
"""

import logging
import sys
import time
import psutil
import gc
from contextlib import contextmanager
from typing import Optional, Dict, Any, Union, Callable
import os
from datetime import datetime

import torch
from tqdm import tqdm

# --- Global Logger Setup ---

def setup_logging(
    log_file: Optional[str] = None,
    console_level: str = "INFO",
    file_level: str = "DEBUG",
    name: str = "voxelflex"  # Root logger name
) -> logging.Logger:
    """
    Configures logging for the entire application.
    
    Args:
        log_file: Path to the log file. If None, only console logging is enabled.
        console_level: Logging level for console output (e.g., "DEBUG", "INFO", "WARNING").
        file_level: Logging level for file output (e.g., "DEBUG", "INFO").
        name: Name of the root logger for the application.
        
    Returns:
        The configured root logger instance.
    """
    logger = logging.getLogger(name)
    # Prevent adding multiple handlers if called again
    if logger.hasHandlers():
        logger.handlers.clear()

    logger.setLevel(logging.DEBUG)  # Set root logger level to lowest (DEBUG)

    # --- Console Handler ---
    console_handler = logging.StreamHandler(sys.stdout)
    try:
        # Convert string level names to logging constants
        console_log_level_int = getattr(logging, console_level.upper(), logging.INFO)
    except AttributeError:
        print(f"Warning: Invalid console log level '{console_level}'. Defaulting to INFO.")
        console_log_level_int = logging.INFO
        
    console_handler.setLevel(console_log_level_int)
    console_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s', 
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    # --- File Handler ---
    if log_file:
        try:
            # Ensure log directory exists
            os.makedirs(os.path.dirname(log_file), exist_ok=True)
            
            # Convert string level names to logging constants
            file_log_level_int = getattr(logging, file_level.upper(), logging.DEBUG)
        except AttributeError:
            print(f"Warning: Invalid file log level '{file_level}'. Defaulting to DEBUG.")
            file_log_level_int = logging.DEBUG

        file_handler = logging.FileHandler(log_file, mode='a')  # Append mode
        file_handler.setLevel(file_log_level_int)
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s', 
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)
        logger.info(f"Logging to file: {log_file} (Level: {file_level.upper()})")

    logger.info(f"Console logging level set to: {console_level.upper()}")

    # --- Handle external libraries ---
    # Reduce verbosity of common libraries
    logging.getLogger("matplotlib").setLevel(logging.WARNING)
    logging.getLogger("h5py").setLevel(logging.WARNING)
    logging.getLogger("torch").setLevel(logging.WARNING)

    return logger


def get_logger(name: str) -> logging.Logger:
    """
    Get a logger instance, inheriting configuration from the root.
    
    Args:
        name: Logger name suffix (will be prefixed with "voxelflex.")
        
    Returns:
        Logger instance
    """
    return logging.getLogger(f"voxelflex.{name}")  # Use hierarchical naming


# --- Progress Bar ---
class EnhancedProgressBar(tqdm):
    """
    Custom tqdm progress bar with additional features.
    
    Features:
    - Customizable formatting
    - Optional timing and ETA
    - Memory usage display
    """
    def __init__(self, *args, stage_info: Optional[str] = None, **kwargs):
        """
        Initialize enhanced progress bar.
        
        Args:
            *args: Arguments to pass to tqdm
            stage_info: Optional stage information to display
            **kwargs: Keyword arguments to pass to tqdm
        """
        # Set some sensible defaults if not provided by the caller
        kwargs.setdefault('ncols', 100)  # Wider default
        kwargs.setdefault('bar_format', '{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]')
        
        # Store stage info for potential use
        self.stage_info = stage_info
        
        # Initialize the parent class
        super().__init__(*args, **kwargs)
        
        # Initialize memory tracking
        self.last_mem_check = 0
        self.mem_check_interval = 5  # seconds

    def update(self, n=1):
        """
        Update the progress bar.
        
        Args:
            n: Amount to increment the progress bar
        """
        super().update(n)
        
        # Optionally add memory usage in postfix
        current_time = time.time()
        if current_time - self.last_mem_check > self.mem_check_interval:
            try:
                mem_info = psutil.virtual_memory()
                self.set_postfix_str(f"Mem: {mem_info.percent:.1f}%", refresh=False)
                self.last_mem_check = current_time
            except:
                pass  # Ignore memory info errors

    def finish(self):
        """Close the progress bar."""
        # Ensure the bar is refreshed before closing to show final state
        self.refresh()
        self.close()


# --- Context Managers and Helpers ---

@contextmanager
def log_stage(stage_name: str, description: Optional[str] = None):
    """
    Logs the start and end of a processing stage, with timing.
    
    Args:
        stage_name: Name of the stage
        description: Optional description of the stage
        
    Yields:
        None
    """
    logger = get_logger("pipeline")  # Use a dedicated pipeline logger
    logger.info(f"--- Starting Stage: {stage_name} ---")
    if description:
        logger.info(f"  {description}")
    start_time = time.time()
    log_memory_usage(logger, level=logging.DEBUG)  # Log memory at start
    try:
        yield
    finally:
        duration = time.time() - start_time
        logger.info(f"--- Finished Stage: {stage_name} (Duration: {duration:.2f}s) ---")
        log_memory_usage(logger, level=logging.DEBUG)  # Log memory at end
        gc.collect()  # Force GC after stage


def log_section_header(logger_instance: logging.Logger, title: str):
    """
    Logs a formatted section header.
    
    Args:
        logger_instance: Logger to use
        title: Title of the section
    """
    logger_instance.info("")
    logger_instance.info("=" * 80)
    logger_instance.info(f"===== {title.upper()} =====")
    logger_instance.info("=" * 80)


def log_memory_usage(logger_instance: logging.Logger, level: int = logging.INFO):
    """
    Logs current system and GPU memory usage.
    
    Args:
        logger_instance: Logger to use
        level: Logging level for the memory information
    """
    try:
        # System RAM
        mem_info = psutil.virtual_memory()
        total_gb = mem_info.total / (1024**3)
        available_gb = mem_info.available / (1024**3)
        used_gb = mem_info.used / (1024**3)
        percent_used = mem_info.percent
        logger_instance.log(level, f"Sys Memory: {used_gb:.2f}/{total_gb:.2f} GB Used ({percent_used:.1f}%) | Available: {available_gb:.2f} GB")

        # Process Memory (Current Python process)
        process = psutil.Process(os.getpid())
        process_mem_mb = process.memory_info().rss / (1024**2)  # Resident Set Size
        logger_instance.log(level, f"Process Memory: {process_mem_mb:.2f} MB")

        # GPU Memory (if CUDA available)
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                reserved_mem = torch.cuda.memory_reserved(i) / (1024**3)
                allocated_mem = torch.cuda.memory_allocated(i) / (1024**3)
                free_mem = total_mem - reserved_mem  # More accurate available estimate
                logger_instance.log(level, f"  GPU {i} Mem: Allocated={allocated_mem:.2f} GB | Reserved={reserved_mem:.2f} GB | Total={total_mem:.2f} GB")
    except Exception as e:
        logger_instance.warning(f"Could not retrieve memory usage details: {e}", exc_info=False)


def log_final_memory_state(logger_instance: logging.Logger):
    """
    Log detailed final memory state for diagnostics.
    
    Args:
        logger_instance: Logger to use
    """
    logger_instance.info("--- Final Memory State ---")
    log_memory_usage(logger_instance, level=logging.INFO)


# --- Performance Timing ---
class Timing:
    """
    Simple class to track execution time of code sections.
    
    Features:
    - Multiple section tracking
    - Statistics calculation
    - Total, average, min, max timing
    """
    def __init__(self):
        """Initialize timing tracker."""
        self.timings = {}
        self.starts = {}
        
    def start(self, section: str):
        """
        Start timing a section.
        
        Args:
            section: Name of the section to time
        """
        self.starts[section] = time.time()
        
    def end(self, section: str) -> float:
        """
        End timing a section.
        
        Args:
            section: Name of the section to end
            
        Returns:
            Elapsed time in seconds
        """
        if section in self.starts:
            elapsed = time.time() - self.starts.pop(section)
            self.timings.setdefault(section, []).append(elapsed)
            return elapsed
        return 0
        
    def get_stats(self) -> Dict[str, Dict[str, float]]:
        """
        Get timing statistics.
        
        Returns:
            Dictionary with stats for each section
        """
        stats = {}
        for section, times in self.timings.items():
            stats[section] = {
                'avg': sum(times) / len(times) if times else 0,
                'min': min(times) if times else 0,
                'max': max(times) if times else 0,
                'total': sum(times),
                'count': len(times)
            }
        return stats
        
    def reset(self):
        """Reset all timings."""
        self.timings = {}
        self.starts = {}


# --- Resource Monitor ---
class ResourceMonitor:
    """
    Monitor system resources and log or take action if thresholds are exceeded.
    
    Features:
    - Memory and GPU monitoring
    - Configurable thresholds
    - Callback support for handling threshold violations
    """
    def __init__(self, 
                ram_threshold: float = 0.90,  # 90% of system RAM
                gpu_threshold: float = 0.95,  # 95% of GPU memory
                check_interval: float = 5.0,  # Check every 5 seconds
                logger: Optional[logging.Logger] = None):
        """
        Initialize resource monitor.
        
        Args:
            ram_threshold: System RAM threshold (0.0-1.0)
            gpu_threshold: GPU memory threshold (0.0-1.0)
            check_interval: Check interval in seconds
            logger: Logger to use (if None, create a new one)
        """
        self.ram_threshold = ram_threshold
        self.gpu_threshold = gpu_threshold
        self.check_interval = check_interval
        self.logger = logger or get_logger("resources")
        
        self.running = False
        self.on_threshold_exceeded = None  # Callback function
        
        self.logger.info(f"ResourceMonitor initialized with RAM threshold: {ram_threshold:.1%}, GPU threshold: {gpu_threshold:.1%}")
    
    def check(self) -> Dict[str, Any]:
        """
        Check current resource usage.
        
        Returns:
            Dictionary with resource usage information
        """
        import gc
        
        # Collect memory stats
        result = {}
        
        # System RAM
        mem_info = psutil.virtual_memory()
        ram_usage = mem_info.percent / 100.0
        result['ram_usage'] = ram_usage
        result['ram_exceeded'] = ram_usage > self.ram_threshold
        
        # GPU if available
        result['gpu_usage'] = []
        result['gpu_exceeded'] = False
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                allocated = torch.cuda.memory_allocated(i)
                reserved = torch.cuda.memory_reserved(i)
                total = props.total_memory
                
                # Calculate usage based on allocated memory
                gpu_usage = allocated / total
                result['gpu_usage'].append(gpu_usage)
                
                if gpu_usage > self.gpu_threshold:
                    result['gpu_exceeded'] = True
                    result['gpu_id'] = i
        
        # Force garbage collection
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        return result
    
    def log_status(self, level: int = logging.INFO):
        """
        Log current resource status.
        
        Args:
            level: Logging level to use
        """
        status = self.check()
        
        # Log RAM
        ram_usage = status['ram_usage']
        self.logger.log(level, f"System RAM: {ram_usage:.1%} used, threshold: {self.ram_threshold:.1%}")
        
        # Log GPU if available
        if torch.cuda.is_available():
            for i, gpu_usage in enumerate(status['gpu_usage']):
                self.logger.log(level, f"GPU {i}: {gpu_usage:.1%} used, threshold: {self.gpu_threshold:.1%}")
    
    def handle_threshold_violations(self) -> bool:
        """
        Check for threshold violations and handle them.
        
        Returns:
            True if any threshold was exceeded, False otherwise
        """
        status = self.check()
        violations = []
        
        # Check RAM
        if status['ram_exceeded']:
            violations.append(f"System RAM usage ({status['ram_usage']:.1%}) exceeded threshold ({self.ram_threshold:.1%})")
            
        # Check GPU
        if status['gpu_exceeded']:
            gpu_id = status.get('gpu_id', 0)
            gpu_usage = status['gpu_usage'][gpu_id]
            violations.append(f"GPU {gpu_id} memory usage ({gpu_usage:.1%}) exceeded threshold ({self.gpu_threshold:.1%})")
        
        # Handle violations
        if violations:
            message = "RESOURCE THRESHOLD EXCEEDED: " + ", ".join(violations)
            self.logger.warning(message)
            
            # Call callback if registered
            if callable(self.on_threshold_exceeded):
                try:
                    self.on_threshold_exceeded(violations)
                except Exception as e:
                    self.logger.error(f"Error in threshold callback: {e}")
                    
            return True
            
        return False
==========================================================
===== FILE: src/voxelflex/utils/system_utils.py =====
==========================================================

"""
System related utilities (CPU, GPU, Memory).

Provides functions for system resource management, monitoring, and optimization.
"""

import os
import gc
import logging
import psutil
import torch
from typing import Optional, Dict, Any

logger = logging.getLogger("voxelflex.utils.system")

def get_cpu_cores() -> int:
    """
    Get the number of logical CPU cores.
    
    Returns:
        Number of logical CPU cores (1 if detection fails)
    """
    try:
        cores = os.cpu_count()
        logger.debug(f"Detected {cores} logical CPU cores.")
        return cores
    except NotImplementedError:
        logger.warning("Could not detect number of CPU cores. Defaulting to 1.")
        return 1

def get_gpu_details() -> Dict[str, Any]:
    """
    Get details about available NVIDIA GPUs.
    
    Returns:
        Dictionary with GPU details including count, names, memory
    """
    details = {
        "count": 0, 
        "names": [], 
        "memory_gb": [], 
        "cuda_available": False
    }
    
    if torch.cuda.is_available():
        details["cuda_available"] = True
        details["count"] = torch.cuda.device_count()
        
        for i in range(details["count"]):
            try:
                props = torch.cuda.get_device_properties(i)
                details["names"].append(props.name)
                details["memory_gb"].append(props.total_memory / (1024**3))
            except Exception as e:
                logger.error(f"Could not get properties for GPU {i}: {e}")
                details["names"].append("Error")
                details["memory_gb"].append(0)

    return details

def log_gpu_details(logger_instance: logging.Logger = logger):
    """
    Logs GPU details.
    
    Args:
        logger_instance: Logger to use
    """
    gpu_info = get_gpu_details()
    if gpu_info["cuda_available"]:
        logger_instance.info(f"CUDA Available: Yes. Found {gpu_info['count']} GPU(s).")
        for i in range(gpu_info['count']):
            logger_instance.info(f"  GPU {i}: {gpu_info['names'][i]} - Memory: {gpu_info['memory_gb'][i]:.2f} GB")
    else:
        logger_instance.info("CUDA Available: No. Running on CPU.")

def get_device(prefer_gpu: bool = True) -> torch.device:
    """
    Gets the recommended device (GPU if available and preferred, else CPU).
    
    Args:
        prefer_gpu: Whether to prefer GPU if available
        
    Returns:
        PyTorch device to use
    """
    if prefer_gpu and torch.cuda.is_available():
        device = torch.device("cuda")
        log_gpu_details(logger)  # Log details when selecting GPU
    else:
        device = torch.device("cpu")
        logger.info("Using CPU device.")
        
    logger.info(f"Selected device: {device}")
    return device

def check_memory_usage() -> Dict[str, float]:
    """
    Returns system memory usage statistics.
    
    Returns:
        Dictionary with memory usage information in GB and percentages
    """
    mem_info = psutil.virtual_memory()
    return {
        "total_gb": mem_info.total / (1024**3),
        "available_gb": mem_info.available / (1024**3),
        "percent_used": mem_info.percent,
        "used_gb": mem_info.used / (1024**3)
    }

def clear_memory(force_gc: bool = True, clear_cuda: bool = True):
    """
    Attempts to clear memory by running GC and emptying CUDA cache.
    
    Args:
        force_gc: Whether to run garbage collection
        clear_cuda: Whether to clear CUDA cache if available
    """
    logger.debug("Attempting to clear memory...")
    if force_gc:
        gc.collect()
        logger.debug("Ran garbage collector.")
        
    if clear_cuda and torch.cuda.is_available():
        torch.cuda.empty_cache()
        logger.debug("Emptied CUDA cache.")

def set_num_threads(num_threads: Optional[int] = None):
    """
    Sets the number of threads used by PyTorch.
    
    Args:
        num_threads: Number of threads to use (None for default)
    """
    if num_threads is not None and num_threads > 0:
        logger.info(f"Setting PyTorch CPU threads to: {num_threads}")
        torch.set_num_threads(num_threads)
    else:
        # Use default setting (often uses all cores, which is usually fine)
        cores = get_cpu_cores()
        logger.debug(f"Using default PyTorch CPU thread settings (likely based on {cores} cores).")

def adjust_workers_for_memory(requested_workers: int, memory_threshold: float = 85.0) -> int:
    """
    Adjusts the number of DataLoader workers based on available memory.
    Simple heuristic: reduces workers if memory usage is high.
    
    Args:
        requested_workers: Initially requested number of workers
        memory_threshold: Memory usage percentage threshold for reduction
        
    Returns:
        Adjusted number of workers
    """
    if requested_workers <= 0:
        return 0

    try:
        mem_percent = check_memory_usage().get("percent_used", 0)
        if mem_percent > memory_threshold:
            reduced_workers = max(1, requested_workers // 2)  # Halve workers, minimum 1
            logger.warning(f"High memory usage ({mem_percent:.1f}%) detected. "
                           f"Reducing DataLoader workers from {requested_workers} to {reduced_workers}.")
            return reduced_workers
        else:
            return requested_workers
    except Exception as e:
        logger.warning(f"Could not check memory to adjust workers: {e}. Using requested value: {requested_workers}")
        return requested_workers

class AdaptiveChunkSizer:
    """
    Dynamically adjust chunk size based on memory pressure.
    
    Useful for iterative data loading to prevent out-of-memory errors.
    """
    def __init__(self, 
                initial_chunk_size: int = 100,
                min_chunk_size: int = 10,
                memory_headroom: float = 0.25,  # Aim to keep 25% memory free
                adjustment_factor: float = 0.8):  # Reduce by 20% when needed
        """
        Initialize adaptive chunk sizer.
        
        Args:
            initial_chunk_size: Starting chunk size
            min_chunk_size: Minimum allowed chunk size
            memory_headroom: Fraction of memory to keep free
            adjustment_factor: Factor to multiply chunk size by when adjusting down
        """
        self.current_chunk_size = initial_chunk_size
        self.min_chunk_size = min_chunk_size
        self.memory_headroom = memory_headroom
        self.adjustment_factor = adjustment_factor
        self.reduction_count = 0
        
    def get_chunk_size(self) -> int:
        """
        Get current recommended chunk size.
        
        Returns:
            Current chunk size
        """
        return self.current_chunk_size
        
    def check_and_adjust(self) -> int:
        """
        Check memory conditions and adjust chunk size if needed.
        
        Returns:
            New recommended chunk size
        """
        # Check system RAM
        ram_usage = psutil.virtual_memory().percent / 100.0
        free_ram_fraction = 1.0 - ram_usage
        
        # Check GPU if available
        gpu_free_fraction = 1.0
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                allocated = torch.cuda.memory_allocated(i)
                gpu_usage = allocated / props.total_memory
                gpu_free_fraction = min(gpu_free_fraction, 1.0 - gpu_usage)
        
        # Determine if adjustment needed
        headroom_ok = (free_ram_fraction >= self.memory_headroom and 
                       gpu_free_fraction >= self.memory_headroom)
                       
        if not headroom_ok and self.current_chunk_size > self.min_chunk_size:
            # Reduce chunk size to maintain headroom
            old_size = self.current_chunk_size
            self.current_chunk_size = max(
                self.min_chunk_size,
                int(self.current_chunk_size * self.adjustment_factor)
            )
            self.reduction_count += 1
            logger.warning(
                f"Memory pressure detected (RAM free: {free_ram_fraction:.2%}, "
                f"GPU free: {gpu_free_fraction:.2%}). "
                f"Reducing chunk size: {old_size} → {self.current_chunk_size}"
            )
        
        return self.current_chunk_size

def configure_h5py_for_domain_reading(chunk_size_mb: int = 64):
    """
    Configure h5py settings for optimal domain-level reading.
    
    Args:
        chunk_size_mb: Cache size in MB
    """
    try:
        import h5py
        
        # Set chunk cache size
        propfaid = h5py.h5p.create(h5py.h5p.FILE_ACCESS)
        settings = list(propfaid.get_cache())
        
        # Adjust cache for our access pattern
        # nslots: number of chunk slots (more means fewer collisions)
        # nbytes: size of cache in bytes
        # w0: chunk preemption policy (0-1, higher keeps chunks in cache longer)
        cache_size_bytes = chunk_size_mb * 1024 * 1024
        settings[1] = cache_size_bytes  # Cache size in bytes
        settings[2] = 1.0  # Keep chunks in cache as long as possible
        propfaid.set_cache(*settings)
        
        # Register the property list with h5py
        h5py.get_config().default_file_access_proplist = propfaid
        
        logger.info(f"Configured h5py for domain reading with {chunk_size_mb}MB cache.")
    except (ImportError, AttributeError) as e:
        logger.warning(f"Could not configure h5py cache: {e}")
==========================================================
===== FILE: src/voxelflex/utils/temp_scaling.py =====
==========================================================

"""
Utility functions for temperature scaling.

Provides functions for temperature normalization and scaling parameter management
needed for temperature-aware RMSF prediction.
"""

import json
import os
import logging
from typing import Callable, Tuple, List, Dict, Any, Optional

import numpy as np

from voxelflex.utils.file_utils import ensure_dir, load_json, save_json

logger = logging.getLogger("voxelflex.utils.temp_scaling")

def calculate_and_save_temp_scaling(
    train_temps: List[float],
    output_path: str
) -> Tuple[float, float]:
    """
    Calculates min/max temperature from the training data and saves them.
    
    Args:
        train_temps: List of raw temperature values from the training set
        output_path: Full path where the JSON file with scaling params will be saved
        
    Returns:
        Tuple of (temp_min, temp_max)
        
    Raises:
        ValueError: If no valid temperature values are provided
    """
    if not train_temps:
        raise ValueError("No training temperatures provided. Cannot calculate scaling parameters.")

    # Filter out None/NaN values
    valid_temps = [t for t in train_temps if t is not None and not np.isnan(t)]
    if not valid_temps:
        raise ValueError("No valid (non-NaN) temperatures found in training data.")

    # Calculate min/max
    temp_min = float(np.min(valid_temps))
    temp_max = float(np.max(valid_temps))
    temp_range = temp_max - temp_min

    logger.info(f"Calculating temperature scaling based on {len(valid_temps)} training samples.")
    logger.info(f"  Min Temperature: {temp_min:.2f} K")
    logger.info(f"  Max Temperature: {temp_max:.2f} K")
    logger.info(f"  Temperature Range: {temp_range:.2f} K")

    # Check for near-zero range
    if abs(temp_range) < 1e-6:
        logger.warning("Temperature range is near zero. Scaling will result in a constant value (0.5).")

    # Create scaling parameters dictionary
    scaling_params = {'temp_min': temp_min, 'temp_max': temp_max}

    try:
        # Ensure the directory exists before saving
        ensure_dir(os.path.dirname(output_path))
        save_json(scaling_params, output_path)
        logger.info(f"Saved temperature scaling parameters to {output_path}")
    except Exception as e:
        logger.error(f"Failed to save temperature scaling parameters to {output_path}: {e}")
        raise  # Re-raise error as saving params is crucial

    return temp_min, temp_max


def get_temperature_scaler(params_path: Optional[str] = None, params: Optional[Dict[str, float]] = None) -> Callable[[float], float]:
    """
    Loads scaling parameters from a JSON file or dict and returns a scaling function.
    
    Args:
        params_path: Path to the JSON file containing 'temp_min' and 'temp_max'
        params: Alternatively, provide the parameters directly as a dictionary
        
    Returns:
        A function that takes a raw temperature (float) and returns a scaled value [0, 1]
        
    Raises:
        ValueError: If neither params_path nor params is provided, or if params are invalid
        FileNotFoundError: If params_path is provided but the file doesn't exist
        KeyError: If required keys are missing in the parameters
    """
    if params is None and params_path:
        logger.info(f"Loading temperature scaling parameters from: {params_path}")
        if not os.path.exists(params_path):
            raise FileNotFoundError(f"Temperature scaling file not found: {params_path}")
        try:
            params = load_json(params_path)
        except Exception as e:
            logger.error(f"Error loading or parsing temperature scaling file {params_path}: {e}")
            raise
    elif params is None:
        raise ValueError("Must provide either params_path or params dictionary to get_temperature_scaler.")
    # else: use provided params dict

    try:
        temp_min = float(params['temp_min'])
        temp_max = float(params['temp_max'])
    except KeyError as e:
        raise KeyError(f"Missing key '{e}' in temperature scaling parameters: {params}") from e
    except (TypeError, ValueError) as e:
        raise ValueError(f"Invalid values in temperature scaling parameters {params}: {e}") from e

    temp_range = temp_max - temp_min
    logger.info(f"  Loaded Temp Scaler -> Min: {temp_min:.2f}, Max: {temp_max:.2f}, Range: {temp_range:.2f}")

    # Define the scaling function using the loaded parameters
    if abs(temp_range) < 1e-6:
        logger.warning("Temperature range is near zero. Scaling function will return 0.5.")
        # Return a lambda function that outputs a constant 0.5
        scaler_func = lambda t: 0.5
    else:
        # Return the standard Min-Max scaling function
        # Add epsilon for numerical stability if range is very small but non-zero
        epsilon = 1e-8
        # Use closure to capture temp_min and temp_range
        def scaler_func(t: float) -> float:
            """Scale temperature to [0, 1] range."""
            scaled = (float(t) - temp_min) / (temp_range + epsilon)
            # Clamp to [0, 1] for robustness against out-of-range temperatures
            return max(0.0, min(1.0, scaled))

    return scaler_func


def create_temperature_collection(
    temperatures: List[float],
    num_points: int = 5
) -> List[float]:
    """
    Creates a representative collection of temperatures from a list.
    
    Useful for creating a diverse set of temperature points for evaluation.
    
    Args:
        temperatures: List of raw temperature values
        num_points: Number of representative points to select
        
    Returns:
        List of selected temperature values
    """
    if not temperatures:
        return []
        
    # Filter out None/NaN
    valid_temps = np.array([t for t in temperatures if t is not None and not np.isnan(t)])
    
    if len(valid_temps) == 0:
        return []
        
    if len(valid_temps) <= num_points:
        return valid_temps.tolist()
        
    # Get min and max
    temp_min = np.min(valid_temps)
    temp_max = np.max(valid_temps)
    
    # If all temps are the same, return a single value
    if abs(temp_max - temp_min) < 1e-6:
        return [float(temp_min)]
        
    # Otherwise select evenly spaced points including min and max
    selected = np.linspace(temp_min, temp_max, num_points)
    
    # Try to select actual temperatures from the data that are closest to these points
    actual_temps = []
    for target in selected:
        idx = np.abs(valid_temps - target).argmin()
        actual_temps.append(float(valid_temps[idx]))
        
    return actual_temps
==========================================================
===== FILE: src/voxelflex/cli/cli.py =====
==========================================================

"""
Command Line Interface for VoxelFlex (Temperature-Aware).

Main entry point for voxelflex commands: preprocess, train, predict, evaluate, visualize.
"""

import argparse
import logging
import os
import sys
import time
from typing import List, Optional

from voxelflex.utils.logging_utils import setup_logging, get_logger, log_section_header, log_final_memory_state
logger = get_logger("cli")

# Define master samples filename constant
MASTER_SAMPLES_FILENAME = "master_samples.parquet"

def parse_args(args: Optional[List[str]] = None) -> argparse.Namespace:
    """
    Parse command line arguments.
    
    Args:
        args: Optional list of command line arguments (default: sys.argv[1:])
        
    Returns:
        Parsed arguments namespace
    """
    parser = argparse.ArgumentParser(
        prog="voxelflex", 
        description="VoxelFlex: Preprocess metadata, train, predict, evaluate, and visualize temperature-aware protein flexibility predictions.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Define common arguments for all commands
    common_parser_args = argparse.ArgumentParser(add_help=False)
    common_parser_args.add_argument(
        '-v', '--verbose', 
        action='count', 
        default=0, 
        help="Increase verbosity (-v INFO, -vv DEBUG)."
    )
    common_parser_args.add_argument(
        "--config", 
        type=str, 
        required=True, 
        help="Path to YAML config file."
    )
    
    # Define subcommands
    subparsers = parser.add_subparsers(
        dest="command", 
        help="Sub-command to run", 
        required=True
    )

    # --- Preprocess command ---
    preprocess_parser = subparsers.add_parser(
        "preprocess", 
        help="Preprocess metadata (generate sample list).", 
        parents=[common_parser_args], 
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # --- Train command ---
    train_parser = subparsers.add_parser(
        "train", 
        help="Train model (loads HDF5 on demand).", 
        parents=[common_parser_args], 
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    train_parser.add_argument(
        "--force_preprocess", 
        action="store_true", 
        help="Run metadata preprocessing first."
    )
    train_parser.add_argument(
        "--subset", 
        type=float, 
        default=1.0,
        help="Use a subset of training data (0.0-1.0)"
    )
    
    # --- Predict command ---
    predict_parser = subparsers.add_parser(
        "predict", 
        help="Predict RMSF at a target temperature.", 
        parents=[common_parser_args], 
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    predict_parser.add_argument(
        "--model", 
        type=str, 
        required=True, 
        help="Path to trained model checkpoint (.pt)."
    )
    predict_parser.add_argument(
        "--temperature", 
        type=float, 
        required=True, 
        help="Target prediction temperature (K)."
    )
    predict_parser.add_argument(
        "--domains", 
        type=str, 
        nargs='*', 
        default=None, 
        help="Optional: List HDF5 domain keys. Uses test split if omitted."
    )
    predict_parser.add_argument(
        "--output_csv", 
        type=str, 
        default=None, 
        help="Optional: Specify output CSV filename."
    )
    
    # --- Evaluate command ---
    evaluate_parser = subparsers.add_parser(
        "evaluate", 
        help="Evaluate model predictions.", 
        parents=[common_parser_args], 
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    evaluate_parser.add_argument(
        "--model", 
        type=str, 
        required=True, 
        help="Path to trained model checkpoint (.pt)."
    )
    evaluate_parser.add_argument(
        "--predictions", 
        type=str, 
        required=True, 
        help="Path to predictions CSV file."
    )
    
    # --- Visualize command ---
    visualize_parser = subparsers.add_parser(
        "visualize", 
        help="Create performance visualizations.", 
        parents=[common_parser_args], 
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    visualize_parser.add_argument(
        "--predictions", 
        type=str, 
        required=True, 
        help="Path to predictions CSV file."
    )
    visualize_parser.add_argument(
        "--history", 
        type=str, 
        default=None, 
        help="Optional: Path to training history JSON."
    )

    return parser.parse_args(args)

def main(cli_args: Optional[List[str]] = None) -> int:
    """
    Main CLI entry point for voxelflex.
    
    Args:
        cli_args: Optional list of command line arguments (default: sys.argv[1:])
        
    Returns:
        Exit code (0 = success, >0 = error)
    """
    start_time = time.time()
    args = parse_args(cli_args)
    
    # Configure initial logging based on verbosity
    console_log_level = "DEBUG" if args.verbose >= 2 else "INFO" if args.verbose == 1 else "WARNING"
    setup_logging(console_level=console_log_level, file_level="DEBUG", log_file=None)
    
    config = None
    log_file_path = None

    try:
        if not hasattr(args, 'config') or not args.config:
            raise ValueError("--config required.")
            
        # Load configuration
        from voxelflex.config.config import load_config
        config = load_config(args.config)
        
        # Set up logging to file
        log_file_path = os.path.join(config["output"]["log_dir"], config["output"]["log_file"])
        os.makedirs(config["output"]["log_dir"], exist_ok=True)
        setup_logging(log_file=log_file_path, console_level=console_log_level, file_level=config["logging"].get("file_level", "DEBUG"))
        logger.info(f"Logging re-initialized. Log file: {log_file_path}")
        logger.info(f"Run output directory: {config['output']['run_dir']}")
        
        # Log GPU information
        from voxelflex.utils.system_utils import log_gpu_details
        log_gpu_details(logger)

        log_section_header(logger, f"EXECUTING COMMAND: {args.command}")

        # Get the master sample filename from config
        master_samples_filename_from_config = config["data"].get("master_samples_file", MASTER_SAMPLES_FILENAME)

        # --- Command Execution ---
        if args.command == "preprocess":
            from voxelflex.cli.commands.preprocess import run_preprocessing
            run_preprocessing(config)
            
        elif args.command == "train":
            from voxelflex.cli.commands.train import train_model
            
            # Check for master sample file
            master_samples_path = os.path.join(config["data"]["processed_dir"], master_samples_filename_from_config)
            preprocessed_exists = os.path.exists(master_samples_path)

            if not preprocessed_exists or args.force_preprocess:
                if args.force_preprocess:
                    logger.info("Metadata preprocessing forced.")
                else:
                    logger.warning("Master sample file not found. Running preprocessing first...")
                    
                from voxelflex.cli.commands.preprocess import run_preprocessing
                run_preprocessing(config)
                
                if not os.path.exists(master_samples_path):
                    raise RuntimeError(f"Preprocessing ran but master sample file missing: {master_samples_path}")
                    
                logger.info("Preprocessing finished. Proceeding with training.")
            else:
                logger.info("Master sample file found. Skipping preprocessing.")
                
            # Apply subset parameter if provided
            if hasattr(args, 'subset') and args.subset < 1.0:
                if args.subset <= 0.0 or args.subset > 1.0:
                    logger.warning(f"Invalid subset value {args.subset}. Using full dataset.")
                else:
                    logger.info(f"Using {args.subset:.1%} of training data as requested")
                    config['training']['subset'] = args.subset
                
            train_model(config)
            
        elif args.command == "predict":
            from voxelflex.cli.commands.predict import predict_rmsf
            predict_rmsf(
                config=config, 
                model_path=args.model, 
                target_temperature=args.temperature, 
                domain_ids_to_predict=args.domains, 
                output_csv_filename=args.output_csv
            )
            
        elif args.command == "evaluate":
            from voxelflex.cli.commands.evaluate import evaluate_model
            evaluate_model(
                config=config, 
                model_path=args.model, 
                predictions_path=args.predictions
            )
            
        elif args.command == "visualize":
            from voxelflex.cli.commands.visualize import create_visualizations
            
            history_file_resolved = None
            if args.history:
                from voxelflex.utils.file_utils import resolve_path
                history_file_resolved = resolve_path(args.history)
                
            if history_file_resolved and not os.path.exists(history_file_resolved):
                logger.warning(f"History file not found: {history_file_resolved}")
                history_file_resolved = None
                
            create_visualizations(
                config=config, 
                predictions_path=args.predictions, 
                history_path=history_file_resolved
            )
            
        else:
            logger.error(f"Unknown command: {args.command}")
            return 1

        log_section_header(logger, f"COMMAND '{args.command}' COMPLETED")
        return 0

    except FileNotFoundError as e:
        logger.error(f"File Not Found Error: {e}")
        return 1
    except ValueError as e:
        logger.error(f"Value Error: {e}")
        return 1
    except RuntimeError as e:
        logger.error(f"Runtime Error: {e}")
        return 1
    except ImportError as e:
        logger.error(f"Import Error: {e}. Ensure required libraries (e.g., pyarrow) are installed.")
        return 1
    except Exception as e:
        logger.exception(f"An unexpected error occurred: {e}")
        return 1
    finally:
        end_time = time.time()
        total_duration = end_time - start_time
        logger.info(f"Total execution time: {total_duration:.2f} seconds.")
        log_final_memory_state(logger)
        logging.shutdown()

if __name__ == "__main__":
    sys.exit(main())
==========================================================
===== FILE: src/voxelflex/cli/commands/preprocess.py =====
==========================================================

"""
Preprocessing command for VoxelFlex (Temperature-Aware).

Performs metadata-only preprocessing - creating the master samples list
and temperature scaling parameters without processing voxel data.
"""

import os
import time
import json
import logging
import gc
import math
import h5py
from typing import Dict, Any, Tuple, List, Optional, Callable, Set, DefaultDict
from collections import defaultdict, OrderedDict

import numpy as np
import pandas as pd

try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    PYARROW_AVAILABLE = True
except ImportError:
    PYARROW_AVAILABLE = False

logger = logging.getLogger("voxelflex.cli.preprocess")

from voxelflex.data.data_loader import (
    load_aggregated_rmsf_data, create_master_rmsf_lookup, create_domain_mapping
)
from voxelflex.utils.logging_utils import (
    log_stage, EnhancedProgressBar, log_memory_usage, log_section_header, get_logger
)
from voxelflex.utils.file_utils import ensure_dir, save_json, load_json, load_list_from_file, save_list_to_file, resolve_path
from voxelflex.utils.system_utils import clear_memory
from voxelflex.utils.temp_scaling import calculate_and_save_temp_scaling

MASTER_SAMPLES_FILENAME = "master_samples.parquet"  # Default to Parquet

def run_preprocessing(config: Dict[str, Any]) -> bool:
    """
    Main function for metadata-only preprocessing.
    Generates and saves master sample list and temperature scaler.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        True if preprocessing succeeded, False otherwise
        
    Raises:
        Various exceptions may be raised for data or file issues
    """
    if MASTER_SAMPLES_FILENAME.endswith(".parquet"):
        try:
            import pyarrow as pa
            import pyarrow.parquet as pq
            PYARROW_AVAILABLE = True
        except ImportError:
            logger.error("PyArrow needed for Parquet output. Please install with `pip install pyarrow`")
            raise ImportError("PyArrow needed for Parquet output. `pip install pyarrow`")

    log_section_header(logger, "STARTING PREPROCESSING (Metadata Only)")
    start_time = time.time()

    # Extract paths from config
    input_cfg = config['input']
    data_cfg = config['data']
    output_cfg = config['output']
    voxel_file_path = input_cfg['voxel_file']
    rmsf_file_path = input_cfg['aggregated_rmsf_file']
    processed_dir = data_cfg['processed_dir']
    run_output_dir = output_cfg['run_dir']
    master_samples_output_path = os.path.join(processed_dir, MASTER_SAMPLES_FILENAME)

    # Ensure output directories exist
    ensure_dir(processed_dir)
    ensure_dir(run_output_dir)

    failed_domains: Set[str] = set()
    master_samples_list: List[Dict[str, Any]] = []

    try:
        # --- 1. Load RMSF Data and Create Lookups ---
        with log_stage("PREPROCESS", "Loading RMSF Data & Mappings"):
            # Load with optimized dtypes
            rmsf_df = load_aggregated_rmsf_data(rmsf_file_path)
            
            # Create and serialize the lookup
            rmsf_lookup = create_master_rmsf_lookup(rmsf_df)
            if not rmsf_lookup:
                raise ValueError("RMSF lookup empty.")
                
            # Read HDF5 keys efficiently
            try:
                # Get domain keys from HDF5 - optimized to just read keys not data
                with h5py.File(voxel_file_path, 'r') as f_h5:
                    hdf5_domain_keys = list(f_h5.keys())
                    # Get temperature range while we have the data
                    temp_values = rmsf_df['temperature_feature'].dropna().unique()
                    temp_min = float(np.min(temp_values))
                    temp_max = float(np.max(temp_values))
                    logger.info(f"Detected temperature range: [{temp_min:.1f}, {temp_max:.1f}]")
                    
            except Exception as e:
                raise RuntimeError(f"Failed read HDF5 keys: {e}")
                
            if not hdf5_domain_keys:
                raise ValueError("No keys in HDF5.")
                
            # Create and serialize domain mapping
            rmsf_domain_ids = rmsf_df['domain_id'].unique().tolist()
            domain_mapping = create_domain_mapping(hdf5_domain_keys, rmsf_domain_ids)
            available_hdf5_keys = set(domain_mapping.keys())
            
            if not available_hdf5_keys:
                raise ValueError("No HDF5 keys mappable.")
                
            logger.info(f"Found {len(available_hdf5_keys)} mappable HDF5 domain keys.")
            
            # Calculate temperature scaler parameters from all the data
            temp_scaling_params_path = data_cfg["temp_scaling_params_file"]
            
            # Directly save temperature scaling parameters
            temp_scaling_params = {'temp_min': temp_min, 'temp_max': temp_max}
            try:
                ensure_dir(os.path.dirname(temp_scaling_params_path))
                save_json(temp_scaling_params, temp_scaling_params_path)
                logger.info(f"Saved temperature scaling parameters to {temp_scaling_params_path}")
            except Exception as e:
                logger.error(f"Failed to save temperature scaling parameters: {e}")
            
            # We can release the full dataframe to save memory
            del rmsf_df
            gc.collect()

        # --- 2. Load Domain Splits & Filter ---
        with log_stage("PREPROCESS", "Loading and Filtering Domain Splits"):
            split_domains_map: Dict[str, str] = {}
            all_split_hdf5_keys_in_use: Set[str] = set()
            
            for split in ["train", "val", "test"]:
                split_file = input_cfg.get(f"{split}_split_file")
                
                if not split_file or not os.path.exists(split_file):
                    logger.warning(f"Split file '{split}' not found.")
                    continue
                    
                domains_in_file = load_list_from_file(split_file)
                
                if not domains_in_file:
                    logger.warning(f"Split file '{split}' empty.")
                    continue
                    
                valid_split_domains = []
                for d in domains_in_file:
                    if d in available_hdf5_keys:
                        valid_split_domains.append(d)
                    elif d in hdf5_domain_keys:
                        logger.warning(f"Split '{split}': Domain '{d}' unmappable.")
                    else:
                        logger.warning(f"Split '{split}': Domain '{d}' not in HDF5.")
                        
                max_doms = input_cfg.get('max_domains')
                if max_doms is not None and max_doms > 0 and len(valid_split_domains) > max_doms:
                    valid_split_domains = valid_split_domains[:max_doms]
                    
                for d in valid_split_domains:
                    split_domains_map[d] = split
                    
                all_split_hdf5_keys_in_use.update(valid_split_domains)
                logger.info(f"Split '{split}': Using {len(valid_split_domains)} domains.")
                
            if not any(s == 'train' for s in split_domains_map.values()):
                raise ValueError("Train split empty/invalid.")
                
            if not any(s == 'val' for s in split_domains_map.values()):
                raise ValueError("Validation split empty/invalid.")
                
            if not any(s == 'test' for s in split_domains_map.values()):
                logger.warning("Test split empty/not specified.")
                
            logger.info(f"Total unique domains across valid splits: {len(all_split_hdf5_keys_in_use)}")

        # --- 3. Generate Master Sample List (with Split Info) ---
        with log_stage("PREPROCESS", "Generating Master Sample List"):
            residues_without_rmsf = 0
            domains_with_residues_checked: Set[str] = set()
            logger.info(f"Checking HDF5 residues for {len(all_split_hdf5_keys_in_use)} domains...")
            progress_domains = EnhancedProgressBar(len(all_split_hdf5_keys_in_use), desc="Checking HDF5 Residues")

            with h5py.File(voxel_file_path, 'r') as f_h5:
                for i, hdf5_domain_id in enumerate(all_split_hdf5_keys_in_use):
                    residue_group = None
                    domain_had_residues = False
                    samples_before_domain = len(master_samples_list)

                    try:
                        if hdf5_domain_id not in f_h5:
                            continue

                        potential_chain_keys = [k for k in f_h5[hdf5_domain_id].keys() 
                                               if isinstance(f_h5[hdf5_domain_id][k], h5py.Group)]
                        
                        # Find residue group efficiently
                        for chain_key in potential_chain_keys:
                            try:
                                chain_group = f_h5[hdf5_domain_id][chain_key]
                                # Check if any residue IDs exist (must be digit strings)
                                if any(k.isdigit() for k in chain_group.keys()):
                                    residue_group = chain_group
                                    break
                            except Exception:
                                continue

                        if residue_group is None:
                            logger.debug(f"No valid residue group found for {hdf5_domain_id}")
                            failed_domains.add(hdf5_domain_id)
                            progress_domains.update(1)
                            continue

                        # --- Process the found residue_group ---
                        domains_with_residues_checked.add(hdf5_domain_id)
                        rmsf_domain_id = domain_mapping.get(hdf5_domain_id)
                        
                        if rmsf_domain_id is None:
                            logger.debug(f"Domain {hdf5_domain_id} lost mapping.")
                            failed_domains.add(hdf5_domain_id)
                            progress_domains.update(1)
                            continue

                        split_assignment = split_domains_map.get(hdf5_domain_id, "unknown")
                        
                        # Get residue keys efficiently
                        residue_keys = [k for k in residue_group.keys() if k.isdigit()]
                        
                        for resid_str in residue_keys:
                            domain_had_residues = True
                            
                            try:
                                resid_int = int(resid_str)
                                lookup_key = (rmsf_domain_id, resid_int)
                                temp_rmsf_pairs = rmsf_lookup.get(lookup_key)
                                
                                if temp_rmsf_pairs is None:
                                    base_rmsf_id = rmsf_domain_id.split('_')[0]
                                    if base_rmsf_id != rmsf_domain_id:
                                        temp_rmsf_pairs = rmsf_lookup.get((base_rmsf_id, resid_int))
                                        
                                if temp_rmsf_pairs:
                                    for raw_temp, target_rmsf in temp_rmsf_pairs:
                                        if (raw_temp is not None and not np.isnan(raw_temp) and 
                                            target_rmsf is not None and not np.isnan(target_rmsf) and 
                                            target_rmsf >= 0):
                                            
                                            # Create a sample with all required fields
                                            # Ensure domain_id and resid_str are strings
                                            master_samples_list.append({
                                                'hdf5_domain_id': str(hdf5_domain_id),
                                                'resid_str': str(resid_str),
                                                'resid_int': resid_int,
                                                'raw_temp': float(raw_temp),
                                                'target_rmsf': float(target_rmsf),
                                                'split': split_assignment
                                            })
                                else:
                                    residues_without_rmsf += 1
                                    
                            except ValueError:
                                logger.debug(f"Invalid resid '{resid_str}' in {hdf5_domain_id}.")
                            except Exception as e:
                                logger.debug(f"Error processing {hdf5_domain_id}:{resid_str}: {e}")
                        
                        # Verify we got samples for this domain
                        samples_after_domain = len(master_samples_list)
                        if domain_had_residues and samples_after_domain == samples_before_domain:
                            logger.warning(f"Domain '{hdf5_domain_id}' had residues but no valid samples (check RMSF lookup).")
                            failed_domains.add(hdf5_domain_id)

                    except Exception as e:
                        logger.warning(f"Error processing domain {hdf5_domain_id}: {e}")
                        failed_domains.add(hdf5_domain_id)

                    progress_domains.update(1)  # Update progress
                
            progress_domains.finish()

            if not master_samples_list:
                raise ValueError("Master sample list is empty after processing all domains.")
                
            logger.info(f"Generated {len(master_samples_list)} total sample entries.")
            
            if residues_without_rmsf > 0:
                logger.info(f"  {residues_without_rmsf} HDF5 residues lacked corresponding RMSF data.")
                
            for domain_id in all_split_hdf5_keys_in_use:
                if domain_id not in domains_with_residues_checked and domain_id not in failed_domains:
                    logger.warning(f"Domain '{domain_id}' was in splits but never processed?")
                    failed_domains.add(domain_id)
                    
            # Free memory
            del rmsf_lookup
            gc.collect()

        # --- 4. Save Master Sample List ---
        with log_stage("PREPROCESS", "Saving Master Sample List"):
            if not master_samples_list:
                raise ValueError("Cannot save empty master sample list.")
                
            master_samples_df = pd.DataFrame(master_samples_list)
            logger.info(f"Saving {len(master_samples_df)} samples to {master_samples_output_path}...")
            
            try:
                if MASTER_SAMPLES_FILENAME.endswith(".parquet"):
                    # Use pyarrow with specified schema for more efficient storage and proper types
                    schema = pa.schema([
                        ('hdf5_domain_id', pa.string()),
                        ('resid_str', pa.string()),
                        ('resid_int', pa.int64()),
                        ('raw_temp', pa.float32()),
                        ('target_rmsf', pa.float32()),
                        ('split', pa.string())
                    ])
                    
                    table = pa.Table.from_pandas(master_samples_df, schema=schema, preserve_index=False)
                    # Use compression for smaller file size
                    pq.write_table(table, master_samples_output_path, compression='snappy')
                else:
                    # If using CSV, ensure proper types and format
                    master_samples_df.to_csv(master_samples_output_path, index=False, float_format='%.6f')
                    
                logger.info(f"Master sample list saved successfully to {master_samples_output_path}")
                
            except Exception as e:
                logger.error(f"Failed to save master sample list: {e}")
                raise
                
            if not os.path.exists(master_samples_output_path) or os.path.getsize(master_samples_output_path) == 0:
                raise RuntimeError(f"Master sample file {master_samples_output_path} not created/empty.")

            # Calculate temperature scaling using training samples
            train_samples = master_samples_df[master_samples_df['split'] == 'train']
            train_temps = train_samples['raw_temp'].tolist()
            temp_min, temp_max = calculate_and_save_temp_scaling(train_temps, temp_scaling_params_path)

        # --- 5. Final Summary ---
        log_section_header(logger, "PREPROCESSING FINISHED (Metadata Only)")
        total_duration = time.time() - start_time
        logger.info(f"Total Preprocessing Time: {total_duration:.2f}s.")
        logger.info(f"Saved master sample list ({len(master_samples_list)} samples) to {master_samples_output_path}")
        logger.info("Voxel data was NOT processed or saved in this step.")
        
        num_files_in_processed = len([f for f in os.listdir(processed_dir) if os.path.isfile(os.path.join(processed_dir, f))])
        logger.info(f"Found {num_files_in_processed} file(s) in {processed_dir}.")

        if failed_domains:
            failed_list_path = os.path.join(run_output_dir, "failed_preprocess_domains.txt")
            logger.warning(f"Found {len(failed_domains)} domains with check/mapping/residue failure.")
            try:
                save_list_to_file(sorted(list(failed_domains)), failed_list_path)
                logger.info(f"Failed domains list saved: {failed_list_path}")
            except Exception as save_err:
                logger.error(f"Could not save failed domains list: {save_err}")
        else:
            logger.info("No domains completely failed during initial checks/mapping.")

        return True

    except Exception as e:
        logger.exception(f"Metadata preprocessing pipeline failed: {e}")
        if failed_domains:
            try:
                failed_list_path = os.path.join(run_output_dir, "failed_preprocess_domains_partial.txt")
                save_list_to_file(sorted(list(failed_domains)), failed_list_path)
                logger.info(f"Saved partial failed domains list: {failed_list_path}")
            except:
                pass
        return False
    finally:
        if 'master_samples_list' in locals():
            del master_samples_list
        gc.collect()
        logger.info("End of metadata preprocessing run.")
        log_memory_usage(logger, level=logging.INFO)
==========================================================
===== FILE: src/voxelflex/cli/commands/train.py =====
==========================================================

"""
Training command for VoxelFlex (Temperature-Aware).

Implements training using the chunked data loading approach for
memory-efficient processing of large HDF5 files.
"""

import os
import time
import json
import logging
import math
from typing import Dict, Any, Tuple, List, Optional, Callable, Union
import shutil
import gc

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, StepLR
from scipy.stats import pearsonr

logger = logging.getLogger("voxelflex.cli.train")

# Project imports
from voxelflex.data.data_loader import (
    ChunkedVoxelDataset,  # Use this for train/val/test
    worker_init_fn,       # Simplified version is fine
    simple_collate_fn,    # Keep using this
    load_list_from_file   # Utility to load domain lists
)
from voxelflex.models.cnn_models import get_model
from voxelflex.utils.logging_utils import (
    log_stage, EnhancedProgressBar, log_memory_usage, log_section_header, get_logger, Timing
)
from voxelflex.utils.file_utils import ensure_dir, save_json, load_json, resolve_path
from voxelflex.utils.system_utils import (
    get_device, clear_memory, check_memory_usage, set_num_threads
)
from voxelflex.utils.temp_scaling import get_temperature_scaler

# --- Train/Validate Epoch Functions ---
def train_epoch(
    model: nn.Module,
    train_loader: DataLoader,
    criterion: nn.Module,
    optimizer: optim.Optimizer,
    device: torch.device,
    epoch: int,
    config: Dict[str, Any],
    scaler: Optional[torch.cuda.amp.GradScaler] = None,
    timing: Optional[Timing] = None
) -> Tuple[float, float]:
    """
    Performs one training epoch with performance tracking.
    
    Args:
        model: Model to train
        train_loader: DataLoader for training data
        criterion: Loss function
        optimizer: Optimizer
        device: Device to train on
        epoch: Current epoch number
        config: Configuration dictionary
        scaler: Optional GradScaler for mixed precision training
        timing: Optional timing tracker
        
    Returns:
        Tuple of (average loss, pearson correlation)
    """
    model.train()
    running_loss_sum = 0.0
    batch_preds_all = []
    batch_targets_all = []
    num_samples_processed = 0
    
    train_cfg = config['training']
    grad_clip_cfg = train_cfg.get('gradient_clipping', {})
    grad_clip_norm = grad_clip_cfg.get('max_norm') if grad_clip_cfg.get('enabled') else None
    show_progress = config['logging'].get('show_progress_bars', True)
    autocast_enabled = scaler is not None and scaler.is_enabled()
    grad_accum_steps = train_cfg.get('gradient_accumulation_steps', 1)

    # Estimate total batches for progress bar
    total_batches_estimate = None
    est_samples = config.get('runtime', {}).get('estimated_samples_per_epoch')
    batch_size = train_cfg.get('batch_size')
    if est_samples and batch_size:
        total_batches_estimate = math.ceil(est_samples / batch_size)
        logger.debug(f"Using estimated train batches for progress: {total_batches_estimate}")

    progress = EnhancedProgressBar(total=total_batches_estimate, desc=f"Epoch {epoch+1} Train") if show_progress else None

    if timing: timing.start('train_epoch_total')
    optimizer.zero_grad(set_to_none=True)
    running_grad_norm = 0.0

    # --- Batch Loop ---
    for i, batch_dict in enumerate(train_loader):
        if timing: timing.start('batch_process')
        if batch_dict is None:
            logger.debug(f"Skipping None batch {i}")
            if progress: progress.update(1)
            if timing: timing.end('batch_process')
            continue
            
        try: # Data Transfer
            if timing: timing.start('data_transfer')
            voxel_inputs = batch_dict['voxels'].to(device, non_blocking=True)
            scaled_temps = batch_dict['scaled_temps'].to(device, non_blocking=True)
            targets = batch_dict['targets'].to(device, non_blocking=True)
            if timing: timing.end('data_transfer')
            current_batch_size = voxel_inputs.size(0)
        except Exception as load_e:
            logger.debug(f"Batch prep error {i}: {load_e}")
            if progress: progress.update(1)
            if timing: timing.end('batch_process')
            continue
            
        try: # Forward Pass
            if timing: timing.start('forward')
            with torch.autocast(device_type=device.type, enabled=autocast_enabled):
                outputs = model(voxel_input=voxel_inputs, scaled_temp=scaled_temps)
                loss = criterion(outputs, targets) / grad_accum_steps
            if timing: timing.end('forward')
            
            if torch.isnan(loss) or torch.isinf(loss):
                logger.warning(f"NaN/Inf loss batch {i}. Skip.")
                if progress: progress.update(1)
                if timing: timing.end('batch_process')
                continue
        except Exception as forward_e:
            logger.debug(f"Forward error batch {i}: {forward_e}")
            if progress: progress.update(1)
            if timing: timing.end('batch_process')
            continue
            
        try: # Backward Pass
            if timing: timing.start('backward')
            if scaler is not None:
                scaler.scale(loss).backward()
            else:
                loss.backward()
            if timing: timing.end('backward')
            
            if grad_clip_norm: # Grad norm calculation
                with torch.no_grad():
                    for param in model.parameters():
                        if param.grad is not None:
                            param_norm = param.grad.detach().data.norm(2)
                            running_grad_norm += param_norm.item() ** 2
        except Exception as backward_e:
            logger.debug(f"Backward error batch {i}: {backward_e}")
            optimizer.zero_grad(set_to_none=True)
            if progress: progress.update(1)
            if timing: timing.end('batch_process')
            continue

        # --- Step Optimizer ---
        is_last_batch = False # Hard to determine precisely with IterableDataset
        if progress and total_batches_estimate and progress.n >= total_batches_estimate - 1:
            is_last_batch = True # Approximation
            
        if (i + 1) % grad_accum_steps == 0 or is_last_batch:
            if timing: timing.start('optimizer_step')
            total_norm = 0.0
            if grad_clip_norm:
                total_norm = running_grad_norm**0.5
                running_grad_norm = 0.0
                
            try:
                if scaler is not None:
                    if grad_clip_norm:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    if grad_clip_norm:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)
                    optimizer.step()
                    
                optimizer.zero_grad(set_to_none=True)
            except Exception as optim_e:
                logger.debug(f"Optim step error {i}: {optim_e}")
                optimizer.zero_grad(set_to_none=True)
                
            if timing: timing.end('optimizer_step')

        # --- Accumulate & Cleanup ---
        running_loss_sum += (loss.item() * grad_accum_steps) * current_batch_size
        batch_preds_all.append(outputs.detach().cpu().numpy())
        batch_targets_all.append(targets.detach().cpu().numpy())
        num_samples_processed += current_batch_size
        
        if progress: progress.update(1)
        del voxel_inputs, scaled_temps, targets, outputs, loss
        if i % 50 == 0 and device.type == 'cuda':
            torch.cuda.empty_cache()
            
        if timing: timing.end('batch_process')
    # --- End Batch Loop ---

    if progress: progress.close()

    # --- Calculate Metrics ---
    if timing: timing.start('metrics_calculation')
    
    avg_loss = running_loss_sum / num_samples_processed if num_samples_processed > 0 else 0.0
    epoch_pearson = 0.0
    
    if num_samples_processed > 1 and batch_preds_all and batch_targets_all:
        try:
            all_preds_flat = np.concatenate([b.flatten() for b in batch_preds_all])
            all_targets_flat = np.concatenate([b.flatten() for b in batch_targets_all])
            
            valid_mask = np.isfinite(all_preds_flat) & np.isfinite(all_targets_flat)
            preds_valid = all_preds_flat[valid_mask]
            targets_valid = all_targets_flat[valid_mask]
            
            if len(preds_valid) > 1 and np.std(preds_valid) > 1e-6 and np.std(targets_valid) > 1e-6:
                corr, _ = pearsonr(preds_valid, targets_valid)
                epoch_pearson = corr if not np.isnan(corr) else 0.0
        except Exception as e:
            logger.debug(f"Train Corr calc error: {e}")
            
    if timing: timing.end('metrics_calculation')
    if timing: timing.end('train_epoch_total')
    
    logger.debug(f"Train epoch timing: {timing.get_stats().get('train_epoch_total', {}).get('total', 0):.2f}s")
    return avg_loss, epoch_pearson


def validate(
    model: nn.Module,
    val_loader: DataLoader,
    criterion: nn.Module,
    device: torch.device,
    config: Dict[str, Any],
    timing: Optional[Timing] = None
) -> Tuple[float, float]:
    """
    Performs validation with performance tracking.
    
    Args:
        model: Model to validate
        val_loader: DataLoader for validation data
        criterion: Loss function
        device: Device to validate on
        config: Configuration dictionary
        timing: Optional timing tracker
        
    Returns:
        Tuple of (average loss, pearson correlation)
    """
    model.eval()
    running_loss_sum = 0.0
    batch_preds_all = []
    batch_targets_all = []
    num_samples_processed = 0
    
    autocast_enabled = config['training'].get('mixed_precision', {}).get('enabled', False) and device.type == 'cuda'
    show_progress = config['logging'].get('show_progress_bars', True)

    # Estimate total batches for progress bar
    val_batches_estimate = None
    est_val_samples = config.get('runtime', {}).get('estimated_val_samples')
    batch_size = config['training'].get('batch_size')  # Assuming same base batch size logic
    if est_val_samples and batch_size:
        val_batch_size = batch_size * 2  # Match loader config
        val_batches_estimate = math.ceil(est_val_samples / val_batch_size)
        logger.debug(f"Using estimated val batches for progress: {val_batches_estimate}")

    progress = EnhancedProgressBar(total=val_batches_estimate, desc="Validation") if show_progress else None

    if timing: timing.start('val_epoch_total')
    
    with torch.no_grad():
        for i, batch_dict in enumerate(val_loader):
            if timing: timing.start('val_batch_process')
            
            if batch_dict is None:
                if progress: progress.update(1)
                if timing: timing.end('val_batch_process')
                continue
                
            try: # Data Transfer
                if timing: timing.start('val_data_transfer')
                voxel_inputs = batch_dict['voxels'].to(device, non_blocking=True)
                scaled_temps = batch_dict['scaled_temps'].to(device, non_blocking=True)
                targets = batch_dict['targets'].to(device, non_blocking=True)
                if timing: timing.end('val_data_transfer')
                current_batch_size = voxel_inputs.size(0)
            except Exception as e:
                logger.debug(f"Val batch prep {i}: {e}")
                if progress: progress.update(1)
                if timing: timing.end('val_batch_process')
                continue
                
            try: # Forward
                if timing: timing.start('val_forward')
                with torch.autocast(device_type=device.type, enabled=autocast_enabled):
                    outputs = model(voxel_input=voxel_inputs, scaled_temp=scaled_temps)
                    loss = criterion(outputs, targets)
                if timing: timing.end('val_forward')
                
                if torch.isnan(loss) or torch.isinf(loss):
                    logger.debug(f"Val NaN/Inf loss {i}. Skip.")
                    if progress: progress.update(1)
                    if timing: timing.end('val_batch_process')
                    continue
                    
                running_loss_sum += loss.item() * current_batch_size
                batch_preds_all.append(outputs.cpu().numpy())
                batch_targets_all.append(targets.cpu().numpy())
                num_samples_processed += current_batch_size
            except Exception as e:
                logger.debug(f"Val forward error {i}: {e}")
                if progress: progress.update(1)
                if timing: timing.end('val_batch_process')
                continue
                
            # Cleanup & Progress
            if progress: progress.update(1)
            del voxel_inputs, scaled_temps, targets, outputs, loss
            if i % 50 == 0 and device.type == 'cuda':
                torch.cuda.empty_cache()
                
            if timing: timing.end('val_batch_process')
    # End Batch Loop
    
    if progress: progress.close()

    # --- Calculate Metrics ---
    if timing: timing.start('val_metrics_calculation')
    
    avg_loss = running_loss_sum / num_samples_processed if num_samples_processed > 0 else float('inf')
    epoch_pearson = 0.0
    
    if num_samples_processed > 1 and batch_preds_all and batch_targets_all:
        try:
            all_preds_flat = np.concatenate([b.flatten() for b in batch_preds_all])
            all_targets_flat = np.concatenate([b.flatten() for b in batch_targets_all])
            
            valid_mask = np.isfinite(all_preds_flat) & np.isfinite(all_targets_flat)
            preds_valid = all_preds_flat[valid_mask]
            targets_valid = all_targets_flat[valid_mask]
            
            if len(preds_valid) > 1 and np.std(preds_valid) > 1e-6 and np.std(targets_valid) > 1e-6:
                corr, _ = pearsonr(preds_valid, targets_valid)
                epoch_pearson = corr if not np.isnan(corr) else 0.0
        except Exception as e:
            logger.debug(f"Val Corr calc error: {e}")
            
    if timing: timing.end('val_metrics_calculation')
    if timing: timing.end('val_epoch_total')
    
    logger.debug(f"Val epoch timing: {timing.get_stats().get('val_epoch_total', {}).get('total', 0):.2f}s")
    return avg_loss, epoch_pearson


# --- Helper Functions: Optimizer and Scheduler ---
def get_optimizer(model: nn.Module, config: Dict[str, Any]) -> optim.Optimizer:
    """
    Get optimizer based on config.
    
    Args:
        model: Model to optimize
        config: Configuration dictionary
        
    Returns:
        PyTorch optimizer
    """
    lr = float(config['training']['learning_rate'])
    weight_decay = float(config['training']['weight_decay'])
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    logger.info(f"Optimizer: AdamW (LR={lr:.2e}, WD={weight_decay:.2e})")
    return optimizer

def get_scheduler(optimizer: optim.Optimizer, config: Dict[str, Any], num_epochs: int) -> Tuple[Optional[Any], str, str]:
    """
    Get learning rate scheduler based on config.
    
    Args:
        optimizer: Optimizer to schedule
        config: Configuration dictionary
        num_epochs: Total number of epochs
        
    Returns:
        Tuple of (scheduler, metric_to_monitor, mode)
    """
    s_cfg = config['training'].get('scheduler', {})
    s_type = s_cfg.get('type', 'reduce_on_plateau').lower()
    metric = s_cfg.get('monitor_metric', 'val_pearson')
    
    if metric not in ['val_loss', 'val_pearson']:
        logger.warning(f"Invalid scheduler metric '{metric}'. Default='val_pearson'.")
        metric = 'val_pearson'
        
    mode = s_cfg.get('mode')
    if mode not in ['min', 'max']:
        mode = 'max' if 'pearson' in metric else 'min'
        logger.debug(f"Scheduler mode auto={mode}.")
        
    scheduler = None
    
    if s_type == 'reduce_on_plateau':
        scheduler = ReduceLROnPlateau(
            optimizer,
            mode=mode,
            factor=float(s_cfg.get('factor', 0.5)),
            patience=int(s_cfg.get('patience', 5)),
            threshold=float(s_cfg.get('threshold', 0.001)),
            min_lr=float(s_cfg.get('min_lr', 1e-7)),
            verbose=True
        )
        logger.info(f"Scheduler: ReduceLROnPlateau (Metric={metric}, Mode={mode})")
        
    elif s_type == 'cosine_annealing':
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=int(s_cfg.get('T_max', num_epochs)),
            eta_min=float(s_cfg.get('eta_min', 1e-7))
        )
        logger.info(f"Scheduler: CosineAnnealingLR")
        
    elif s_type == 'step':
        scheduler = StepLR(
            optimizer,
            step_size=int(s_cfg.get('step_size', 10)),
            gamma=float(s_cfg.get('gamma', 0.1))
        )
        logger.info(f"Scheduler: StepLR")
        
    else:
        logger.warning(f"Unknown scheduler type: {s_type}. None used.")
        
    return scheduler, metric, mode


def train_model(config: Dict[str, Any]) -> Tuple[Optional[str], Optional[Dict[str, List[float]]]]:
    """
    Main function to train the model using chunked data loading.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Tuple of (path to best model, training history)
    """
    run_output_dir = config["output"]["run_dir"]
    models_dir = config["output"]["models_dir"]
    ensure_dir(models_dir)
    
    log_section_header(logger, f"STARTING TRAINING RUN: {config['output']['run_name']} (Chunked Loading)")
    log_memory_usage(logger)
    
    # Set up environment
    device = get_device(config["system_utilization"]["adjust_for_gpu"])
    seed = config['training']['seed']
    np.random.seed(seed)
    torch.manual_seed(seed)
    if device.type == 'cuda':
        torch.cuda.manual_seed_all(seed)
        
    log_timing = config.get('logging', {}).get('log_timing', False)
    timing = Timing() if log_timing else None

    # --- Load Datasets & DataLoaders ---
    with log_stage("DATA_PREPARATION", "Creating Datasets & DataLoaders (Chunked)"):
        try:
            master_samples_path = os.path.join(config["data"]["processed_dir"], config["data"]["master_samples_file"])
            if not os.path.exists(master_samples_path):
                raise FileNotFoundError(f"Master sample file missing: {master_samples_path}")
                
            voxel_hdf5_path = config['input']['voxel_file']
            if not os.path.exists(voxel_hdf5_path):
                raise FileNotFoundError(f"HDF5 file missing: {voxel_hdf5_path}")

            # Set up voxel shape information
            voxel_c = config['model']['input_channels']
            voxel_d = config['model'].get('voxel_depth', 21)
            voxel_h = config['model'].get('voxel_height', 21)
            voxel_w = config['model'].get('voxel_width', 21)
            voxel_shape = (voxel_c, voxel_d, voxel_h, voxel_w)
            expected_channels = voxel_c

            # Load temp scaling params with better error handling
            temp_scaling_params = None
            scaler_path = config.get('data', {}).get('temp_scaling_params_file')
            if scaler_path and os.path.exists(scaler_path):
                try:
                    temp_scaling_params = load_json(scaler_path)
                    logger.info(f"Loaded temp scaling: {temp_scaling_params}")
                except Exception as e:
                    logger.error(f"Failed to load scaler file '{scaler_path}': {e}")
                    
            if not temp_scaling_params:
                temp_scaling_params = {'temp_min': 280.0, 'temp_max': 360.0}
                logger.warning(f"Using default temp scaling: {temp_scaling_params}")
                
            config.setdefault('runtime', {})['temp_scaling_params'] = temp_scaling_params

            # Load domain lists
            train_split_file = config['input'].get('train_split_file')
            val_split_file = config['input'].get('val_split_file')
            
            if not train_split_file or not os.path.exists(train_split_file):
                raise FileNotFoundError(f"Train split file missing: {train_split_file}")
                
            if not val_split_file or not os.path.exists(val_split_file):
                raise FileNotFoundError(f"Validation split file missing: {val_split_file}")
                
            train_domain_list = load_list_from_file(train_split_file)
            val_domain_list = load_list_from_file(val_split_file)
            
            if not train_domain_list:
                raise ValueError("Training domain list is empty.")
                
            if not val_domain_list:
                raise ValueError("Validation domain list is empty.")

            # Estimate samples for progress bars (handle PyArrow availability correctly)
            try:
                import pyarrow.parquet as pq
                PYARROW_AVAILABLE = True
            except ImportError:
                PYARROW_AVAILABLE = False
                
            try:
                logger.debug("Estimating samples per epoch for progress bars...")
                n_train, n_val = 0, 0
                
                if PYARROW_AVAILABLE:
                    train_filter = [('split', '==', 'train')]
                    val_filter = [('split', '==', 'val')]
                    train_meta = pq.read_table(master_samples_path, columns=['split'], filters=train_filter)
                    val_meta = pq.read_table(master_samples_path, columns=['split'], filters=val_filter)
                    n_train = len(train_meta)
                    n_val = len(val_meta)
                    del train_meta, val_meta  # Free memory
                else:
                    # Fallback: load full and filter (slower, more memory)
                    df_full = pd.read_csv(master_samples_path, usecols=['split'])
                    n_train = len(df_full[df_full['split'] == 'train'])
                    n_val = len(df_full[df_full['split'] == 'val'])
                    del df_full
                    
                config['runtime']['estimated_samples_per_epoch'] = n_train
                config['runtime']['estimated_val_samples'] = n_val
                logger.info(f"Estimated samples: Train={n_train}, Val={n_val}")
                gc.collect()
            except Exception as e:
                logger.warning(f"Could not estimate sample counts from {master_samples_path}: {e}. "
                              f"Progress bars may be indefinite.")

            # Instantiate Datasets using ChunkedVoxelDataset
            train_chunk_size = config['training'].get('chunk_size', 100)
            logger.info(f"Creating training dataset (ChunkedIterable, chunk_size={train_chunk_size})")
            train_dataset = ChunkedVoxelDataset(
                master_samples_path=master_samples_path, 
                split='train', 
                domain_list=train_domain_list,
                voxel_hdf5_path=voxel_hdf5_path, 
                temp_scaling_params=temp_scaling_params,
                chunk_size=train_chunk_size, 
                shuffle_domain_list=True  # Shuffle for training
            )
            
            # Added metadata info logging for better debugging
            logger.info(f"Train dataset created with {len(train_domain_list)} domains and {len(train_dataset.metadata_lookup)} metadata entries")
            
            logger.info(f"Creating validation dataset (ChunkedIterable, chunk_size={train_chunk_size})")
            val_dataset = ChunkedVoxelDataset(
                master_samples_path=master_samples_path, 
                split='val', 
                domain_list=val_domain_list,
                voxel_hdf5_path=voxel_hdf5_path, 
                temp_scaling_params=temp_scaling_params,
                chunk_size=train_chunk_size, 
                shuffle_domain_list=False  # No shuffle for validation
            )
            
            logger.info(f"Val dataset created with {len(val_domain_list)} domains and {len(val_dataset.metadata_lookup)} metadata entries")

            # Instantiate DataLoaders
            train_batch_size = config['training']['batch_size']
            val_batch_size = train_batch_size * 2  # Larger batches for validation
            num_workers = config['training'].get('num_workers', 4)
            pin_memory = config['training'].get('pin_memory', True) and (device.type == 'cuda')

            logger.info(f"Train DataLoader: Chunked, BatchSize={train_batch_size}, Workers={num_workers}")
            train_loader = DataLoader(
                train_dataset, 
                batch_size=train_batch_size, 
                num_workers=num_workers,
                pin_memory=pin_memory, 
                worker_init_fn=worker_init_fn, 
                collate_fn=simple_collate_fn,
                prefetch_factor=config['training'].get('prefetch_factor', 2) if num_workers > 0 else None
            )
            
            logger.info(f"Val DataLoader: Chunked, BatchSize={val_batch_size}, Workers={num_workers}")
            val_loader = DataLoader(
                val_dataset, 
                batch_size=val_batch_size, 
                num_workers=num_workers,
                pin_memory=pin_memory, 
                worker_init_fn=worker_init_fn, 
                collate_fn=simple_collate_fn,
                prefetch_factor=config['training'].get('prefetch_factor', 2) if num_workers > 0 else None
            )

        except Exception as e:
            logger.exception(f"Fatal error creating Datasets/DataLoaders: {e}")
            return None, None

    # --- Model & Optimizer ---
    with log_stage("MODEL_CREATION", "Creating model and optimizer"):
        try:
            input_shape = voxel_shape
            logger.info(f"Using model input shape: {input_shape}")
            model = get_model(config['model'], input_shape=input_shape)
            model.to(device)
            
            # Log model size
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            logger.info(f"Model '{config['model']['architecture']}' created. "
                       f"Params: Total={total_params:,}, Trainable={trainable_params:,}")
                       
            # Create optimizer and scheduler
            optimizer = get_optimizer(model, config)
            num_epochs = config['training']['num_epochs']
            scheduler, monitor_metric, scheduler_mode = get_scheduler(optimizer, config, num_epochs)
        except Exception as e:
            logger.exception(f"Fatal error creating model/optimizer: {e}")
            return None, None

    # --- Resume / Training Prep ---
    start_epoch = 0
    history = {
        'train_loss': [], 
        'val_loss': [], 
        'train_pearson': [], 
        'val_pearson': [], 
        'lr': [], 
        'epoch_time': []
    }
    best_metric_value = -float('inf') if scheduler_mode == 'max' else float('inf')
    best_epoch = -1
    
    resume_path = config['training'].get('resume_checkpoint')
    if resume_path and os.path.exists(resume_path):
        logger.info(f"Attempting resume from: {resume_path}")
        try:
            checkpoint = torch.load(resume_path, map_location=device)
            
            # Load model state dict first
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)  # Allow non-strict for flexibility
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint.get('epoch', -1) + 1
            history = checkpoint.get('history', history)
            
            # Load saved best metric value from checkpoint
            best_metric_value_saved = checkpoint.get(f'best_{monitor_metric}', best_metric_value)
            # Ensure compatibility if metric name changed
            if isinstance(best_metric_value_saved, (float, int)):
                best_metric_value = best_metric_value_saved
                
            best_epoch = checkpoint.get('best_epoch', -1)
            
            # Load scheduler state if compatible
            if scheduler and 'scheduler_state_dict' in checkpoint and type(scheduler).__name__ == checkpoint.get('scheduler_type'):
                try:
                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    logger.info("Resuming scheduler state.")
                except Exception as sch_e:
                    logger.warning(f"Could not load scheduler state: {sch_e}")
                    
            # Update config with potentially loaded temp params from checkpoint for consistency
            if 'config' in checkpoint and 'runtime' in checkpoint['config'] and 'temp_scaling_params' in checkpoint['config']['runtime']:
                config['runtime']['temp_scaling_params'] = checkpoint['config']['runtime']['temp_scaling_params']
                logger.info("Loaded temp scaling params from checkpoint into runtime config.")

            logger.info(f"Resumed from epoch {start_epoch}. Best '{monitor_metric}': {best_metric_value:.6f} @ epoch {best_epoch}")
            del checkpoint
            gc.collect()
        except Exception as e:
            logger.exception(f"Failed resume: {e}. Starting fresh.")
            start_epoch = 0
            history = {
                'train_loss': [], 
                'val_loss': [], 
                'train_pearson': [], 
                'val_pearson': [], 
                'lr': [], 
                'epoch_time': []
            }
            best_metric_value = -float('inf') if scheduler_mode == 'max' else float('inf')
            best_epoch = -1

    # --- Training Loop Setup ---
    criterion = nn.MSELoss()
    mixed_precision_enabled = config['training']['mixed_precision'].get('enabled', False)
    use_amp = mixed_precision_enabled and device.type == 'cuda'
    
    if use_amp:
        logger.info("Using AMP")
        scaler = torch.cuda.amp.GradScaler(enabled=True)
    else:
        logger.info("AMP disabled")
        scaler = torch.cuda.amp.GradScaler(enabled=False)
        
    # Early stopping configuration
    early_stopping_cfg = config['training'].get('early_stopping', {})
    early_stopping_enabled = early_stopping_cfg.get('enabled', True)
    early_stopping_patience = early_stopping_cfg.get('patience', 10)
    early_stopping_delta = early_stopping_cfg.get('min_delta', 0.001)
    early_stopping_counter = 0
    early_stopping_metric = early_stopping_cfg.get('monitor_metric', monitor_metric)
    early_stopping_mode = early_stopping_cfg.get('mode', scheduler_mode)
    
    if early_stopping_metric != monitor_metric or early_stopping_mode != scheduler_mode:
        logger.warning(f"Early stopping metric/mode differs. Using scheduler's.")
        early_stopping_metric = monitor_metric
        early_stopping_mode = scheduler_mode
        
    # Best model saving configuration
    save_best_metric = config['training'].get('save_best_metric', monitor_metric)
    save_best_mode = config['training'].get('save_best_mode', scheduler_mode)
    
    # Gradient accumulation for large batches
    gradient_accumulation_steps = config['training'].get('gradient_accumulation_steps', 1)
    if gradient_accumulation_steps > 1:
        logger.info(f"Using gradient accumulation: {gradient_accumulation_steps} steps")
        logger.info(f"Effective batch size: {config['training']['batch_size'] * gradient_accumulation_steps}")

    # --- TRAINING LOOP ---
    start_train_loop = time.time()
    log_section_header(logger, f"STARTING TRAINING LOOP (Epochs {start_epoch+1} to {num_epochs})")
    for epoch in range(start_epoch, num_epochs):
        epoch_start_time = time.time()
        
        # Train epoch
        avg_epoch_train_loss, avg_epoch_train_corr = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch, config, scaler, timing
        )
        
        # Validation epoch
        avg_epoch_val_loss, avg_epoch_val_corr = validate(
            model, val_loader, criterion, device, config, timing
        )
        
        # --- Log, History, Scheduler ---
        history['train_loss'].append(avg_epoch_train_loss)
        history['train_pearson'].append(avg_epoch_train_corr)
        history['val_loss'].append(avg_epoch_val_loss)
        history['val_pearson'].append(avg_epoch_val_corr)
        history['lr'].append(optimizer.param_groups[0]['lr'])
        
        epoch_duration = time.time() - epoch_start_time
        history['epoch_time'].append(epoch_duration)
        
        logger.info(f"--- Epoch {epoch+1}/{num_epochs} --- | Time: {epoch_duration:.1f}s | "
                  f"LR: {history['lr'][-1]:.2e}")
        logger.info(f"  Train -> Loss: {avg_epoch_train_loss:.6f} | Pearson: {avg_epoch_train_corr:.6f}")
        logger.info(f"  Valid -> Loss: {avg_epoch_val_loss:.6f} | Pearson: {avg_epoch_val_corr:.6f}")
        
        # Update scheduler
        current_metric_val = avg_epoch_val_corr if monitor_metric == 'val_pearson' else avg_epoch_val_loss
        if scheduler:
            if isinstance(scheduler, ReduceLROnPlateau):
                scheduler.step(current_metric_val)
            else:
                scheduler.step()
                
        # --- Checkpointing & Early Stopping ---
        is_best = False
        if not np.isnan(current_metric_val):
            if save_best_mode == 'max' and current_metric_val > best_metric_value + early_stopping_delta:
                is_best = True
            elif save_best_mode == 'min' and current_metric_val < best_metric_value - early_stopping_delta:
                is_best = True
        else:
            logger.warning(f"Ep {epoch+1}: Monitored metric '{save_best_metric}' NaN.")
            
        # Create checkpoint data
        checkpoint_data = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'config': config,  # Save potentially updated config
            'input_shape': input_shape,
            'temp_scaling_params': config['runtime']['temp_scaling_params'],  # Explicitly save params used
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'scheduler_type': type(scheduler).__name__ if scheduler else None,
            f'best_{save_best_metric}': best_metric_value,
            'best_epoch': best_epoch  # Save best info in all checkpoints
        }

        if is_best:
            logger.info(f"  >>> New best {save_best_metric}: {current_metric_val:.6f}. Saving best model...")
            best_metric_value = current_metric_val
            best_epoch = epoch + 1
            early_stopping_counter = 0
            
            best_model_path = os.path.join(models_dir, "best_model.pt")
            best_checkpoint_data = checkpoint_data.copy()  # Already includes best info
            try:
                torch.save(best_checkpoint_data, best_model_path)
            except Exception as e:
                logger.error(f"Failed save best model: {e}")
        else:
            early_stopping_counter += 1
            logger.info(f"  {save_best_metric} did not improve. Best: {best_metric_value:.6f} @ Ep {best_epoch}. "
                      f"EarlyStop: {early_stopping_counter}/{early_stopping_patience}")

        # Save periodic checkpoints if configured
        chkpt_interval = config['training'].get('checkpoint_interval', 0)
        if chkpt_interval > 0 and (epoch + 1) % chkpt_interval == 0:
            chkpt_path = os.path.join(models_dir, f"checkpoint_epoch_{epoch+1}.pt")
            logger.info(f"Saving periodic checkpoint: {chkpt_path}")
            periodic_checkpoint_data = checkpoint_data.copy()
            periodic_checkpoint_data['history'] = history
            try:
                torch.save(periodic_checkpoint_data, chkpt_path)
            except Exception as e:
                logger.error(f"Failed save periodic ckpt: {e}")
                
        # Always save latest model
        latest_model_path = os.path.join(models_dir, "latest_model.pt")
        latest_checkpoint_data = checkpoint_data.copy()
        latest_checkpoint_data['history'] = history
        try:
            torch.save(latest_checkpoint_data, latest_model_path)
        except Exception as e:
            logger.error(f"Failed save latest model: {e}")
            
        # Check for early stopping
        if early_stopping_enabled and early_stopping_counter >= early_stopping_patience:
            logger.info(f"Early stopping triggered at epoch {epoch+1}.")
            break
            
        # Clean up memory
        clear_memory(force_gc=True, clear_cuda=(device.type == 'cuda'))
        if config.get('logging', {}).get('log_memory_usage', True):
            log_memory_usage(logger)
            
        # Log timing stats if enabled
        if timing:
            stats = timing.get_stats()
            logger.debug("--- Epoch Timing Stats ---")
            for section, s_data in stats.items():
                logger.debug(f"  {section:<20}: avg={s_data['avg']:.4f}s, total={s_data['total']:.2f}s, count={s_data['count']}")
            logger.debug("--------------------------")

    # --- Finalization ---
    training_duration = time.time() - start_train_loop
    log_section_header(logger, "TRAINING FINISHED")
    logger.info(f"Total Training Time: {training_duration:.2f}s ({training_duration/60:.1f} mins)")
    
    final_model_path = None
    if best_epoch != -1:
        logger.info(f"Best {save_best_metric}: {best_metric_value:.6f} @ epoch {best_epoch}")
        final_model_path = os.path.join(models_dir, "best_model.pt")
    else:
        logger.warning("No improvement detected during training.")
        final_model_path = os.path.join(models_dir, "latest_model.pt")
        logger.info(f"Using latest model: {final_model_path}")
        
    if not final_model_path or not os.path.exists(final_model_path):
        logger.error("Could not find final model.")
        final_model_path = None
        
    # Save training history
    history_path = os.path.join(run_output_dir, "training_history.json")
    try:
        save_json(history, history_path)
        logger.info(f"History saved: {history_path}")
    except Exception as e:
        logger.error(f"Failed save history: {e}")
        
    # Generate basic plots if visualization module is available
    try:
        from voxelflex.cli.commands.visualize import create_loss_curve, create_correlation_curve
        viz_dir = config["output"]["visualizations_dir"]
        logger.info("Generating final plots...")
        
        if viz_dir:
            if history.get('train_loss') and history.get('val_loss'):
                create_loss_curve(
                    history, viz_dir, 
                    save_format=config['visualization']['save_format'], 
                    dpi=config['visualization']['dpi']
                )
                
            if history.get('train_pearson') and history.get('val_pearson'):
                create_correlation_curve(
                    history, viz_dir, 
                    save_format=config['visualization']['save_format'], 
                    dpi=config['visualization']['dpi']
                )
    except ImportError:
        logger.warning("Could not import visualize functions.")
        
    # Final cleanup
    clear_memory(force_gc=True, clear_cuda=(device.type == 'cuda'))
    return final_model_path, history
==========================================================
===== FILE: src/voxelflex/cli/commands/predict.py =====
==========================================================

# src/voxelflex/cli/commands/predict.py
"""
Prediction command for VoxelFlex (Temperature-Aware).

Loads a trained model and predicts RMSF for specified domains at a given temperature.
Uses on-demand loading of raw voxel data.
"""
import os
import time
import json
import logging
import gc
from typing import Dict, Any, List, Optional, Tuple, Callable

import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader

# Use centralized logger
logger = logging.getLogger("voxelflex.cli.predict")

# Project imports
from voxelflex.data.data_loader import (
    PredictionDataset,
    load_process_voxels_from_hdf5, # Use the robust HDF5 loader
    load_aggregated_rmsf_data,
    create_domain_mapping
)
from voxelflex.models.cnn_models import get_model
from voxelflex.utils.logging_utils import EnhancedProgressBar, log_memory_usage, log_stage, log_section_header
from voxelflex.utils.file_utils import ensure_dir, resolve_path, save_json, load_json # Import load_json
from voxelflex.utils.system_utils import get_device, clear_memory, check_memory_usage
from voxelflex.utils.temp_scaling import get_temperature_scaler

def predict_rmsf(
    config: Dict[str, Any],
    model_path: str,
    target_temperature: float,
    domain_ids_to_predict: Optional[List[str]] = None,
    output_csv_filename: Optional[str] = None # Allow specifying output filename
 ) -> Optional[str]:
    """
    Runs prediction for specified domains at a target temperature.

    Args:
        config: Configuration dictionary.
        model_path: Path to the trained model checkpoint (.pt).
        target_temperature: The temperature (in K) for which to predict RMSF.
        domain_ids_to_predict: Optional list of specific HDF5 domain keys to predict.
                                If None, uses domains from the test split file defined in config.
        output_csv_filename: Optional base filename for the output CSV.

    Returns:
        Path to the saved predictions CSV file, or None on failure.
    """
    run_output_dir = config["output"]["run_dir"]
    metrics_dir = config["output"]["metrics_dir"]
    ensure_dir(metrics_dir)

    # Determine output filename
    if output_csv_filename is None:
        # Generate default name if not provided
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        temp_str = f"{target_temperature:.0f}K"
        model_name = os.path.splitext(os.path.basename(model_path))[0]
        output_csv_filename = f"predictions_{model_name}_{temp_str}_{timestamp}.csv"

    # Construct full path
    predictions_path = os.path.join(metrics_dir, output_csv_filename)

    log_section_header(logger, f"STARTING PREDICTION at {target_temperature:.1f}K")
    logger.info(f"Model: {model_path}")
    logger.info(f"Output CSV: {predictions_path}")
    log_memory_usage(logger)
    device = get_device(config["system_utilization"].get("adjust_for_gpu", True))
    clear_memory(force_gc=True, clear_cuda=(device.type == 'cuda'))

    model = None # Ensure model is defined for finally block
    try:
        # --- Load Model & Temperature Scaler ---
        with log_stage("SETUP", "Loading model and temperature scaler"):
            try:
                logger.info(f"Loading checkpoint from: {model_path}")
                checkpoint = torch.load(model_path, map_location='cpu') # Load to CPU first

                # Extract necessary info from checkpoint
                model_cfg_from_ckpt = checkpoint.get('config', {}).get('model', {})
                input_shape = checkpoint.get('input_shape')
                scaling_params_from_ckpt = checkpoint.get('temp_scaling_params')
                train_cfg_from_ckpt = checkpoint.get('config', {}).get('training', {}) # For mixed precision setting

                if not model_cfg_from_ckpt: raise ValueError("Model config section missing from checkpoint.")
                if not input_shape: raise ValueError("Model input_shape missing from checkpoint.")
                if not scaling_params_from_ckpt:
                     logger.warning("Temperature scaling parameters missing from checkpoint. Attempting to load from config path...")
                     # Try loading from the path specified in the *current* config, assuming it matches the run
                     scaler_path_from_config = config["data"]["temp_scaling_params_file"]
                     if os.path.exists(scaler_path_from_config):
                         scaling_params_from_ckpt = load_json(scaler_path_from_config)
                         logger.info(f"Loaded scaling params from config path: {scaler_path_from_config}")
                     else:
                          raise ValueError("Temperature scaling parameters missing from checkpoint and config path.")

                # Create model instance
                model = get_model(model_cfg_from_ckpt, input_shape=input_shape)
                # Load weights
                missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                if missing_keys: logger.warning(f"Prediction Load Warning: Missing keys in model state_dict: {missing_keys}")
                if unexpected_keys: logger.warning(f"Prediction Load Warning: Unexpected keys in model state_dict: {unexpected_keys}")

                model.to(device)
                model.eval() # Set to evaluation mode
                logger.info(f"Loaded model '{model_cfg_from_ckpt.get('architecture')}' to {device}.")

                # Get temperature scaler function
                temp_scaler_func = get_temperature_scaler(params=scaling_params_from_ckpt)
                scaled_target_temp_value = temp_scaler_func(target_temperature)
                logger.info(f"Target temp {target_temperature}K scaled to: {scaled_target_temp_value:.4f}")
                # Create tensor for prediction - can be expanded per batch later
                scaled_target_temp_tensor_base = torch.tensor([[scaled_target_temp_value]], dtype=torch.float32).to(device)

                del checkpoint; gc.collect() # Free checkpoint memory

            except Exception as e:
                logger.exception(f"Error loading model or temperature scaler: {e}")
                return None


        # --- Prepare Domain List ---
        with log_stage("SETUP", "Preparing domain list for prediction"):
            if domain_ids_to_predict is None:
                logger.info("No specific domains provided. Using domains from test split file.")
                test_split_file = config['input'].get('test_split_file')
                if not test_split_file or not os.path.exists(test_split_file):
                    logger.error(f"Test split file not found or specified ({test_split_file}). Cannot determine domains to predict.")
                    return None
                domain_ids_to_predict = load_list_from_file(test_split_file)
                if not domain_ids_to_predict:
                     logger.error(f"Test split file ({test_split_file}) is empty. No domains to predict.")
                     return None
                logger.info(f"Loaded {len(domain_ids_to_predict)} domains from test split file.")

            # Ensure the list contains unique domains
            domain_ids_to_predict = sorted(list(set(domain_ids_to_predict)))
            logger.info(f"Predicting for {len(domain_ids_to_predict)} unique domains.")


        # --- Identify Samples to Predict (Domain + Residue from HDF5) ---
        with log_stage("SETUP", "Identifying target residues from HDF5"):
            samples_to_predict: List[Tuple[str, str]] = []
            try:
                 with h5py.File(config["input"]["voxel_file"], 'r') as f_h5:
                      for domain_id in domain_ids_to_predict:
                           if domain_id not in f_h5:
                                logger.warning(f"Requested prediction domain '{domain_id}' not found in HDF5. Skipping.")
                                continue
                           # Find residue group (same logic as preprocess/data_loader)
                           domain_group = f_h5[domain_id]
                           residue_group = None
                           potential_chain_keys = sorted([k for k in domain_group.keys() if isinstance(domain_group[k], h5py.Group)])
                           for chain_key in potential_chain_keys:
                                try:
                                     potential_residue_group = domain_group[chain_key]
                                     if any(key.isdigit() for key in potential_residue_group.keys()):
                                          residue_group = potential_residue_group; break
                                except Exception: continue

                           if residue_group:
                                for resid_str in residue_group.keys():
                                     if resid_str.isdigit():
                                          samples_to_predict.append((domain_id, resid_str))
                           else:
                                logger.warning(f"No residue group found for domain '{domain_id}'. Skipping.")
            except Exception as e:
                 logger.exception(f"Error reading HDF5 to identify target residues: {e}")
                 return None

            if not samples_to_predict:
                 logger.error("No valid residues found in HDF5 for the specified domains. Cannot run prediction.")
                 return None
            logger.info(f"Identified {len(samples_to_predict)} target residues from HDF5.")


        # --- Create Dataset & DataLoader ---
        # PredictionDataset now handles pre-loading/filtering based on voxel availability
        with log_stage("SETUP", "Creating Prediction Dataset and DataLoader"):
            try:
                pred_dataset = PredictionDataset(
                    samples_to_load=samples_to_predict,
                    voxel_hdf5_path=config["input"]["voxel_file"],
                    expected_channels=model_cfg_from_ckpt['input_channels'], # Use channels from loaded model config
                    target_shape_chw=input_shape # Validate against loaded model shape
                )

                if len(pred_dataset) == 0:
                    logger.error("Prediction dataset is empty after attempting to load voxels. Cannot proceed.")
                    return None

                # Use configured prediction batch size
                pred_batch_size = config.get('prediction', {}).get('batch_size', 128)
                pred_loader = DataLoader(
                    pred_dataset,
                    batch_size=pred_batch_size,
                    shuffle=False, # Important for consistent output order if needed
                    num_workers=0, # Keep prediction simple, load in main thread
                    pin_memory=False # Not typically needed for CPU->GPU transfer here
                )
                logger.info(f"Prediction DataLoader created: {len(pred_loader)} batches, BatchSize={pred_batch_size}.")

            except Exception as e:
                logger.exception(f"Error creating prediction dataset/loader: {e}")
                return None

        # --- Load Optional Resname Information (for output CSV) ---
        resname_lookup: Dict[Tuple[str, int], str] = {}
        try:
             with log_stage("SETUP", "Loading residue name information (optional)"):
                  rmsf_df_for_names = load_aggregated_rmsf_data(config['input']['aggregated_rmsf_file'])
                  # We only need domain_id, resid, resname
                  resname_df = rmsf_df_for_names[['domain_id', 'resid', 'resname']].copy()
                  resname_df['resid'] = pd.to_numeric(resname_df['resid'], errors='coerce').astype('Int64')
                  resname_df.dropna(inplace=True)
                  resname_df.drop_duplicates(subset=['domain_id', 'resid'], keep='first', inplace=True)
                  resname_map = resname_df.set_index(['domain_id', 'resid'])['resname'].to_dict()

                  # Create lookup using HDF5 key -> RMSF mapping if needed (or assume direct match?)
                  # Let's assume prediction uses HDF5 keys, so we need mapping
                  hdf5_keys_in_dataset = list(set(s[0] for s in pred_dataset.samples))
                  domain_mapping_pred = create_domain_mapping(hdf5_keys_in_dataset, resname_df['domain_id'].unique().tolist())

                  # Populate the lookup for samples that are actually in the dataset
                  for domain_id, resid_str in pred_dataset.samples:
                       try:
                            resid_int = int(resid_str)
                            mapped_rmsf_id = domain_mapping_pred.get(domain_id, domain_id) # Fallback to original ID
                            resname = resname_map.get((mapped_rmsf_id, resid_int))
                            if resname is None: # Try base name if mapped failed
                                 base_rmsf_id = mapped_rmsf_id.split('_')[0]
                                 if base_rmsf_id != mapped_rmsf_id:
                                      resname = resname_map.get((base_rmsf_id, resid_int))

                            resname_lookup[(domain_id, resid_int)] = resname if resname else "UNK"
                       except ValueError: continue # Skip if resid_str is not int

                  logger.info(f"Residue name lookup created with {len(resname_lookup)} entries.")
                  del rmsf_df_for_names, resname_df, resname_map, domain_mapping_pred; gc.collect()
        except Exception as e:
             logger.warning(f"Could not load residue names from RMSF file: {e}. Resnames will be 'UNK'.")


        # --- Run Prediction Loop ---
        with log_stage("PREDICTION", "Running inference loop"):
            logger.info("Starting prediction loop...")
            results_list = []
            progress = EnhancedProgressBar(len(pred_loader), desc=f"Predict {target_temperature:.0f}K")
            # Use mixed precision if enabled during training
            autocast_enabled = train_cfg_from_ckpt.get('mixed_precision', {}).get('enabled', False) and device.type == 'cuda'

            with torch.no_grad(): # Ensure no gradients are calculated
                for i, batch_data in enumerate(pred_loader):
                    # batch_data is (list_of_domain_ids, list_of_resid_strs, voxel_tensor)
                    if not batch_data or len(batch_data) != 3:
                        logger.warning(f"Skipping invalid batch data at index {i}.")
                        continue

                    batch_domains, batch_resids_str, batch_voxels = batch_data
                    # Check if batch is effectively empty
                    if not batch_domains or batch_voxels.numel() == 0:
                        logger.warning(f"Skipping empty batch {i+1}.")
                        continue

                    try:
                        batch_voxels = batch_voxels.to(device, non_blocking=True)
                        current_batch_size = batch_voxels.size(0)
                        # Expand the base scaled temperature tensor for the current batch size
                        batch_scaled_temps = scaled_target_temp_tensor_base.expand(current_batch_size, 1)

                        # Run model inference
                        with torch.autocast(device_type=device.type, enabled=autocast_enabled):
                             batch_outputs = model(voxel_input=batch_voxels, scaled_temp=batch_scaled_temps)

                        # Process outputs
                        preds_np = batch_outputs.detach().cpu().numpy().flatten()

                        # Collect results for this batch
                        for j in range(current_batch_size):
                             domain_id = batch_domains[j]
                             resid_str = batch_resids_str[j]
                             try:
                                  resid_int = int(resid_str)
                                  resname = resname_lookup.get((domain_id, resid_int), "UNK") # Get resname
                                  results_list.append({
                                       'domain_id': domain_id, # Use HDF5 key as identifier
                                       'resid': resid_int,
                                       'resname': resname,
                                       'predicted_rmsf': preds_np[j],
                                       'prediction_temperature': target_temperature
                                  })
                             except ValueError:
                                  logger.warning(f"Invalid residue ID format encountered in prediction output: {domain_id}:{resid_str}")
                             except IndexError:
                                  logger.warning(f"Index error accessing prediction output for batch {i+1}, item {j}")

                    except Exception as e:
                         logger.exception(f"Error predicting batch {i+1}: {e}")
                         # Continue to next batch if one fails

                    progress.update(i + 1)
                    del batch_voxels, batch_scaled_temps, batch_outputs, preds_np # Clean up batch tensors
                    if i % 50 == 0: clear_memory(force_gc=False, clear_cuda=(device.type=='cuda'))

            progress.finish()
            logger.info(f"Prediction loop finished. Collected {len(results_list)} results.")


        # --- Save Results ---
        with log_stage("OUTPUT", "Saving prediction results"):
            if results_list:
                try:
                    results_df = pd.DataFrame(results_list)
                    # Ensure correct order of columns
                    output_cols = ['domain_id', 'resid', 'resname', 'prediction_temperature', 'predicted_rmsf']
                    results_df = results_df[output_cols]
                    results_df.to_csv(predictions_path, index=False, float_format='%.6f')
                    logger.info(f"Predictions saved successfully to: {predictions_path}")
                except Exception as e:
                    logger.exception(f"Failed to save predictions CSV to {predictions_path}: {e}")
                    return None # Indicate failure
            else:
                logger.warning("No results generated during prediction loop. Output file will not be created.")
                return None # Indicate no results

        logger.info("Prediction command finished successfully.")
        return predictions_path # Return path on success

    finally:
        # Cleanup
        del model # Ensure model is deleted
        clear_memory(force_gc=True, clear_cuda=(device.type == 'cuda'))
        log_memory_usage(logger)


==========================================================
===== FILE: src/voxelflex/cli/commands/evaluate.py =====
==========================================================

"""
Evaluation command for VoxelFlex (Temperature-Aware).

Calculates performance metrics by comparing predictions against ground truth
from the aggregated RMSF data. Includes stratification and permutation importance.
"""

import os
import time
import json
import logging
from typing import Dict, Any, Optional, List, Tuple, Callable, DefaultDict
from collections import defaultdict

import numpy as np
import pandas as pd
import torch
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import pearsonr, spearmanr
import torch.nn as nn
from torch.utils.data import DataLoader, SequentialSampler

# Use centralized logger
logger = logging.getLogger("voxelflex.cli.evaluate")

# Project imports
from voxelflex.utils.logging_utils import (
    get_logger, log_stage, log_memory_usage, log_section_header, EnhancedProgressBar
)
from voxelflex.utils.file_utils import ensure_dir, save_json, load_json, resolve_path
from voxelflex.utils.system_utils import clear_memory, check_memory_usage, get_device
from voxelflex.models.cnn_models import get_model
from voxelflex.utils.temp_scaling import get_temperature_scaler
from voxelflex.data.data_loader import (
    PredictionDataset,
    load_process_domain_from_handle,  # Use robust HDF5 loader for permutation importance
    load_aggregated_rmsf_data,
    create_domain_mapping  # Needed for resname lookup
)

# --- Metric Calculation Helpers ---

def safe_metric(metric_func: Callable, y_true: np.ndarray, y_pred: np.ndarray, **kwargs) -> float:
    """
    Safely compute metric, handling NaNs, Infs, and insufficient data.
    
    Args:
        metric_func: Metric function to call (e.g., pearsonr, r2_score)
        y_true: Ground truth values
        y_pred: Predicted values
        **kwargs: Additional arguments for metric function
        
    Returns:
        Computed metric value or NaN if calculation fails
    """
    try:
        # Ensure inputs are numpy arrays and float64 for precision
        x_np = np.asarray(y_true).astype(np.float64)
        y_np = np.asarray(y_pred).astype(np.float64)

        # Filter out non-finite values
        valid_mask = np.isfinite(x_np) & np.isfinite(y_np)
        x_clean = x_np[valid_mask]
        y_clean = y_np[valid_mask]

        # Check for sufficient data points
        if len(x_clean) < 2:
            return np.nan

        # Check for zero variance in correlation metrics
        is_correlation = 'pearsonr' in metric_func.__name__ or 'spearmanr' in metric_func.__name__
        if is_correlation:
            if np.std(x_clean) < 1e-8 or np.std(y_clean) < 1e-8:
                return np.nan

        # Calculate metric based on function type
        if 'pearsonr' in metric_func.__name__:
            result, _ = metric_func(x_clean, y_clean)
        elif 'spearmanr' in metric_func.__name__:
            result, _ = metric_func(x_clean, y_clean)
        else:
            result = metric_func(x_clean, y_clean, **kwargs)

        # Return result only if finite
        return float(result) if np.isfinite(result) else np.nan

    except Exception as e:
        logger.debug(f"Error calculating metric {metric_func.__name__}: {e}")
        return np.nan  # Return NaN on any exception


def calculate_metrics(df: pd.DataFrame, label: str = "Overall") -> Dict[str, float]:
    """
    Calculate standard regression metrics from a DataFrame.
    
    Args:
        df: DataFrame with 'target_rmsf' and 'predicted_rmsf' columns
        label: Label for logging purposes
        
    Returns:
        Dictionary with calculated metrics
    """
    metrics = {}
    count = len(df)
    metrics['count'] = int(count)

    if count < 2:
        logger.warning(f"Metrics ({label}): Insufficient samples ({count}) for calculation.")
        # Return dict with count and NaNs for other metrics
        nan_metrics = [
            'pearson', 'spearman', 'r2', 'mse', 'rmse', 'mae',
            'mean_absolute_relative_error', 'median_absolute_relative_error',
            'cv_rmse_percent'
        ]
        for key in nan_metrics:
            metrics[key] = np.nan
        return metrics

    if 'target_rmsf' not in df or 'predicted_rmsf' not in df:
        logger.warning(f"Metrics ({label}): Missing target_rmsf or predicted_rmsf columns.")
        return metrics  # Should not happen if merge worked, but safety check

    y_true = df['target_rmsf'].values
    y_pred = df['predicted_rmsf'].values

    # Calculate standard metrics safely
    metrics['pearson'] = safe_metric(pearsonr, y_true, y_pred)
    metrics['spearman'] = safe_metric(spearmanr, y_true, y_pred)
    metrics['r2'] = safe_metric(r2_score, y_true, y_pred)
    metrics['mse'] = safe_metric(mean_squared_error, y_true, y_pred)
    metrics['rmse'] = np.sqrt(metrics['mse']) if pd.notna(metrics['mse']) else np.nan
    metrics['mae'] = safe_metric(mean_absolute_error, y_true, y_pred)

    # Calculate relative errors safely
    try:
        valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)
        y_true_clean = y_true[valid_mask]
        y_pred_clean = y_pred[valid_mask]
        
        if len(y_true_clean) > 0:
            abs_error = np.abs(y_true_clean - y_pred_clean)
            # Avoid division by zero or very small numbers for relative error
            safe_y_true = np.maximum(np.abs(y_true_clean), 1e-6)
            relative_error = abs_error / safe_y_true
            
            # Filter NaNs/Infs that might arise from division
            finite_rel_err = relative_error[np.isfinite(relative_error)]
            
            if len(finite_rel_err) > 0:
                metrics['mean_absolute_relative_error'] = float(np.mean(finite_rel_err)) * 100
                metrics['median_absolute_relative_error'] = float(np.median(finite_rel_err)) * 100
            else:
                metrics['mean_absolute_relative_error'] = np.nan
                metrics['median_absolute_relative_error'] = np.nan
                
            # Calculate CV(RMSE)
            mean_true = np.mean(y_true_clean)
            if pd.notna(metrics['rmse']) and abs(mean_true) > 1e-6:
                metrics['cv_rmse_percent'] = (metrics['rmse'] / mean_true) * 100
            else:
                metrics['cv_rmse_percent'] = np.nan
        else:
            metrics['mean_absolute_relative_error'] = np.nan
            metrics['median_absolute_relative_error'] = np.nan
            metrics['cv_rmse_percent'] = np.nan

    except Exception as e:
        logger.warning(f"Error calculating relative/CV metrics ({label}): {e}")
        metrics['mean_absolute_relative_error'] = np.nan
        metrics['median_absolute_relative_error'] = np.nan
        metrics['cv_rmse_percent'] = np.nan

    return metrics

# --- Permutation Importance ---
def perform_permutation_importance(
    model: nn.Module,
    perm_dataset: PredictionDataset,  # Dataset containing samples for permutation
    temp_scaler_func: Callable[[float], float],
    target_temperature: float,  # The original temperature (used to find baseline performance)
    baseline_metrics: Dict[str, float],  # Baseline performance on the same dataset
    device: torch.device,
    config: Dict[str, Any],
    n_repeats: int = 5
) -> Dict[str, float]:
    """
    Performs permutation importance analysis for the temperature feature.
    
    Args:
        model: The trained model
        perm_dataset: PredictionDataset containing the samples to evaluate on
        temp_scaler_func: Function to scale raw temperatures
        target_temperature: The original temperature the baseline metrics correspond to
        baseline_metrics: Dictionary of baseline metrics (e.g., pearson, rmse)
        device: The device to run inference on
        config: Configuration dictionary
        n_repeats: Number of times to repeat the permutation
        
    Returns:
        Dictionary containing permutation importance scores (e.g., 'pearson_drop')
    """
    logger.info(f"Performing Permutation Importance for Temperature (n_repeats={n_repeats})...")
    model.eval()  # Ensure model is in eval mode
    batch_size = config.get('prediction', {}).get('batch_size', 128)
    rng = np.random.default_rng(config['training']['seed'])

    # Create a shuffling pool of scaled temperatures from the training data distribution
    # This avoids biasing the permutation test with only the target temperature
    scaled_shuffling_pool = []
    try:
        # Try loading the full RMSF file to get all unique temperatures
        rmsf_df_full = load_aggregated_rmsf_data(config['input']['aggregated_rmsf_file'])
        all_temps_unique = rmsf_df_full['temperature_feature'].dropna().unique()
        scaled_shuffling_pool = [temp_scaler_func(t) for t in all_temps_unique]
        logger.info(f"Created temperature shuffling pool with {len(scaled_shuffling_pool)} unique scaled values.")
        del rmsf_df_full
        gc.collect()
    except Exception as e:
        logger.error(f"Could not load temps for shuffling pool from {config['input']['aggregated_rmsf_file']}: {e}. "
                    f"Using default [0, 1] range.")
        # Fallback to a simple range if file loading fails
        scaled_shuffling_pool = np.linspace(0.0, 1.0, 20).tolist()  # Use 20 points between 0 and 1

    if not scaled_shuffling_pool:
        logger.warning("Temp shuffling pool is empty. Using default [0, 1] range.")
        scaled_shuffling_pool = np.linspace(0.0, 1.0, 20).tolist()

    # Get baseline metrics from parameter
    baseline_pearson = baseline_metrics.get('pearson', np.nan)
    baseline_rmse = baseline_metrics.get('rmse', np.nan)

    if pd.isna(baseline_pearson) or pd.isna(baseline_rmse):
        logger.warning("Baseline metrics (Pearson, RMSE) are missing or NaN. "
                      "Cannot calculate importance drop/increase.")

    # Store metrics from each permutation repeat
    permuted_metrics_repeats: Dict[str, List[float]] = {'pearson': [], 'rmse': []}

    # Get the ground truth targets aligned with the dataset order
    aligned_targets_np = np.full(len(perm_dataset), np.nan, dtype=np.float64)  # Initialize with NaNs
    try:
        gt_df = load_aggregated_rmsf_data(config['input']['aggregated_rmsf_file'])
        gt_df['resid'] = pd.to_numeric(gt_df['resid'], errors='coerce').astype('Int64')
        gt_df['temperature_feature'] = pd.to_numeric(gt_df['temperature_feature'], errors='coerce')
        gt_df_filtered = gt_df[np.isclose(gt_df['temperature_feature'], target_temperature)].copy()
        gt_lookup = gt_df_filtered.set_index(['domain_id', 'resid'])['target_rmsf']
        
        # Create domain mapping for this lookup
        hdf5_keys_in_dataset = list(set(s[0] for s in perm_dataset.samples))
        domain_mapping_perm = create_domain_mapping(
            hdf5_keys_in_dataset, 
            gt_df_filtered['domain_id'].unique().tolist()
        )

        for i, (domain_id, resid_str) in enumerate(perm_dataset.samples):
            try:
                resid_int = int(resid_str)
                mapped_id = domain_mapping_perm.get(domain_id, domain_id)
                target = gt_lookup.get((mapped_id, resid_int))
                
                if target is None:  # Try base name
                    base_id = mapped_id.split('_')[0]
                    if base_id != mapped_id:
                        target = gt_lookup.get((base_id, resid_int))
                        
                if target is not None and not pd.isna(target):
                    aligned_targets_np[i] = float(target)
            except (ValueError, TypeError, KeyError):
                continue  # Ignore errors for specific samples
                
        del gt_df, gt_df_filtered, gt_lookup, domain_mapping_perm
        gc.collect()
        
        if np.isnan(aligned_targets_np).all():
            raise ValueError("Could not align any ground truth targets for permutation importance samples.")
    except Exception as e:
        logger.error(f"Failed to get aligned ground truth targets for permutation importance: {e}")
        return {"error": "Failed to get ground truth targets"}

    # Use mixed precision if enabled during training (get from config)
    autocast_enabled = (config.get('training', {})
                       .get('mixed_precision', {})
                       .get('enabled', False) and device.type == 'cuda')

    # --- Permutation Loop ---
    for n in range(n_repeats):
        logger.info(f"  Permutation importance repeat {n+1}/{n_repeats}...")
        permuted_preds_batches: List[np.ndarray] = []  # Store predictions for this repeat
        
        # Create a data loader for sequential processing
        permuted_loader = DataLoader(
            perm_dataset, 
            batch_size=batch_size, 
            sampler=SequentialSampler(perm_dataset), 
            num_workers=0
        )
        progress = EnhancedProgressBar(len(permuted_loader), desc=f"  Permutation {n+1}")

        with torch.no_grad():
            for i, batch_data in enumerate(permuted_loader):
                if not batch_data or len(batch_data) != 3:
                    continue
                    
                _, _, batch_voxels = batch_data

                if batch_voxels.numel() == 0:
                    continue
                    
                batch_voxels = batch_voxels.to(device, non_blocking=True)
                current_batch_size = batch_voxels.size(0)

                # --- Permute Temperature Feature ---
                # Create permuted temperature tensor for this batch
                shuffled_scaled_values = rng.choice(scaled_shuffling_pool, size=current_batch_size, replace=True)
                batch_shuffled_temps = torch.tensor(
                    shuffled_scaled_values, device=device, dtype=torch.float32
                ).unsqueeze(1)

                try:
                    # Run inference with permuted temperature
                    with torch.autocast(device_type=device.type, enabled=autocast_enabled):
                        batch_outputs = model(voxel_input=batch_voxels, scaled_temp=batch_shuffled_temps)
                    permuted_preds_batches.append(batch_outputs.detach().cpu().numpy().flatten())
                except Exception as e:
                    logger.warning(f"Error predicting with permuted temps (batch {i}): {e}")
                    # Append NaNs if prediction fails for a batch
                    permuted_preds_batches.append(np.full(current_batch_size, np.nan))

                progress.update(1)
                del batch_voxels, batch_shuffled_temps, batch_outputs
                if i % 100 == 0:
                    clear_memory(force_gc=False, clear_cuda=True)
        progress.finish()

        # Concatenate predictions for the current repeat
        if permuted_preds_batches:
            perm_preds_np = np.concatenate(permuted_preds_batches)
            if len(perm_preds_np) != len(aligned_targets_np):
                logger.error(f"Permutation repeat {n+1}: Prediction length mismatch ({len(perm_preds_np)}) "
                            f"vs target length ({len(aligned_targets_np)}). Skipping metrics for this repeat.")
                continue

            # Calculate metrics for this repeat using the permuted predictions
            perm_pearson = safe_metric(pearsonr, aligned_targets_np, perm_preds_np)
            perm_mse = safe_metric(mean_squared_error, aligned_targets_np, perm_preds_np)
            perm_rmse = np.sqrt(perm_mse) if pd.notna(perm_mse) else np.nan

            if pd.notna(perm_pearson):
                permuted_metrics_repeats['pearson'].append(perm_pearson)
            if pd.notna(perm_rmse):
                permuted_metrics_repeats['rmse'].append(perm_rmse)
        else:
            logger.warning(f"No predictions generated for permutation repeat {n+1}.")

    # --- Calculate Final Importance Scores ---
    importance_scores: Dict[str, float] = {}
    if permuted_metrics_repeats['pearson'] and pd.notna(baseline_pearson):
        mean_perm_pearson = float(np.mean(permuted_metrics_repeats['pearson']))
        std_perm_pearson = float(np.std(permuted_metrics_repeats['pearson']))
        importance_scores['pearson_permuted_mean'] = mean_perm_pearson
        importance_scores['pearson_permuted_std'] = std_perm_pearson
        importance_scores['pearson_drop'] = baseline_pearson - mean_perm_pearson
    else:
        logger.warning("Could not calculate Pearson drop for permutation importance "
                      "(missing baseline or permuted values).")

    if permuted_metrics_repeats['rmse'] and pd.notna(baseline_rmse):
        mean_perm_rmse = float(np.mean(permuted_metrics_repeats['rmse']))
        std_perm_rmse = float(np.std(permuted_metrics_repeats['rmse']))
        importance_scores['rmse_permuted_mean'] = mean_perm_rmse
        importance_scores['rmse_permuted_std'] = std_perm_rmse
        importance_scores['rmse_increase'] = mean_perm_rmse - baseline_rmse
    else:
        logger.warning("Could not calculate RMSE increase for permutation importance "
                      "(missing baseline or permuted values).")

    logger.info("--- Permutation Importance Results ---")
    if importance_scores:
        for key, val in importance_scores.items():
            logger.info(f"  {key}: {val:.4f}")
    else:
        logger.info("  No valid permutation importance scores calculated.")

    return importance_scores


# --- Main Evaluation Function ---
def evaluate_model(
    config: Dict[str, Any],
    model_path: str,
    predictions_path: str,
) -> Optional[str]:
    """
    Evaluates model performance using a predictions file and ground truth data.
    
    Args:
        config: Configuration dictionary
        model_path: Path to the trained model checkpoint (.pt)
        predictions_path: Path to the predictions CSV file generated by 'predict'
        
    Returns:
        Path to the saved evaluation metrics JSON file, or None on failure
    """
    run_output_dir = config["output"]["run_dir"]
    metrics_dir = config["output"]["metrics_dir"]
    ensure_dir(metrics_dir)

    # Construct metrics filename based on predictions filename
    pred_basename = os.path.splitext(os.path.basename(predictions_path))[0]
    metrics_filename = f"evaluation_metrics_{pred_basename}.json"
    metrics_path = os.path.join(metrics_dir, metrics_filename)

    log_section_header(logger, "MODEL EVALUATION")
    logger.info(f"Predictions file: {predictions_path}")
    logger.info(f"Model file: {model_path}")
    logger.info(f"Output metrics JSON: {metrics_path}")
    log_memory_usage(logger)

    all_metrics: Dict[str, Any] = {  # Initialize results dict
        'overall': {},
        'stratified': {},
        'permutation_importance': {},
        'evaluation_temperature': None,
        'model_path': model_path,
        'predictions_path': predictions_path,
        'input_data': {
            'voxel_file': config['input'].get('voxel_file'),
            'aggregated_rmsf_file': config['input'].get('aggregated_rmsf_file'),
            'test_split_file': config['input'].get('test_split_file', 'N/A')
        }
    }

    # Use try-finally to ensure cleanup actions happen
    model = None  # Define outside try block for finally
    perm_dataset = None
    try:
        # --- Load Predictions ---
        with log_stage("EVAL_SETUP", "Loading predictions"):
            try:
                preds_df = pd.read_csv(predictions_path, dtype={'domain_id': str, 'resname': str})
                
                # Validate required columns and types
                preds_df['resid'] = pd.to_numeric(preds_df['resid'], errors='coerce').astype('Int64')
                preds_df['predicted_rmsf'] = pd.to_numeric(preds_df['predicted_rmsf'], errors='coerce')
                preds_df['prediction_temperature'] = pd.to_numeric(preds_df['prediction_temperature'], errors='coerce')
                preds_df.dropna(subset=['domain_id', 'resid', 'predicted_rmsf', 'prediction_temperature'], inplace=True)

                if preds_df.empty:
                    raise ValueError("Predictions file contains no valid rows after validation.")

                # Determine evaluation temperature from predictions file
                unique_temps = preds_df['prediction_temperature'].unique()
                if len(unique_temps) > 1:
                    logger.warning(f"Predictions file contains multiple temperatures ({unique_temps}). "
                                  f"Evaluation will compare against ground truth at the first temperature found: "
                                  f"{unique_temps[0]:.1f}K")
                elif len(unique_temps) == 0:
                    raise ValueError("Cannot determine evaluation temperature from predictions file.")
                    
                eval_temp = unique_temps[0]
                all_metrics['evaluation_temperature'] = float(eval_temp)  # Store eval temp
                logger.info(f"Evaluating predictions made at temperature: {eval_temp:.1f}K")

            except Exception as e:
                logger.exception(f"Failed to load or validate predictions CSV '{predictions_path}': {e}")
                return None  # Exit if predictions fail to load

        # --- Load Ground Truth ---
        with log_stage("EVAL_SETUP", "Loading and filtering ground truth data"):
            try:
                gt_df_raw = load_aggregated_rmsf_data(config['input']['aggregated_rmsf_file'])
                
                # Select necessary columns, including optional ones for stratification
                cols_to_keep = ['domain_id', 'resid', 'resname', 'target_rmsf', 'temperature_feature']
                optional_cols = ['relative_accessibility', 'dssp', 'secondary_structure_encoded']
                available_optional = [col for col in optional_cols if col in gt_df_raw.columns]
                cols_to_keep.extend(available_optional)

                gt_df = gt_df_raw[cols_to_keep].copy()
                gt_df['resid'] = pd.to_numeric(gt_df['resid'], errors='coerce').astype('Int64')
                gt_df['temperature_feature'] = pd.to_numeric(gt_df['temperature_feature'], errors='coerce')
                
                # Drop rows where essential GT info is missing BEFORE filtering by temp
                gt_df.dropna(subset=['domain_id', 'resid', 'target_rmsf', 'temperature_feature'], inplace=True)

                # Filter ground truth to match the evaluation temperature
                gt_df_filtered = gt_df[np.isclose(gt_df['temperature_feature'], eval_temp)].copy()
                logger.info(f"Loaded and filtered ground truth to {len(gt_df_filtered)} rows matching "
                           f"temp ~{eval_temp:.1f}K.")

                if gt_df_filtered.empty:
                    raise ValueError(f"No ground truth data matches evaluation temperature {eval_temp:.1f}K.")
                    
                del gt_df_raw, gt_df
                gc.collect()  # Free memory
            except Exception as e:
                logger.exception(f"Failed to load or process ground truth data: {e}")
                return None  # Exit if GT fails

        # --- Merge Predictions and Ground Truth ---
        with log_stage("EVAL_SETUP", "Merging predictions and ground truth"):
            try:
                logger.info("Merging predictions with filtered ground truth data...")
                
                # Ensure 'resid' is compatible type for merging (int after dropna)
                preds_df['resid'] = preds_df['resid'].astype(int)
                gt_df_filtered['resid'] = gt_df_filtered['resid'].astype(int)

                eval_df = pd.merge(
                    preds_df,
                    gt_df_filtered.drop(columns=['temperature_feature']),  # Don't need temp column from GT anymore
                    on=['domain_id', 'resid'],  # Merge keys
                    how='inner',  # Only keep overlapping entries
                    suffixes=('_pred', '_gt')  # Suffixes for potentially overlapping columns like resname
                )

                # Handle potential duplicate resname columns after merge
                if 'resname_pred' in eval_df.columns and 'resname_gt' in eval_df.columns:
                    eval_df['resname'] = eval_df['resname_gt']  # Prioritize ground truth resname
                    eval_df.drop(columns=['resname_pred', 'resname_gt'], inplace=True)
                elif 'resname_gt' in eval_df.columns:  # Rename if only GT suffix exists
                    eval_df.rename(columns={'resname_gt': 'resname'}, inplace=True)
                elif 'resname_pred' in eval_df.columns:  # Rename if only Pred suffix exists
                    eval_df.rename(columns={'resname_pred': 'resname'}, inplace=True)
                elif 'resname' not in eval_df.columns:  # If neither exists
                    eval_df['resname'] = 'UNK'  # Add placeholder

                logger.info(f"Merged data contains {len(eval_df)} overlapping entries.")
                if eval_df.empty:
                    logger.error("Merge resulted in empty DataFrame. Check domain/resid matching "
                                "between predictions and ground truth.")
                    return None

                # Final check for NaNs in prediction/target columns
                eval_df.dropna(subset=['predicted_rmsf', 'target_rmsf'], inplace=True)
                logger.info(f"{len(eval_df)} valid entries remaining after RMSF NaN check.")
                if eval_df.empty:
                    logger.error("No valid overlapping entries after RMSF NaN check.")
                    return None
            except Exception as e:
                logger.exception(f"Error merging prediction and ground truth data: {e}")
                return None  # Exit on merge error

        # --- Calculate Overall Metrics ---
        with log_stage("EVALUATION", "Calculating overall metrics"):
            overall_metrics = calculate_metrics(eval_df, label="Overall")
            all_metrics['overall'] = overall_metrics
            logger.info("--- Overall Performance ---")
            if overall_metrics:
                for key, val in overall_metrics.items():
                    logger.info(f"  {key}: {val:.4f}" if isinstance(val, float) else f"  {key}: {val}")
            else:
                logger.warning("Overall metrics could not be calculated.")

        # --- Stratified Metrics ---
        if config['evaluation'].get('calculate_stratified_metrics', True):
            with log_stage("EVALUATION", "Calculating stratified metrics"):
                all_metrics['stratified'] = {}  # Ensure key exists

                # By Secondary Structure
                ss_col = next((c for c in ['dssp', 'secondary_structure_encoded'] if c in eval_df.columns), None)
                if ss_col:
                    logger.info(f"Stratifying metrics by Secondary Structure ('{ss_col}')...")
                    strat_ss_metrics = {}
                    for ss_type, group in eval_df.groupby(ss_col):
                        if isinstance(group, pd.DataFrame):  # Ensure group is DataFrame
                            metrics = calculate_metrics(group, label=f"SS={ss_type}")
                            if metrics.get('count', 0) > 10:  # Only report if enough samples
                                strat_ss_metrics[str(ss_type)] = metrics
                            else:
                                logger.debug(f"Skipping SS type '{ss_type}' due to insufficient samples "
                                            f"({metrics.get('count', 0)}).")
                    all_metrics['stratified']['secondary_structure'] = strat_ss_metrics
                else:
                    logger.warning("Secondary structure column ('dssp' or 'secondary_structure_encoded') "
                                  "not found for stratification.")

                # By Solvent Accessibility
                sasa_col = 'relative_accessibility'
                if sasa_col in eval_df.columns:
                    logger.info(f"Stratifying metrics by Relative Accessibility ('{sasa_col}')...")
                    sasa_bins = config['evaluation'].get('sasa_bins', [0.0, 0.1, 0.4, 1.01])
                    bin_labels = [f"{sasa_bins[i]:.1f}-{sasa_bins[i+1]:.1f}" for i in range(len(sasa_bins)-1)]
                    strat_sasa_metrics = {}
                    try:
                        # Ensure column is numeric and handle potential errors during binning
                        eval_df[sasa_col] = pd.to_numeric(eval_df[sasa_col], errors='coerce')
                        eval_df['sasa_bin'] = pd.cut(
                            eval_df[sasa_col], 
                            bins=sasa_bins, 
                            labels=bin_labels, 
                            right=False, 
                            include_lowest=True
                        )

                        for bin_label, group in eval_df.groupby('sasa_bin', observed=False):  # Use observed=False for categorical
                            if isinstance(group, pd.DataFrame):
                                metrics = calculate_metrics(group, label=f"SASA={bin_label}")
                                if metrics.get('count', 0) > 10:
                                    strat_sasa_metrics[str(bin_label)] = metrics
                                else:
                                    logger.debug(f"Skipping SASA bin '{bin_label}' due to insufficient samples "
                                                f"({metrics.get('count', 0)}).")
                        all_metrics['stratified']['sasa'] = strat_sasa_metrics
                    except Exception as sasa_e:
                        logger.error(f"Error during SASA binning/stratification: {sasa_e}")
                else:
                    logger.warning("SASA column ('relative_accessibility') not found for stratification.")

        # --- Permutation Importance Calculation ---
        if config['evaluation'].get('calculate_permutation_importance', True):
            with log_stage("EVALUATION", "Calculating permutation importance"):
                perm_importance_scores = {}  # Initialize scores dict
                try:
                    logger.info("Loading resources for permutation importance...")
                    device = get_device(config["system_utilization"]["adjust_for_gpu"])
                    checkpoint = torch.load(model_path, map_location='cpu')
                    model_config = checkpoint.get('config', {}).get('model', {})
                    input_shape = checkpoint.get('input_shape')
                    scaling_params = checkpoint.get('temp_scaling_params')
                    
                    if not model_config or not input_shape:
                        raise ValueError("Model config or shape missing from checkpoint.")
                    if not scaling_params:
                        raise ValueError("Temperature scaling parameters missing from checkpoint.")

                    model = get_model(model_config, input_shape=input_shape)
                    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                    model.to(device)
                    model.eval()
                    del checkpoint
                    gc.collect()

                    temp_scaler_func = get_temperature_scaler(params=scaling_params)

                    # Get samples present in the final evaluation dataframe
                    perm_samples_to_load = list(eval_df[['domain_id', 'resid']].apply(
                        lambda x: (x['domain_id'], str(x['resid'])), axis=1
                    ).unique())
                    
                    if not perm_samples_to_load:
                        raise ValueError("No samples available in eval_df for permutation importance.")

                    logger.info(f"Creating permutation dataset for {len(perm_samples_to_load)} unique residues...")
                    perm_dataset = PredictionDataset(
                        samples_to_load=perm_samples_to_load,
                        voxel_hdf5_path=config['input']['voxel_file'],
                        expected_channels=model_config['input_channels'],
                        target_shape_chw=input_shape
                    )

                    if len(perm_dataset) == 0:
                        raise ValueError("Permutation dataset is empty after loading voxels.")
                        
                    if len(perm_dataset) != len(eval_df):
                        logger.warning(f"Permutation dataset size ({len(perm_dataset)}) differs from "
                                      f"eval_df size ({len(eval_df)}). Ensure samples match.")
                        # This might happen if some voxels failed loading for eval_df samples
                        # We need to use the intersection for fair comparison
                        # Filter eval_df to only include samples in perm_dataset
                        perm_dataset_keys = set(perm_dataset.samples)
                        eval_df_keys = set(eval_df[['domain_id', 'resid']].apply(
                            lambda x: (x['domain_id'], str(x['resid'])), axis=1
                        ))
                        common_keys = perm_dataset_keys.intersection(eval_df_keys)
                        eval_df_filtered_for_perm = eval_df[eval_df[['domain_id', 'resid']].apply(
                            lambda x: (x['domain_id'], str(x['resid'])), axis=1
                        ).isin(common_keys)].copy()
                        
                        logger.info(f"Filtered eval_df to {len(eval_df_filtered_for_perm)} samples "
                                   f"matching permutation dataset.")
                        if len(eval_df_filtered_for_perm) != len(perm_dataset):
                            # This should not happen if logic is correct
                            raise RuntimeError("Mismatch between filtered eval_df and permutation dataset sizes.")
                            
                        # Recalculate baseline on the filtered set for fair comparison
                        filtered_baseline_metrics = calculate_metrics(
                            eval_df_filtered_for_perm, label="Permutation Baseline"
                        )
                    else:
                        # Sizes match, use original overall metrics
                        filtered_baseline_metrics = overall_metrics

                    n_repeats = config['evaluation'].get('permutation_n_repeats', 5)

                    # Perform permutation importance
                    perm_importance_scores = perform_permutation_importance(
                        model, perm_dataset, temp_scaler_func, eval_temp, 
                        filtered_baseline_metrics, device, config, n_repeats
                    )

                except Exception as perm_e:
                    logger.exception(f"Failed to perform permutation importance: {perm_e}")
                    perm_importance_scores = {"error": str(perm_e)}
                finally:
                    # Clean up permutation specific resources
                    del model  # Ensure model loaded for perm is deleted
                    del perm_dataset
                    clear_memory(force_gc=True, clear_cuda=True)

                all_metrics['permutation_importance'] = perm_importance_scores

        # --- Save All Metrics ---
        with log_stage("OUTPUT", "Saving evaluation metrics"):
            try:
                # Helper function to convert numpy types for JSON serialization
                def convert_numpy_types(obj):
                    if isinstance(obj, (np.int_, np.integer)):
                        return int(obj)
                    elif isinstance(obj, (np.float_, np.floating)):
                        return float(obj) if np.isfinite(obj) else None  # Convert non-finite floats to None
                    elif isinstance(obj, (np.bool_)):
                        return bool(obj)
                    elif isinstance(obj, (np.void)):
                        return None
                    elif isinstance(obj, np.ndarray):
                        return [convert_numpy_types(item) for item in obj]  # Recursively convert array elements
                    elif isinstance(obj, dict):
                        return {k: convert_numpy_types(v) for k, v in obj.items()}
                    elif isinstance(obj, (list, tuple)):
                        return [convert_numpy_types(i) for i in obj]
                    elif pd.isna(obj):
                        return None  # Handle pandas NaNs
                    else:
                        return obj  # Keep other types as is

                metrics_serializable = convert_numpy_types(all_metrics)
                save_json(metrics_serializable, metrics_path)
                logger.info(f"Evaluation metrics saved successfully to {metrics_path}")
            except Exception as e:
                logger.exception(f"Failed to save metrics JSON to {metrics_path}: {e}")
                return None  # Indicate failure to save

        log_memory_usage(logger)
        logger.info("Evaluation finished successfully.")
        return metrics_path  # Return path on success

    finally:
        # Final cleanup
        clear_memory(force_gc=True)
==========================================================
===== FILE: src/voxelflex/cli/commands/visualize.py =====
==========================================================

# src/voxelflex/cli/commands/visualize.py
"""
Visualization command for VoxelFlex (Temperature-Aware).

Generates plots for model performance analysis, optionally saving plot data.
"""

import os
import time
import json
import logging
from typing import Dict, Any, List, Optional, Tuple, Union # Add Union

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg') # Use Agg backend for non-interactive plotting
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.figure import Figure
from matplotlib.colors import Normalize
# Ensure 'viridis' is imported correctly if used directly
from matplotlib.cm import ScalarMappable, viridis #, viridis_r if needed
from scipy.stats import gaussian_kde, pearsonr

# Use centralized logger
logger = logging.getLogger("voxelflex.cli.visualize")

# Project imports
from voxelflex.utils.logging_utils import get_logger, log_stage, log_section_header # Add log_section_header
from voxelflex.utils.file_utils import ensure_dir, load_json, save_json, resolve_path
from voxelflex.data.data_loader import load_aggregated_rmsf_data # Needed for merging GT
# Import the metric calculation from evaluate for consistency
from voxelflex.cli.commands.evaluate import calculate_metrics

# --- Plotting Functions ---

def _save_plot_and_data(
    fig: Figure,
    plot_df: Optional[pd.DataFrame],
    base_filename: str,
    output_dir: str,
    save_format: str,
    dpi: int,
    save_data: bool
) -> Optional[str]:
    """Helper function to save plot image and optionally its data."""
    ensure_dir(output_dir)
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    plot_path = os.path.join(output_dir, f"{base_filename}_{timestamp}.{save_format}")
    csv_path = os.path.join(output_dir, f"{base_filename}_{timestamp}_data.csv")

    plot_saved = False
    try:
        fig.savefig(plot_path, dpi=dpi, bbox_inches='tight')
        logger.info(f"Saved plot: {plot_path}")
        plot_saved = True
    except Exception as e:
        logger.error(f"Failed to save plot {plot_path}: {e}")
    finally:
        plt.close(fig) # Close figure to free memory, regardless of save success

    if save_data and plot_df is not None and not plot_df.empty:
        try:
            plot_df.to_csv(csv_path, index=False, float_format='%.6f')
            logger.info(f"Saved plot data: {csv_path}")
        except Exception as e:
            logger.error(f"Failed to save plot data {csv_path}: {e}")

    return plot_path if plot_saved else None

def create_metric_curve(
    history: Dict[str, List[float]],
    metric_key: str, # e.g., 'loss' or 'pearson'
    output_dir: str,
    save_format: str = 'png',
    dpi: int = 150,
    save_data: bool = True
) -> Optional[str]:
    """Creates a plot of training and validation metrics over epochs."""
    train_key = f'train_{metric_key}'
    val_key = f'val_{metric_key}'
    lr_key = 'lr'

    if train_key not in history or val_key not in history or not history[train_key] or not history[val_key]:
        logger.warning(f"History data missing for '{metric_key}'. Skipping {metric_key} curve plot.")
        return None

    logger.info(f"Creating {metric_key} curve plot...")
    train_values = history[train_key]
    val_values = history[val_key]
    epochs = range(1, len(train_values) + 1)

    plot_df_dict: Dict[str, List[Union[int, float]]] = { # Use Union for types
        'epoch': list(epochs),
        train_key: train_values,
        val_key: val_values
    }
    # Handle potential length mismatch for LR if resuming incomplete epoch
    if lr_key in history and len(history[lr_key]) >= len(epochs):
        plot_df_dict[lr_key] = history[lr_key][:len(epochs)]
    elif lr_key in history:
         logger.warning(f"Length mismatch for '{lr_key}' in history. Cannot plot LR.")

    plot_df = pd.DataFrame(plot_df_dict)


    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(epochs, train_values, 'o-', color='royalblue', label=f'Training {metric_key.capitalize()}', markersize=4, alpha=0.8)
    ax.plot(epochs, val_values, 's-', color='orangered', label=f'Validation {metric_key.capitalize()}', markersize=4, alpha=0.8)

    # Determine best epoch based on metric (loss=min, corr=max)
    try:
        is_loss = 'loss' in metric_key.lower()
        valid_val_values = [v for v in val_values if pd.notna(v)] # Filter NaNs for argmin/argmax
        if not valid_val_values: raise ValueError("No valid validation values found.")

        if is_loss:
            best_val_epoch_idx = np.nanargmin(val_values) # Use nanargmin
        else:
            best_val_epoch_idx = np.nanargmax(val_values) # Use nanargmax

        best_val_epoch = best_val_epoch_idx + 1
        best_val_value = val_values[best_val_epoch_idx]
        ax.axvline(best_val_epoch, linestyle='--', color='gray', alpha=0.7, label=f'Best Val @ Ep {best_val_epoch} ({best_val_value:.4f})')
    except (ValueError, IndexError) as e:
         logger.warning(f"Could not determine best epoch for {metric_key} curve: {e}")


    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel(metric_key.capitalize(), fontsize=12)
    ax.set_title(f'Training and Validation {metric_key.capitalize()}', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, linestyle=':', alpha=0.6)
    ax.tick_params(axis='both', which='major', labelsize=10)

    # Optional: Add Learning Rate on secondary axis if plotting correlation
    if not is_loss and lr_key in plot_df.columns:
        ax2 = ax.twinx()
        ax2.plot(epochs, plot_df[lr_key], 'd--', color='green', label='Learning Rate', markersize=3, alpha=0.5)
        ax2.set_ylabel('Learning Rate', color='green', fontsize=10)
        ax2.tick_params(axis='y', labelcolor='green', labelsize=9)
        # Use log scale if LR varies significantly and has no zeros/negatives
        lr_vals = plot_df[lr_key].dropna()
        if len(lr_vals.unique()) > 2 and (lr_vals > 0).all():
            try:
                 ax2.set_yscale('log')
            except ValueError as e: # Catch potential issues with non-positive values if check failed
                 logger.warning(f"Could not set log scale for LR axis: {e}")
        # Combine legends
        lines, labels = ax.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax.legend(lines + lines2, labels + labels2, loc='best', fontsize=9)


    fig.tight_layout()
    base_filename = f"{metric_key}_curve"
    # Pass the DataFrame used for plotting
    return _save_plot_and_data(fig, plot_df, base_filename, output_dir, save_format, dpi, save_data)

# Alias specific curve functions
def create_loss_curve(history, output_dir, save_format='png', dpi=150, save_data=True):
    return create_metric_curve(history, 'loss', output_dir, save_format, dpi, save_data)

def create_correlation_curve(history, output_dir, save_format='png', dpi=150, save_data=True):
    return create_metric_curve(history, 'pearson', output_dir, save_format, dpi, save_data)

def create_prediction_scatter(
    eval_df: pd.DataFrame,
    output_dir: str,
    save_format: str = 'png',
    dpi: int = 150,
    max_points: int = 1000,
    save_data: bool = True
) -> Optional[str]:
    """Creates Predicted vs. Actual RMSF scatter plot."""
    if eval_df.empty or 'target_rmsf' not in eval_df or 'predicted_rmsf' not in eval_df:
        logger.warning("Cannot create prediction scatter: DataFrame empty or missing columns.")
        return None

    logger.info("Creating prediction scatter plot...")
    # Use calculate_metrics for consistency
    metrics = calculate_metrics(eval_df, label="Scatter")

    y_true = eval_df['target_rmsf'].values
    y_pred = eval_df['predicted_rmsf'].values

    # Filter only finite values for plotting and metrics text
    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true_plot = y_true[valid_mask]
    y_pred_plot = y_pred[valid_mask]
    plot_df_full = eval_df[valid_mask].copy() # Dataframe with only valid points

    if len(y_true_plot) == 0:
        logger.warning("No finite data points for prediction scatter plot.")
        return None

    # Sample points *after* filtering NaNs if needed
    if len(y_true_plot) > max_points:
        logger.debug(f"Sampling {max_points} points for scatter plot from {len(y_true_plot)} valid points.")
        indices = np.random.choice(len(y_true_plot), max_points, replace=False)
        y_true_sampled, y_pred_sampled = y_true_plot[indices], y_pred_plot[indices]
        # Data to save should be the sampled points if sampling occurred
        plot_df_to_save = plot_df_full.iloc[indices][['target_rmsf', 'predicted_rmsf']].copy()
    else:
        y_true_sampled, y_pred_sampled = y_true_plot, y_pred_plot
        plot_df_to_save = plot_df_full[['target_rmsf', 'predicted_rmsf']].copy() # Save all valid points

    fig, ax = plt.subplots(figsize=(8, 8))
    ax.scatter(y_true_sampled, y_pred_sampled, c='steelblue', s=20, alpha=0.5, edgecolors='none', label='Predictions')

    # Add identity line
    # Calculate limits based on the *sampled* data for the plot view
    min_val = min(y_true_sampled.min(), y_pred_sampled.min()) - 0.1 * abs(min(y_true_sampled.min(), y_pred_sampled.min())) # Add padding
    max_val = max(y_true_sampled.max(), y_pred_sampled.max()) + 0.1 * abs(max(y_true_sampled.max(), y_pred_sampled.max()))
    lims = [min_val, max_val]
    ax.plot(lims, lims, 'r--', alpha=0.75, zorder=0, label="y=x")
    ax.set_xlim(lims)
    ax.set_ylim(lims)
    ax.set_aspect('equal', adjustable='box')

    # Use metrics calculated on the *full* valid dataset for annotation
    metrics_text = (
        f"Pearson: {metrics.get('pearson', np.nan):.3f}\n"
        f"R²: {metrics.get('r2', np.nan):.3f}\n"
        f"RMSE: {metrics.get('rmse', np.nan):.3f}\n"
        f"MAE: {metrics.get('mae', np.nan):.3f}\n"
        f"N: {metrics.get('count', 0):,}" # Count is from full valid dataset
    )
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.7)
    ax.text(0.03, 0.97, metrics_text, transform=ax.transAxes, fontsize=9,
            verticalalignment='top', bbox=props)

    ax.set_title('Predicted vs. Actual RMSF', fontsize=14, fontweight='bold')
    ax.set_xlabel('Actual RMSF', fontsize=12)
    ax.set_ylabel('Predicted RMSF', fontsize=12)
    ax.grid(True, linestyle=':', alpha=0.6)
    ax.legend(loc='lower right', fontsize=10)
    ax.tick_params(axis='both', which='major', labelsize=10)

    fig.tight_layout()
    base_filename = "prediction_scatter"
    # Save the potentially sampled data
    return _save_plot_and_data(fig, plot_df_to_save, base_filename, output_dir, save_format, dpi, save_data)


def create_predicted_vs_validation_scatter_density(
    eval_df: pd.DataFrame,
    output_dir: str,
    save_format: str = 'png',
    dpi: int = 150,
    save_data: bool = True # Save the underlying x, y data
) -> Optional[str]:
    """Creates Predicted vs. Actual RMSF scatter plot with density coloring."""
    if eval_df.empty or 'target_rmsf' not in eval_df or 'predicted_rmsf' not in eval_df:
        logger.warning("Cannot create density scatter: DataFrame empty or missing columns.")
        return None

    logger.info("Creating prediction density scatter plot...")
    y_true = eval_df['target_rmsf'].values
    y_pred = eval_df['predicted_rmsf'].values

    # Filter out NaNs/Infs before KDE
    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true_clean = y_true[valid_mask]
    y_pred_clean = y_pred[valid_mask]
    plot_df = eval_df[valid_mask][['target_rmsf', 'predicted_rmsf']].copy() # DataFrame for saving

    if len(y_true_clean) < 5: # Need points for KDE
        logger.warning("Too few valid points (<5) for density estimation. Skipping density scatter.")
        return None

    # Calculate metrics on valid points only
    metrics = calculate_metrics(plot_df, label="Density Scatter")

    fig, ax = plt.subplots(figsize=(8, 8))

    # Calculate the point density
    try:
        xy = np.vstack([y_true_clean, y_pred_clean])
        # Handle potential singular matrix in KDE
        try:
             z = gaussian_kde(xy)(xy)
        except np.linalg.LinAlgError:
             logger.warning("Singular matrix in KDE calculation. Adding small jitter.")
             jitter_scale = 1e-6 * (np.max(xy, axis=1) - np.min(xy, axis=1))
             jitter = jitter_scale[:, np.newaxis] * np.random.randn(*xy.shape)
             z = gaussian_kde(xy + jitter)(xy + jitter)

        # Sort points by density, so dense points are plotted last
        idx = z.argsort()
        x_plot, y_plot, z_plot = y_true_clean[idx], y_pred_clean[idx], z[idx]

        scatter = ax.scatter(x_plot, y_plot, c=z_plot, s=10, cmap=viridis, alpha=0.7, edgecolors='none')
        cbar = fig.colorbar(scatter, ax=ax, shrink=0.7)
        cbar.set_label('Point Density', fontsize=10)
        cbar.ax.tick_params(labelsize=8)
    except Exception as kde_e:
         logger.warning(f"KDE calculation failed ({kde_e}). Falling back to simple scatter.")
         ax.scatter(y_true_clean, y_pred_clean, c='steelblue', s=10, alpha=0.5, edgecolors='none')


    # Add identity line
    min_val = min(y_true_clean.min(), y_pred_clean.min()) - 0.1 * abs(min(y_true_clean.min(), y_pred_clean.min()))
    max_val = max(y_true_clean.max(), y_pred_clean.max()) + 0.1 * abs(max(y_true_clean.max(), y_pred_clean.max()))
    lims = [min_val, max_val]
    ax.plot(lims, lims, 'r--', alpha=0.75, zorder=1, label="y=x") # Ensure line is visible
    ax.set_xlim(lims)
    ax.set_ylim(lims)
    ax.set_aspect('equal', adjustable='box')

    metrics_text = (
        f"Pearson: {metrics.get('pearson', np.nan):.3f}\n"
        f"R²: {metrics.get('r2', np.nan):.3f}\n"
        f"RMSE: {metrics.get('rmse', np.nan):.3f}\n"
        f"MAE: {metrics.get('mae', np.nan):.3f}\n"
        f"N: {metrics.get('count', 0):,}"
    )
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.7)
    ax.text(0.03, 0.97, metrics_text, transform=ax.transAxes, fontsize=9,
            verticalalignment='top', bbox=props)

    ax.set_title('Predicted vs. Actual RMSF (Density Scatter)', fontsize=14, fontweight='bold')
    ax.set_xlabel('Actual RMSF', fontsize=12)
    ax.set_ylabel('Predicted RMSF', fontsize=12)
    ax.grid(True, linestyle=':', alpha=0.6)
    ax.legend(loc='lower right', fontsize=10)
    ax.tick_params(axis='both', which='major', labelsize=10)

    fig.tight_layout()
    base_filename = "prediction_density_scatter"
    return _save_plot_and_data(fig, plot_df, base_filename, output_dir, save_format, dpi, save_data)


def create_error_distribution(
    eval_df: pd.DataFrame,
    output_dir: str,
    save_format: str = 'png',
    dpi: int = 150,
    save_data: bool = True
) -> Optional[str]:
    """Creates histogram of prediction errors (Predicted - Actual)."""
    if eval_df.empty or 'target_rmsf' not in eval_df or 'predicted_rmsf' not in eval_df:
        logger.warning("Cannot create error distribution: DataFrame empty or missing columns.")
        return None

    logger.info("Creating error distribution plot...")
    eval_df_copy = eval_df.copy() # Work on a copy
    eval_df_copy['error'] = eval_df_copy['predicted_rmsf'] - eval_df_copy['target_rmsf']
    errors = eval_df_copy['error'].dropna()
    if errors.empty:
        logger.warning("No valid error values found for distribution plot.")
        return None

    plot_df = pd.DataFrame({'error': errors}) # Data for saving

    fig, ax = plt.subplots(figsize=(10, 6))
    sns.histplot(errors, kde=True, bins=50, ax=ax, color='coral', edgecolor='black', alpha=0.7, stat="density")

    mean_error = errors.mean()
    median_error = errors.median()
    std_error = errors.std()

    ax.axvline(mean_error, color='k', linestyle='--', linewidth=1.5, label=f'Mean: {mean_error:.3f}')
    ax.axvline(median_error, color='k', linestyle=':', linewidth=1.5, label=f'Median: {median_error:.3f}')
    # ax.axvspan(mean_error - std_error, mean_error + std_error, alpha=0.15, color='gray', label=f'StdDev: {std_error:.3f}')

    ax.set_title('Distribution of Prediction Errors (Predicted - Actual)', fontsize=14, fontweight='bold')
    ax.set_xlabel('Prediction Error', fontsize=12)
    ax.set_ylabel('Density', fontsize=12) # Use Density since kde=True
    ax.legend(fontsize=10)
    ax.grid(True, linestyle=':', alpha=0.6)
    ax.tick_params(axis='both', which='major', labelsize=10)

    fig.tight_layout()
    base_filename = "error_distribution"
    return _save_plot_and_data(fig, plot_df, base_filename, output_dir, save_format, dpi, save_data)


def create_stratified_error_boxplot(
    eval_df: pd.DataFrame,
    stratify_by_col: str,
    plot_title: str,
    x_label: str,
    output_dir: str,
    base_filename: str,
    save_format: str = 'png',
    dpi: int = 150,
    save_data: bool = True,
    min_samples_per_group: int = 10
) -> Optional[str]:
    """Creates a boxplot of absolute errors stratified by a given column."""
    if eval_df.empty or stratify_by_col not in eval_df.columns:
        logger.warning(f"Cannot create stratified boxplot: DataFrame empty or column '{stratify_by_col}' missing.")
        return None

    logger.info(f"Creating stratified error boxplot by '{stratify_by_col}'...")
    eval_df_copy = eval_df.copy()
    if 'error' not in eval_df_copy.columns:
         eval_df_copy['error'] = eval_df_copy['predicted_rmsf'] - eval_df_copy['target_rmsf']
    eval_df_copy['abs_error'] = np.abs(eval_df_copy['error'])

    # Ensure stratification column is suitable type (e.g., string or category) and handle NaNs
    eval_df_copy = eval_df_copy.dropna(subset=[stratify_by_col, 'abs_error'])
    eval_df_copy[stratify_by_col] = eval_df_copy[stratify_by_col].astype(str) # Convert to string for consistent grouping

    # Filter groups with enough samples
    group_counts = eval_df_copy[stratify_by_col].value_counts()
    valid_groups = group_counts[group_counts >= min_samples_per_group].index.tolist()
    plot_df = eval_df_copy[eval_df_copy[stratify_by_col].isin(valid_groups)].copy()

    if plot_df.empty:
        logger.warning(f"No groups in '{stratify_by_col}' met the minimum sample requirement ({min_samples_per_group}). Skipping boxplot.")
        return None

    # Sort categories for consistent plotting
    # Try numeric sort first if they look like numbers/bins, else alphabetical
    try:
        # Attempt to extract leading number for sorting (e.g., for SASA bins)
        categories_sorted = sorted(valid_groups, key=lambda x: float(x.split('-')[0]))
    except (ValueError, IndexError):
        # Fallback to alphabetical sort if numeric extraction fails
        categories_sorted = sorted(valid_groups)


    fig, ax = plt.subplots(figsize=(max(8, len(categories_sorted)*0.5), 6)) # Adjust width based on # categories
    sns.boxplot(x=stratify_by_col, y='abs_error', data=plot_df, ax=ax, order=categories_sorted,
                palette='viridis', fliersize=2, linewidth=1.0, showfliers=False) # Hide outliers for clarity?

    ax.set_title(plot_title, fontsize=14, fontweight='bold')
    ax.set_xlabel(x_label, fontsize=12)
    ax.set_ylabel('Absolute Prediction Error', fontsize=12)
    ax.tick_params(axis='x', rotation=45, labelsize=10, ha='right')
    ax.tick_params(axis='y', labelsize=10)
    ax.grid(True, linestyle=':', alpha=0.6, axis='y')

    # Add counts below boxes
    y_min, y_max = ax.get_ylim() # Get current y-limits AFTER plotting boxes
    # y_range = y_max - y_min if y_max > y_min else 1.0 # Avoid zero range
    # Place text slightly below the minimum y-axis value shown
    text_y_pos = y_min - (y_max - y_min) * 0.05 # Position below plot area

    for i, cat in enumerate(categories_sorted):
        count = group_counts[cat]
        ax.text(i, text_y_pos, f"n={count}", ha='center', va='top', fontsize=8, color='gray')

    # Adjust y-limit slightly to make space for text
    ax.set_ylim(bottom=text_y_pos - (y_max - y_min) * 0.02)

    fig.tight_layout()
    # Data to save includes the stratification column and the absolute error
    plot_data_to_save = plot_df[[stratify_by_col, 'abs_error']].copy()
    return _save_plot_and_data(fig, plot_data_to_save, base_filename, output_dir, save_format, dpi, save_data)


# Specific wrappers for stratified plots
def create_residue_type_analysis(eval_df, output_dir, save_format='png', dpi=150, save_data=True):
    return create_stratified_error_boxplot(
        eval_df, 'resname', 'Absolute Error by Residue Type', 'Residue Type',
        output_dir, 'residue_type_error_boxplot', save_format, dpi, save_data
    )

def create_sasa_error_analysis(eval_df, output_dir, sasa_bins, save_format='png', dpi=150, save_data=True):
    sasa_col = 'relative_accessibility'
    if sasa_col not in eval_df.columns:
        logger.warning(f"SASA column '{sasa_col}' not found. Skipping SASA error analysis.")
        return None
    eval_df_copy = eval_df.copy()
    # Ensure bin column exists
    if 'sasa_bin' not in eval_df_copy.columns:
        bin_labels = [f"{sasa_bins[i]:.1f}-{sasa_bins[i+1]:.1f}" for i in range(len(sasa_bins)-1)]
        try:
            # Ensure numeric conversion before cutting
            eval_df_copy[sasa_col] = pd.to_numeric(eval_df_copy[sasa_col], errors='coerce')
            eval_df_copy['sasa_bin'] = pd.cut(eval_df_copy[sasa_col], bins=sasa_bins, labels=bin_labels, right=False, include_lowest=True)
        except Exception as e:
             logger.error(f"Failed to create SASA bins for plotting: {e}")
             return None
    # Pass the dataframe with the 'sasa_bin' column
    return create_stratified_error_boxplot(
        eval_df_copy, 'sasa_bin', 'Absolute Error by Relative Solvent Accessibility', 'SASA Bin',
        output_dir, 'sasa_error_boxplot', save_format, dpi, save_data
    )

def create_ss_error_analysis(eval_df, output_dir, save_format='png', dpi=150, save_data=True):
    ss_col = next((c for c in ['dssp', 'secondary_structure_encoded'] if c in eval_df.columns), None)
    if ss_col is None:
        logger.warning("No secondary structure column found. Skipping SS error analysis.")
        return None
    return create_stratified_error_boxplot(
        eval_df, ss_col, 'Absolute Error by Secondary Structure', f'SS Type ({ss_col})',
        output_dir, 'ss_error_boxplot', save_format, dpi, save_data
    )


def create_amino_acid_performance(
    eval_df: pd.DataFrame,
    output_dir: str,
    save_format: str = 'png',
    dpi: int = 150,
    save_data: bool = True
) -> Optional[str]:
    """Creates bar plots of various metrics grouped by amino acid type."""
    if eval_df.empty or 'resname' not in eval_df.columns:
        logger.warning("Cannot create AA performance plot: DataFrame empty or 'resname' missing.")
        return None

    logger.info("Creating amino acid performance plots...")
    aa_metrics_list = []
    # Ensure resname is string and handle potential NaNs dropped by earlier steps
    eval_df_copy = eval_df.dropna(subset=['resname']).copy()
    eval_df_copy['resname'] = eval_df_copy['resname'].astype(str)

    grouped = eval_df_copy.groupby('resname')
    # Define standard AA order if possible, otherwise sort alphabetically
    aa_order = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']
    resnames_present = eval_df_copy['resname'].unique()
    resnames_sorted = [aa for aa in aa_order if aa in resnames_present] + sorted([aa for aa in resnames_present if aa not in aa_order])

    for resname in resnames_sorted:
        group = grouped.get_group(resname)
        if len(group) >= 2: # Need min 2 points for most metrics
             metrics = calculate_metrics(group, label=f"AA={resname}")
             # Only add if key metrics were calculable
             if pd.notna(metrics.get('pearson')) or pd.notna(metrics.get('rmse')):
                 metrics['resname'] = resname
                 aa_metrics_list.append(metrics)
        # else: logger.debug(f"Skipping AA '{resname}' due to insufficient samples ({len(group)})")

    if not aa_metrics_list:
         logger.warning("No amino acid groups had sufficient valid samples for performance plotting.")
         return None

    metrics_df = pd.DataFrame(aa_metrics_list)
    plot_df_to_save = metrics_df.copy() # Data to save

    # Plotting setup
    metrics_to_plot = ['pearson', 'rmse', 'mae', 'count']
    titles = ['Pearson Correlation', 'RMSE', 'MAE', 'Sample Count']
    palettes = ['coolwarm', 'viridis_r', 'magma_r', 'crest']
    num_metrics = len(metrics_to_plot)
    ncols = 2
    nrows = (num_metrics + ncols - 1) // ncols
    fig, axes = plt.subplots(nrows, ncols, figsize=(7 * ncols, 5 * nrows), sharex=True) # Share x-axis
    axes = axes.flatten()

    for i, metric in enumerate(metrics_to_plot):
        if metric not in metrics_df.columns:
            logger.warning(f"Metric '{metric}' not found in calculated AA metrics. Skipping its plot.")
            # Disable the unused subplot
            if i < len(axes): axes[i].set_visible(False)
            continue

        # Filter out NaN values for the specific metric before plotting
        plot_data = metrics_df.dropna(subset=[metric])
        if plot_data.empty:
            logger.warning(f"No valid data for metric '{metric}' after dropping NaNs. Skipping plot.")
            if i < len(axes): axes[i].set_visible(False)
            continue

        sns.barplot(x='resname', y=metric, data=plot_data, ax=axes[i], palette=palettes[i % len(palettes)], order=resnames_sorted)
        axes[i].set_title(titles[i], fontsize=12, fontweight='bold')
        axes[i].set_xlabel(None) # Remove redundant x-label from upper plots if sharing x
        axes[i].set_ylabel(metric.upper() if metric != 'count' else 'Count', fontsize=10)
        axes[i].tick_params(axis='x', rotation=45, labelsize=9, ha='right')
        axes[i].tick_params(axis='y', labelsize=9)
        axes[i].grid(True, linestyle=':', alpha=0.6, axis='y')

    # Set x-label only on the bottom plots
    for i in range(ncols * (nrows - 1), len(axes)):
        if axes[i].get_visible(): # Only if plot was actually drawn
             axes[i].set_xlabel("Residue Type", fontsize=11)

    # Remove any unused subplots if num_metrics < nrows*ncols
    for i in range(num_metrics, len(axes)):
        axes[i].set_visible(False)


    fig.suptitle("Performance Metrics by Amino Acid Type", fontsize=16, fontweight='bold')#, y=1.02)
    fig.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent title overlap

    base_filename = "amino_acid_performance"
    return _save_plot_and_data(fig, plot_df_to_save, base_filename, output_dir, save_format, dpi, save_data)


# --- Main Visualization Function ---

def create_visualizations(
    config: Dict[str, Any],
    predictions_path: str,
    history_path: Optional[str] = None # Allow passing history file path
) -> List[str]:
    """
    Generate and save performance visualizations based on config settings.

    Args:
        config: Configuration dictionary.
        predictions_path: Path to the prediction CSV file.
        history_path: Optional path to the training history JSON file.

    Returns:
        List of paths to the generated plot files.
    """
    log_section_header(logger, "GENERATING VISUALIZATIONS")
    predictions_path = resolve_path(predictions_path)
    if not os.path.exists(predictions_path):
        logger.error(f"Predictions file not found: {predictions_path}")
        return []
    history_path = resolve_path(history_path) if history_path else None
    viz_output_dir = config["output"]["visualizations_dir"]
    ensure_dir(viz_output_dir) # Ensure output directory exists

    viz_config = config.get("visualization", {})
    save_format = viz_config.get("save_format", "png")
    dpi = viz_config.get("dpi", 150)
    save_plot_data = viz_config.get("save_plot_data", True)
    max_scatter = viz_config.get("max_scatter_points", 1000)
    sasa_bins = config.get('evaluation', {}).get('sasa_bins', [0.0, 0.1, 0.4, 1.01]) # Get bins from eval config

    generated_plots: List[str] = []
    eval_df: Optional[pd.DataFrame] = None # Initialize

    # --- Load Data ---
    try:
        with log_stage("VIS_SETUP", "Loading prediction and ground truth data"):
            logger.info(f"Loading predictions from: {predictions_path}")
            preds_df = pd.read_csv(predictions_path, dtype={'domain_id': str, 'resname': str})
            preds_df['resid'] = pd.to_numeric(preds_df['resid'], errors='coerce').astype('Int64')
            preds_df['predicted_rmsf'] = pd.to_numeric(preds_df['predicted_rmsf'], errors='coerce')
            preds_df['prediction_temperature'] = pd.to_numeric(preds_df['prediction_temperature'], errors='coerce')
            preds_df.dropna(subset=['domain_id', 'resid', 'predicted_rmsf', 'prediction_temperature'], inplace=True)

            if preds_df.empty:
                 logger.error("Predictions file is empty or contains no valid rows. Cannot generate comparison visualizations.")
                 return [] # Stop if no predictions

            # Load ground truth for comparison plots
            gt_file_path = config['input'].get('aggregated_rmsf_file')
            if not gt_file_path:
                logger.warning("Aggregated RMSF file path not specified in config. Cannot generate comparison plots.")
                eval_df = preds_df.copy() # Can only plot prediction distribution
            else:
                logger.info(f"Loading ground truth from: {gt_file_path}")
                gt_df_raw = load_aggregated_rmsf_data(gt_file_path)
                # Select necessary columns, including optional ones
                cols_to_keep = ['domain_id', 'resid', 'resname', 'target_rmsf', 'temperature_feature']
                optional_cols = ['relative_accessibility', 'dssp', 'secondary_structure_encoded']
                available_optional = [col for col in optional_cols if col in gt_df_raw.columns]
                cols_to_keep.extend(available_optional)
                gt_df = gt_df_raw[cols_to_keep].copy()
                gt_df['resid'] = pd.to_numeric(gt_df['resid'], errors='coerce').astype('Int64')
                gt_df['temperature_feature'] = pd.to_numeric(gt_df['temperature_feature'], errors='coerce')
                gt_df.dropna(subset=['domain_id', 'resid', 'target_rmsf', 'temperature_feature'], inplace=True)

                # Merge predictions and ground truth based on prediction temperature
                pred_temp = preds_df['prediction_temperature'].iloc[0]
                gt_df_filtered = gt_df[np.isclose(gt_df['temperature_feature'], pred_temp)].copy()
                logger.info(f"Merging predictions with {len(gt_df_filtered)} ground truth entries for temp ~{pred_temp:.1f}K...")

                preds_df['resid'] = preds_df['resid'].astype(int) # Ensure int for merge
                gt_df_filtered['resid'] = gt_df_filtered['resid'].astype(int)
                eval_df = pd.merge(preds_df, gt_df_filtered.drop(columns=['temperature_feature']), on=['domain_id', 'resid'], how='inner', suffixes=('_pred', '_gt'))

                # Handle resname column merge issues
                if 'resname_pred' in eval_df.columns and 'resname_gt' in eval_df.columns:
                    eval_df['resname'] = eval_df['resname_gt']
                    eval_df.drop(columns=['resname_pred', 'resname_gt'], inplace=True)
                elif 'resname_gt' in eval_df.columns: eval_df.rename(columns={'resname_gt': 'resname'}, inplace=True)
                elif 'resname_pred' in eval_df.columns: eval_df.rename(columns={'resname_pred': 'resname'}, inplace=True)
                elif 'resname' not in eval_df.columns: eval_df['resname'] = 'UNK'

                eval_df.dropna(subset=['predicted_rmsf', 'target_rmsf'], inplace=True)
                logger.info(f"Merged data for plots contains {len(eval_df)} entries.")

                if eval_df.empty:
                    logger.error("No overlapping data found between predictions and ground truth. Cannot generate comparison plots.")
                    eval_df = None # Reset eval_df if merge failed

    except Exception as e:
        logger.exception(f"Error loading data for visualization: {e}")
        return [] # Cannot proceed without data

    # --- Load History ---
    train_history = None
    if history_path and os.path.exists(history_path):
        logger.info(f"Loading training history from: {history_path}")
        try:
             train_history = load_json(history_path)
        except Exception as e:
             logger.error(f"Failed to load history file {history_path}: {e}")
    elif viz_config.get("plot_loss") or viz_config.get("plot_correlation"):
        logger.warning("Training history file not found or specified. Skipping metric curve plots.")


    # --- Generate Plots ---
    with log_stage("VISUALIZATION", "Creating plots"):
        # Training Curves (require history)
        if train_history:
            if viz_config.get("plot_loss", False):
                 path = create_loss_curve(train_history, viz_output_dir, save_format, dpi, save_plot_data)
                 if path: generated_plots.append(path)
            if viz_config.get("plot_correlation", False):
                 if 'train_pearson' in train_history and 'val_pearson' in train_history:
                      path = create_correlation_curve(train_history, viz_output_dir, save_format, dpi, save_plot_data)
                      if path: generated_plots.append(path)
                 else: logger.warning("Pearson correlation data not found in history. Skipping correlation curve.")
        else:
             if viz_config.get("plot_loss") or viz_config.get("plot_correlation"):
                 logger.info("Skipping loss/correlation curves as history file was not provided or loaded.")

        # Comparison Plots (require merged eval_df)
        if eval_df is not None and not eval_df.empty:
            if viz_config.get("plot_predictions", False):
                path = create_prediction_scatter(eval_df, viz_output_dir, save_format, dpi, max_scatter, save_plot_data)
                if path: generated_plots.append(path)
            if viz_config.get("plot_density_scatter", False):
                 path = create_predicted_vs_validation_scatter_density(eval_df, viz_output_dir, save_format, dpi, save_plot_data)
                 if path: generated_plots.append(path)
            if viz_config.get("plot_error_distribution", False):
                path = create_error_distribution(eval_df, viz_output_dir, save_format, dpi, save_plot_data)
                if path: generated_plots.append(path)
            if viz_config.get("plot_residue_type_analysis", False):
                path = create_residue_type_analysis(eval_df, viz_output_dir, save_format, dpi, save_plot_data)
                if path: generated_plots.append(path)
            if viz_config.get("plot_sasa_error_analysis", False):
                 path = create_sasa_error_analysis(eval_df, viz_output_dir, sasa_bins, save_format, dpi, save_plot_data)
                 if path: generated_plots.append(path)
            if viz_config.get("plot_ss_error_analysis", False):
                 path = create_ss_error_analysis(eval_df, viz_output_dir, save_format, dpi, save_plot_data)
                 if path: generated_plots.append(path)
            if viz_config.get("plot_amino_acid_performance", False):
                 path = create_amino_acid_performance(eval_df, viz_output_dir, save_format, dpi, save_plot_data)
                 if path: generated_plots.append(path)
        elif eval_df is None:
             logger.warning("Skipping comparison plots as merged evaluation data could not be created.")


    logger.info(f"Finished generating {len(generated_plots)} plots.")
    return generated_plots

==========================================================
===== FILE: input_data/train_domains.txt (Top 10 Lines) =====
==========================================================

12asA00
153lA00
16pkA02
1a02F00
1a0aA00
1a0hA01
1a0rP01
1a0sP00
1a15A00
1a1zA00

===== (End Snippet of input_data/train_domains.txt) =====

==========================================================
===== FILE: input_data/val_domains.txt (Top 10 Lines) =====
==========================================================

1a05A00
1ac5A00
1ad3A02
1amiA02
1au7A01
1aznA00
1b22A00
1b5tA00
1b9vA00
1bctA00

===== (End Snippet of input_data/val_domains.txt) =====

==========================================================
===== FILE: input_data/test_domains.txt (Top 10 Lines) =====
==========================================================

1e8rA00
1e8uA00
1ehiA03
1ei6A02
1ej5A00
1ej8A00
1eovA02
1ethA01
1ev0A00
1eysH02

===== (End Snippet of input_data/test_domains.txt) =====

==========================================================
              End of VoxelFlex Project Context            
==========================================================
